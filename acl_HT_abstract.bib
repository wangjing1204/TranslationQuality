@inproceedings{shaitarova-etal-2023-machine,
    title = "Machine vs. Human: Exploring Syntax and Lexicon in {G}erman Translations, with a Spotlight on Anglicisms",
    author = {Shaitarova, Anastassia  and
      G{\"o}hring, Anne  and
      Volk, Martin},
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.22",
    pages = "215--227",
    abstract = "Machine Translation (MT) has become an integral part of daily life for millions of people, with its output being so fluent that users often cannot distinguish it from human translation. However, these fluid texts often harbor algorithmic traces, from limited lexical choices to societal misrepresentations. This raises concerns about the possible effects of MT on natural language and human communication and calls for regular evaluations of machine-generated translations for different languages. Our paper explores the output of three widely used engines (Google, DeepL, Microsoft Azure) and one smaller commercial system. We translate the English and French source texts of seven diverse parallel corpora into German and compare MT-produced texts to human references in terms of lexical, syntactic, and morphological features. Additionally, we investigate how MT leverages lexical borrowings and analyse the distribution of anglicisms across the German translations.",
}
@inproceedings{qian-2023-performance,
    title = "Performance Evaluation on Human-Machine Teaming Augmented Machine Translation Enabled by {GPT}-4",
    author = "Qian, Ming",
    editor = "Guti{\'e}rrez, Raquel L{\'a}zaro  and
      Pareja, Antonio  and
      Mitkov, Ruslan",
    booktitle = "Proceedings of the First Workshop on NLP Tools and Resources for Translation and Interpreting Applications",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.nlp4tia-1.4",
    pages = "20--31",
    abstract = "Translation has been modeled as a multiple-phase process where pre-editing analyses guide meaning transfer and interlingual restructure. Present-day machine translation (MT) tools provide no means for source text analyses. Generative AI with Large language modeling (LLM), equipped with prompt engineering and fine-tuning capabilities, can enable augmented MT solutions by explicitly including AI or human generated analyses/instruction, and/or human-generated reference translation as pre-editing or interactive inputs. Using an English-to-Chinese translation piece that had been carefully studied during a translator slam event, Four types of translation outputs on 20 text segments were evaluated: human-generated translation, Google Translate MT, instruction-augmented MT using GPT4-LLM, and Human-Machine-Teaming (HMT)-augmented translation based on both human reference translation and instruction using GPT4-LLM. While human translation had the best performance, both augmented MT approaches performed better than un-augmented MT. The HMT-augmented MT performed better than instruction-augmented MT because it combined the guidance and knowledge provided by both human reference translation and style instruction. However, since it is unrealistic to generate sentence-by-sentence human translation as MT input, better approaches to HMT-augmented MT need to be invented. The evaluation showed that generative AI with LLM can enable new MT workflow facilitating pre-editing analyses and interactive restructuring and achieving better performance.",
}
@inproceedings{schierl-2023-reception,
    title = "Reception of machine-translated and human-translated subtitles {--} A case study",
    author = "Schierl, Frederike",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.4",
    pages = "42--53",
    abstract = {Accessibility and inclusion have become key terms of the last decades, and this does not exclude linguistics. Machine-translated subtitling has become the new approach to over-come linguistic accessibility barriers since it has proven to be fast and thus cost-efficient for audiovisual media, as opposed to human translation, which is time-intensive and costly. Machine translation can be considered as a solution when a translation is urgently needed. Overall, studies researching benefits of subtitling yield different results, also always depending on the application context (see Chan et al. 2022, Hu et al. 2020). Still, the acceptance of machine-translated subtitles is limited (see Tuominen et al., 2023) and users are rather skeptical, especially regarding the quality of MT subtitles. In the presented project, I investigated the effects of machine-translated subtitling (raw machine translation) compared to human-translated subtitling on the consumer, presenting the results of a case study, knowing that HT as the gold standard for translation is more and more put into question and being aware of today{'}s convincing output of NMT. The presented study investigates the use of (machine-translated) subtitles by the average consumer due to the current strong societal interest. I base my research project on the 3 R concept, i.e. response, reaction, and repercussion (Gambier, 2009), in which participants were asked to watch two video presentations on educational topics, one in German and another in Finnish, subtitled either with machine translation or by a human translator, or in a mixed condition (machine-translated and human-translated). Subtitle languages are English, German, and Finnish. Afterwards, they were asked to respond to questions on the video content (information retrieval) and evaluate the subtitles based on the User Experience Questionnaire (Laugwitz et al., 2008) and NASA Task Load Index (NASA, 2006). The case study shows that information retrieval in the HT conditions is higher, except for the direction Finnish-German. However, users generally report a better user experience for all lan-guages, which indicates a higher immersion. Participants also report that long subtitles combined with a fast pace contribute to more stress and more distraction from the other visual elements. Generally, users recognise the potential of MT subtitles, but also state that a human-in-the-loop is still needed to ensure publishable quality. References: Chan, Win Shan, Jan-Louis Kruger, and Stephen Doherty. 2022. {`}An Investigation of Subtitles as Learning Support in University Education{'}. Journal of Specialised Translation, no. 38: 155{--}79. Gambier, Yves. 2009. {`}Challenges in Research on Audiovisual Translation.{'} In Translation Research Projects 2, edited by Pym, Anthony and Alexander Perekrestenko, 17{--}25. Tarragona: Intercultural Studies Group. Hu, Ke, Sharon O{'}Brien, and Dorothy Kenny. 2020. {`}A Reception Study of Machine Translated Subtitles for MOOCs{'}. Perspectives 28 (4): 521{--}38. https://doi.org/10.1080/0907676X.2019.1595069. Laugwitz, Bettina, Theo Held, and Martin Schrepp. 2008. {`}Construction and Evaluation of a User Experience Questionnaire{'}. In Symposium of the Austrian HCI and Usability Engineering Group, edited by Andreas Holzinger, 63{--}76. Springer. NASA. 2006. {`}NASA TLX: Task Load Index{'}. Tuominen, Tiina, Maarit Koponen, Kaisa Vitikainen, Umut Sulubacak, and J{\"o}rg Tiedemann. 2023. {`}Exploring the Gaps in Linguistic Accessibility of Media: The Potential of Automated Subtitling as a Solution{'}. Journal of Specialised Translation, no. 39: 77{--}89.},
}
@inproceedings{lee-choi-2023-machine,
    title = "Machine translation of {K}orean statutes examined from the perspective of quality and productivity",
    author = "Lee, Jieun  and
      Choi, Hyoeun",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.13",
    pages = "143--151",
    abstract = "Because machine translation (MT) still falls short of human parity, human intervention is needed to ensure quality translation. The existing literature indicates that machine translation post-editing (MTPE) generally enhances translation productivity, but the question of quality remains for domain-specific texts (e.g. Aranberri et al., 2014; Jia et al., 2022; Kim et al., 2019; Lee, 2021a,b). Although legal translation is considered as one of the most complex specialist transla-tion domains, because of the demand surge for legal translation, MT has been utilized to some extent for documents of less importance (Roberts, 2022). Given that little research has examined the productivity and quality of MT and MTPE in Korean-English legal translation, we sought to examine the productivity and quality of MT and MTPE of Korean of statutes, using DeepL, a neural machine translation engine which has recently started the Korean language service. This paper presents the preliminary findings from a research project that investigated DeepL MT qua-lity and the quality and productivity of MTPE outputs and human translations by seven professional translators.",
}
@inproceedings{etchegoyhen-ponce-2023-learning,
    title = "Learning from Past Mistakes: Quality Estimation from Monolingual Corpora and Machine Translation Learning Stages",
    author = "Etchegoyhen, Thierry  and
      Ponce, David",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.8",
    pages = "84--98",
    abstract = "Quality Estimation (QE) of Machine Translation output suffers from the lack of annotated data to train supervised models across domains and language pairs. In this work, we describe a method to generate synthetic QE data based on Neural Machine Translation (NMT) models at different learning stages. Our approach consists in training QE models on the errors produced by different NMT model checkpoints, obtained during the course of model training, under the assumption that gradual learning will induce errors that more closely resemble those produced by NMT models in adverse conditions. We test this approach on English-German and Romanian-English WMT QE test sets, demonstrating that pairing translations from earlier checkpoints with translations of converged models outperforms the use of reference human translations and can achieve competitive results against human-labelled data. We also show that combining post-edited data with our synthetic data yields to significant improvements across the board. Our approach thus opens new possibilities for an efficient use of monolingual corpora to generate quality synthetic QE data, thereby mitigating the data bottleneck.",
}
@inproceedings{ganesh-etal-2023-findings,
    title = "Findings of the {C}o{C}o4{MT} 2023 Shared Task on Corpus Construction for Machine Translation",
    author = "Ganesh, Ananya  and
      Carpuat, Marine  and
      Chen, William  and
      Kann, Katharina  and
      Lignos, Constantine  and
      Ortega, John E.  and
      Saleva, Jonne  and
      Tafreshi, Shabnam  and
      Zevallos, Rodolfo",
    booktitle = "Proceedings of the Second Workshop on Corpus Generation and Corpus Augmentation for Machine Translation",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-coco4mt.3",
    pages = "22--27",
    abstract = "This paper provides an overview of the first shared task on choosing beneficial instances for machine translation, conducted as part of the CoCo4MT 2023 Workshop at MTSummit. This shared task was motivated by the need to make the data annotation process for machine translation more efficient, particularly for low-resource languages for which collecting human translations may be difficult or expensive. The task involved developing methods for selecting the most beneficial instances for training a machine translation system without access to an existing parallel dataset in the target language, such that the best selected instances can then be manually translated. Two teams participated in the shared task, namely the Williams team and the AST team. Submissions were evaluated by training a machine translation model on each submission{'}s chosen instances, and comparing their performance with the chRF++ score. The system that ranked first is by the Williams team, that finds representative instances by clustering the training data.",
}
@inproceedings{zhou-etal-2023-train,
    title = "Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages",
    author = "Zhou, Zhong  and
      Niehues, Jan  and
      Waibel, Alexander",
    editor = "Ojha, Atul Kr.  and
      Liu, Chao-hong  and
      Vylomova, Ekaterina  and
      Pirinen, Flammie  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.loresmt-1.1",
    doi = "10.18653/v1/2023.loresmt-1.1",
    pages = "1--15",
    abstract = "In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only âˆ¼1,000 in the new, unknown language.",
}
@inproceedings{zhuo-etal-2023-rethinking,
    title = "Rethinking Round-Trip Translation for Machine Translation Evaluation",
    author = "Zhuo, Terry Yue  and
      Xu, Qiongkai  and
      He, Xuanli  and
      Cohn, Trevor",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.22",
    doi = "10.18653/v1/2023.findings-acl.22",
    pages = "319--337",
    abstract = "Automatic evaluation methods for translation often require model training, and thus the availability of parallel corpora limits their applicability to low-resource settings. Round-trip translation is a potential workaround, which can reframe bilingual evaluation into a much simpler monolingual task. Early results from the era of statistical machine translation (SMT) raised fundamental concerns about the utility of this approach, based on poor correlation with human translation quality judgments. In this paper, we revisit this technique with modern neural translation (NMT) and show that round-trip translation does allow for accurate automatic evaluation without the need for reference translations. These opposite findings can be explained through the copy mechanism in SMT that is absent in NMT. We demonstrate that round-trip translation benefits multiple machine translation evaluation tasks: i) predicting forward translation scores; ii) improving the performance of a quality estimation model; and iii) identifying adversarial competitors in shared tasks via cross-system verification.",
}
@inproceedings{vincent-etal-2023-mtcue,
    title = "{MTC}ue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation",
    author = "Vincent, Sebastian  and
      Flynn, Robert  and
      Scarton, Carolina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.521",
    doi = "10.18653/v1/2023.findings-acl.521",
    pages = "8210--8226",
    abstract = "Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker{'}s gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCue significantly outperforms a {``}tagging{''} baseline at translating English text. Analysis reveals that the context encoder of MTCue learns a representation space that organises context based on specific attributes, such as formality, enabling effective zero-shot control. Pre-training on context embeddings also improves MTCue{'}s few-shot performance compared to the {``}tagging{''} baseline. Finally, an ablation study conducted on model components and contextual variables further supports the robustness of MTCue for context-based NMT.",
}
@inproceedings{lim-etal-2023-predicting,
    title = "Predicting Human Translation Difficulty Using Automatic Word Alignment",
    author = "Lim, Zheng Wei  and
      Cohn, Trevor  and
      Kemp, Charles  and
      Vylomova, Ekaterina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.736",
    doi = "10.18653/v1/2023.findings-acl.736",
    pages = "11590--11601",
    abstract = "Translation difficulty arises when translators are required to resolve translation ambiguity from multiple possible translations. Translation difficulty can be measured by recording the diversity of responses provided by human translators and the time taken to provide these responses, but these behavioral measures are costly and do not scale. In this work, we use word alignments computed over large scale bilingual corpora to develop predictors of lexical translation difficulty. We evaluate our approach using behavioural data from translations provided both in and out of context, and report results that improve on a previous embedding-based approach (Thompson et al., 2020). Our work can therefore contribute to a deeper understanding of cross-lingual differences and of causes of translation difficulty.",
}
@inproceedings{leena-etal-2023-humans,
    title = "Do Humans Translate like Machines? Students{'} Conceptualisations of Human and Machine Translation",
    author = "Leena, Salmi  and
      Dorst, Aletta G.  and
      Koponen, Maarit  and
      Zeven, Katinka",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.29",
    pages = "295--304",
    abstract = "This paper explores how students conceptualise the processes involved in human and machine translation, and how they describe the similarities and differences between them. The paper presents the results of a survey involving university students (B.A. and M.A.) taking a course on translation who filled out an online questionnaire distributed in Finnish, Dutch and English. Our study finds that students often describe both human translation and machine translation in similar terms, suggesting they do not sufficiently distinguish between them and do not fully understand how machine translation works. The current study suggests that training in Machine Translation Literacy may need to focus more on the conceptualisations involved and how conceptual and vernacular misconceptions may affect how translators understand human and machine translation.",
}
@inproceedings{popovic-etal-2023-computational,
    title = "Computational analysis of different translations: by professionals, students and machines",
    author = "Popovic, Maja  and
      Lapshinova-Koltunski, Ekaterina  and
      Koponen, Maarit",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.36",
    pages = "365--374",
    abstract = "In this work, we analyse different translated texts in terms of various text features. We compare two types of human translations, professional and students{'}, and machine translation outputs in terms of lexical and grammatical variety, sentence length,as well as frequencies of different POS tags and POS-trigrams. Our experimentsare carried out on parallel translations into three languages, Croatian, Finnish andRussian, all originating from the same source English texts. Our results indicatethat machine translations are closest to the source text, followed by student translations. Also, student translations are similar both to professional as well as to MT, sometimes even more to MT. Furthermore, we identify sets of features which are convenient for distinguishing machine from human translations.",
}
@inproceedings{kolar-kumar-2023-chatgpt,
    title = "{C}hat{GPT}{\_}{P}owered{\_}{T}ourist{\_}{A}id{\_}{A}pplications{\_}{\_}{P}roficient{\_}in{\_}{H}indi{\_}{\_}{Y}et{\_}{T}o{\_}{M}aster{\_}{T}elugu{\_}and{\_}{K}annada",
    author = "Kolar, Sanjana  and
      Kumar, Rohit",
    editor = "Chakravarthi, Bharathi R.  and
      Priyadharshini, Ruba  and
      M, Anand Kumar  and
      Thavareesan, Sajeetha  and
      Sherly, Elizabeth",
    booktitle = "Proceedings of the Third Workshop on Speech and Language Technologies for Dravidian Languages",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.dravidianlangtech-1.13",
    pages = "97--107",
    abstract = "This research investigates the effectiveness of Chat- GPT, an AI language model by OpenAI, in translating English into Hindi, Telugu, and Kannada languages, aimed at assisting tourists in India{'}s linguistically diverse environment. To measure the translation quality, a test set of 50 questions from diverse fields such as general knowledge, food, and travel was used. These were assessed by five volunteers for accuracy and fluency, and the scores were subsequently converted into a BLEU score. The BLEU score evaluates the closeness of a machine-generated translation to a human translation, with a higher score indicating better translation quality. The Hindi translations outperformed others, showcasing superior accuracy and fluency, whereas Telugu translations lagged behind. Human evaluators rated both the accuracy and fluency of translations, offering a comprehensive perspective on the language model{'}s performance.",
}
@inproceedings{dalayli-2023-use,
    title = "Use of {NLP} Techniques in Translation by {C}hat{GPT}: Case Study",
    author = "Dalayli, Feyza",
    editor = "Haddad, Amal Haddad  and
      Terryn, Ayla Rigouts  and
      Mitkov, Ruslan  and
      Rapp, Reinhard  and
      Zweigenbaum, Pierre  and
      Sharoff, Serge",
    booktitle = "Proceedings of the Workshop on Computational Terminology in NLP and Translation Studies (ConTeNTS) Incorporating the 16th Workshop on Building and Using Comparable Corpora (BUCC)",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.contents-1.3",
    pages = "19--25",
    abstract = "Use of NLP Techniques in Translation by ChatGPT: Case Study Natural Language Processing (NLP) refers to a field of study within the domain of artificial intelligence (AI) and computational linguistics that focuses on the interaction between computers and human language. NLP seeks to develop computational models and algorithms capable of understanding, analyzing, and generating natural language text and speech (Brown et al., 1990). At its core, NLP aims to bridge the gap between human language and machine understanding by employing various techniques from linguistics, computer science, and statistics. It involves the application of linguistic and computational theories to process, interpret, and extract meaningful information from unstructured textual data (Bahdanau, Cho and Bengio, 2015). Researchers and practitioners in NLP employ diverse methodologies, including rule-based approaches, statistical models, machine learning techniques (such as neural networks), and more recently, deep learning architectures. These methodologies enable the development of robust algorithms that can learn from large-scale language data to improve the accuracy and effectiveness of language processing systems (Nilsson, 2010). NLP has numerous real-world applications across various domains, including information retrieval, virtual assistants, chatbots, social media analysis, sentiment monitoring, automated translation services, and healthcare, among others (kaynak). As the field continues to advance, NLP strives to overcome challenges such as understanding the nuances of human language, handling ambiguity, context sensitivity, and incorporating knowledge from diverse sources to enable machines to effectively communicate and interact with humans in a more natural and intuitive manner. Natural Language Processing (NLP) and translation are interconnected fields that share a symbiotic relationship, as NLP techniques and methodologies greatly contribute to the advancement and effectiveness of machine translation systems. NLP, a subfield of artificial intelligence (AI), focuses on the interaction between computers and human language. It encompasses a wide range of tasks, including text analysis, syntactic and semantic parsing, sentiment analysis, information extraction, and machine translation (Bahdanau, Cho and Bengio, 2014). NMT models employ deep learning architectures, such as recurrent neural networks (RNNs) and more specifically, long short-term memory (LSTM) networks, to learn the mapping between source and target language sentences. These models are trained on large-scale parallel corpora, consisting of aligned sentence pairs in different languages. The training process involves optimizing model parameters to minimize the discrepancy between predicted translations and human-generated translations (Wu et al., 2016) NLP techniques are crucial at various stages of machine translation. Preprocessing techniques, such as tokenization, sentence segmentation, and morphological analysis, help break down input text into meaningful linguistic units, making it easier for translation models to process and understand the content. Syntactic and semantic parsing techniques aid in capturing the structural and semantic relationships within sentences, improving the overall coherence and accuracy of translations. Furthermore, NLP-based methods are employed for handling specific translation challenges, such as handling idiomatic expressions, resolving lexical ambiguities, and addressing syntactic divergences between languages. For instance, statistical alignment models, based on NLP algorithms, enable the identification of correspondences between words or phrases in source and target languages, facilitating the generation of more accurate translations (kaynak). Several studies have demonstrated the effectiveness of NLP techniques in enhancing machine translation quality. For example, Bahdanau et al. (2015) introduced the attention mechanism, an NLP technique that enables NMT models to focus on relevant parts of the source sentence during translation. This attention mechanism significantly improved the translation quality of neural machine translation models. ChatGPT is a language model developed by OpenAI that utilizes the principles of Natural Language Processing (NLP) for various tasks, including translations. NLP is a field of artificial intelligence that focuses on the interaction between computers and human language. It encompasses a range of techniques and algorithms for processing, analyzing, and understanding natural language. When it comes to translation, NLP techniques can be applied to facilitate the conversion of text from one language to another. ChatGPT employs a sequence-to-sequence model, a type of neural network architecture commonly used in machine translation tasks. This model takes an input sequence in one language and generates a corresponding output sequence in the target language (OpenAI, 2023). The training process for ChatGPT involves exposing the model to large amounts of multilingual data, allowing it to learn patterns, syntax, and semantic relationships across different languages. This exposure enables the model to develop a general understanding of language structures and meanings, making it capable of performing translation tasks. To enhance translation quality, ChatGPT leverages the Transformer architecture, which has been highly successful in NLP tasks. Transformers utilize attention mechanisms, enabling the model to focus on different parts of the input sequence during the translation process. This attention mechanism allows the model to capture long-range dependencies and improve the overall coherence and accuracy of translations. Additionally, techniques such as subword tokenization, which divides words into smaller units, are commonly employed in NLP translation systems like ChatGPT. Subword tokenization helps handle out-of-vocabulary words and improves the model{'}s ability to handle rare or unknown words (GPT-4 Technical Report, 2023). As can be seen, there have been significant developments in artificial intelligence translations thanks to NLP. However, it is not possible to say that it has fully reached the quality of translation made by people. The only goal in artificial intelligence translations is to reach translations made by humans. In general, there are some fundamental differences between human and ChatGPT translations. Human-made translations and translations generated by ChatGPT (or similar language models) have several key differences (Kelly and Zetzsche, 2014; Koehn, 2010; Sutskever, Vinyals and Le, 2014; Costa-juss{\`a} and Fonollosa, 2018) Translation Quality: Human translators are capable of producing high-quality translations with a deep understanding of both the source and target languages. They can accurately capture the nuances, cultural references, idioms, and context of the original text. On the other hand, ChatGPT translations can sometimes be less accurate or may not fully grasp the intended meaning due to the limitations of the training data and the model{'}s inability to comprehend context in the same way a human can. While ChatGPT can provide reasonable translations, they may lack the finesse and precision of a human translator. Natural Language Processing: Human translators are skilled at processing and understanding natural language, taking into account the broader context, cultural implications, and the intended audience. They can adapt their translations to suit the target audience, tone, and purpose of the text. ChatGPT, although trained on a vast amount of text data, lacks the same level of natural language understanding. It often relies on pattern matching and statistical analysis to generate translations, which can result in less nuanced or contextually appropriate outputs. Subject Matter Expertise: Human translators often specialize in specific domains or subject areas, allowing them to have deep knowledge and understanding of technical or specialized terminology. They can accurately translate complex or industry-specific texts, ensuring the meaning is preserved. ChatGPT, while having access to a wide range of general knowledge, may struggle with domain-specific vocabulary or terminology, leading to inaccuracies or incorrect translations in specialized texts. Cultural Sensitivity: Human translators are well-versed in the cultural nuances of both the source and target languages. They can navigate potential pitfalls, adapt the translation to the cultural context, and avoid unintended offensive or inappropriate language choices. ChatGPT lacks this level of cultural sensitivity and may produce translations that are culturally tone-deaf or insensitive, as it lacks the ability to understand the subtleties and implications of language choices. Revision and Editing: Human translators go through an iterative process of revision and editing to refine their translations, ensuring accuracy, clarity, and quality. They can self-correct errors and refine their translations based on feedback or additional research. ChatGPT, while capable of generating translations, does not have the same ability to self-correct or improve based on feedback. It generates translations in a single pass, without the iterative refinement process that humans can employ. In summary, while ChatGPT can be a useful tool for generating translations, human-made translations generally outperform machine-generated translations in terms of quality, accuracy, contextuality, cultural sensitivity, and domain-specific expertise. In conclusion, NLP and machine translation are closely intertwined, with NLP providing essential tools, methodologies, and techniques that contribute to the development and improvement of machine translation systems. The integration of NLP methods has led to significant advancements in translation accuracy, fluency, and the ability to handle various linguistic complexities. As NLP continues to evolve, its impact on the field of machine translation is expected to grow, enabling the creation of more sophisticated and context-aware translation systems. On the basis of all this information, in this research, it is aimed to compare the translations from English to Turkish made by ChatGPT, one of the most advanced artificial intelligences, with the translations made by humans. In this context, an academic 1 page English text was chosen. The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience. Afterwards, two different translations were examined comparatively by 5 different translators who are experts in their fields. Semi-structured in-depth interviews were conducted with these translators. The aim of this study is to reveal the role of artificial intelligence tools in translation, which are increasing day by day and suggesting that there will be no need for language learning in the future. On the other hand, many translators argue that artificial intelligence and human translations can be understood. Therefore, if artificial intelligence is successful, there will be no profession called translator in the future. This research seems to be very useful in terms of shedding light on the future. The method of this research is semi-structured in-depth interview. References Bahdanau, D., Cho, K. and Bengio Y. (2015). Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations. Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. A. (1990) statistical approach to machine translation. Computational linguistics 16, 2, 79{--}85. Costa-juss{\`a}, M. R., {\&} Fonollosa, J. A. R. (2018). {``}An Overview of Neural Machine Translation.{''} IEEE Transactions on Neural Networks and Learning Systems. GPT-4 Technical Report (2023). https://arxiv.org/abs/2303.08774. Kelly, N. and Zetzsche, J. (2014). Found in Translation: How Language Shapes Our Lives and Transforms the World. USA: Penguin Book. Koehn, P. (2010). {``}Statistical Machine Translation.{''} Cambridge University Press. Nilsson, N. J. (2010). The Quest For AI- A History Of Ideas And Achievements. http://ai.standford.edu/ nilsson/. OpenAI (2023). https://openai.com/blog/chatgpt/. Sutskever, I., Vinyals, O., {\&} Le, Q. V. (2014). {``}Sequence to Sequence Learning with Neural Networks.{''} Advances in Neural Information Processing Systems. Wu,Y. Schuster, M., Chen, Z., Le, Q. V. and Norouzi M. (2016). Google{'}s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. https://arxiv.org/pdf/1609.08144.pdf.",
}
@inproceedings{jiang-etal-2023-discourse,
    title = "Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated Parallel Corpus of Novels",
    author = "Jiang, Yuchen Eleanor  and
      Liu, Tianyu  and
      Ma, Shuming  and
      Zhang, Dongdong  and
      Sachan, Mrinmaya  and
      Cotterell, Ryan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.435",
    doi = "10.18653/v1/2023.acl-long.435",
    pages = "7853--7872",
    abstract = "Several recent papers claim to have achieved human parity at sentence-level machine translation (MT){---}especially between high-resource language pairs. In response, the MT community has, in part, shifted its focus to document-level translation. Translating documents requires a deeper understanding of the structure and meaning of text, which is often captured by various kinds of discourse phenomena such as consistency, coherence, and cohesion. However, this renders conventional sentence-level MT evaluation benchmarks inadequate for evaluating the performance of context-aware MT systems. This paperpresents a new dataset with rich discourse annotations, built upon the large-scale parallel corpus BWB introduced in Jiang et al. (2022a). The new BWB annotation introduces four extra evaluation aspects, i.e., entity, terminology, coreference, and quotation, covering 15,095 entity mentions in both languages. Using these annotations, we systematically investigate the similarities and differences between the discourse structures of source and target languages, and the challenges they pose to MT. We discover that MT outputs differ fundamentally from human translations in terms of their latent discourse structures. This gives us a new perspective on the challenges and opportunities in document-level MT. We make our resource publicly available to spur future research in document-level MT and its generalization to other language translation tasks.",
}
@inproceedings{li-etal-2023-best,
    title = "The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning",
    author = "Li, Zhuang  and
      Qu, Lizhen  and
      Cohen, Philip  and
      Tumuluri, Raj  and
      Haffari, Gholamreza",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.529",
    doi = "10.18653/v1/2023.acl-long.529",
    pages = "9511--9528",
    abstract = "Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues. However, human translations are expensive, while machine translations are cheap but prone to error and bias. In this work, we propose an active learning approach that exploits the strengths of both human and machine translations by iteratively adding small batches of human translations into the machine-translated training set. Besides, we propose novel aggregated acquisition criteria that help our active learning method select utterances to be manually translated. Our experiments demonstrate that an ideal utterance selection can significantly reduce the error and bias in the translated data, resulting in higher parser accuracies than the parsers merely trained on the machine-translated data.",
}
@inproceedings{huang-etal-2022-luls,
    title = "{LUL}{'}s {WMT}22 Automatic Post-Editing Shared Task Submission",
    author = "Huang, Xiaoying  and
      Lou, Xingrui  and
      Zhang, Fan  and
      Mei, Tu",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.68",
    pages = "689--693",
    abstract = "By learning the human post-edits, the automatic post-editing (APE) models are often used to modify the output of the machine translation (MT) system to make it as close as possible to human translation. We introduce the system used in our submission of WMT{'}22 Automatic Post-Editing (APE) English-Marathi (En-Mr) shared task. In this task, we first train the MT system of En-Mr to generate additional machine-translation sentences. Then we use the additional triple to bulid our APE model and use APE dataset to further fine-tuning. Inspired by the mixture of experts (MoE), we use GMM algorithm to roughly divide the text of APE dataset into three categories. After that, the experts are added to the APE model and different domain data are sent to different experts. Finally, we ensemble the models to get better performance. Our APE system significantly improves the translations of provided MT results by -2.848 and +3.74 on the development dataset in terms of TER and BLEU, respectively. Finally, the TER and BLEU scores are improved by -1.22 and +2.41 respectively on the blind test set.",
}
@inproceedings{bestgen-2022-comparing,
    title = "Comparing Formulaic Language in Human and Machine Translation: Insight from a Parliamentary Corpus",
    author = "Bestgen, Yves",
    editor = "Fi{\v{s}}er, Darja  and
      Eskevich, Maria  and
      Lenardi{\v{c}}, Jakob  and
      de Jong, Franciska",
    booktitle = "Proceedings of the Workshop ParlaCLARIN III within the 13th Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.parlaclarin-1.14",
    pages = "101--106",
    abstract = "A recent study has shown that, compared to human translations, neural machine translations contain more strongly-associated formulaic sequences made of relatively high-frequency words, but far less strongly-associated formulaic sequences made of relatively rare words. These results were obtained on the basis of translations of quality newspaper articles in which human translations can be thought to be not very literal. The present study attempts to replicate this research using a parliamentary corpus. The results confirm the observations on the news corpus, but the differences are less strong. They suggest that the use of text genres that usually result in more literal translations, such as parliamentary corpora, might be preferable when comparing human and machine translations.",
}
@inproceedings{lin-etal-2022-automatic,
    title = "Automatic Correction of Human Translations",
    author = "Lin, Jessy  and
      Kovacs, Geza  and
      Shastry, Aditya  and
      Wuebker, Joern  and
      DeNero, John",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.36",
    doi = "10.18653/v1/2022.naacl-main.36",
    pages = "494--507",
    abstract = "We introduce translation error correction (TEC), the task of automatically correcting human-generated translations. Imperfections in machine translations (MT) have long motivated systems for improving translations post-hoc with automatic post-editing. In contrast, little attention has been devoted to the problem of automatically correcting human translations, despite the intuition that humans make distinct errors that machines would be well-suited to assist with, from typos to inconsistencies in translation conventions. To investigate this, we build and release the Aced corpus with three TEC datasets (available at: github.com/lilt/tec). We show that human errors in TEC exhibit a more diverse range of errors and far fewer translation fluency errors than the MT errors in automatic post-editing datasets, suggesting the need for dedicated TEC models that are specialized to correct human errors. We show that pre-training instead on synthetic errors based on human errors improves TEC F-score by as much as 5.1 points. We conducted a human-in-the-loop user study with nine professional translation editors and found that the assistance of our TEC system led them to produce significantly higher quality revised translations.",
}
@inproceedings{marchisio-etal-2022-systematic,
    title = "On Systematic Style Differences between Unsupervised and Supervised {MT} and an Application for High-Resource Machine Translation",
    author = "Marchisio, Kelly  and
      Freitag, Markus  and
      Grangier, David",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.161",
    doi = "10.18653/v1/2022.naacl-main.161",
    pages = "2214--2225",
    abstract = "Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask whether the different training methods result in systematically different output beyond what is visible via quality metrics like adequacy or BLEU. We compare translations from supervised and unsupervised MT systems of similar quality, finding that unsupervised output is more fluent and more structurally different in comparison to human translation than is supervised MT. We then demonstrate a way to combine the benefits of both methods into a single system which results in improved adequacy and fluency as rated by human evaluators. Our results open the door to interesting discussions about how supervised and unsupervised MT might be different yet mutually-beneficial.",
}
@inproceedings{ni-etal-2022-original,
    title = "Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance",
    author = {Ni, Jingwei  and
      Jin, Zhijing  and
      Freitag, Markus  and
      Sachan, Mrinmaya  and
      Sch{\"o}lkopf, Bernhard},
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.389",
    doi = "10.18653/v1/2022.naacl-main.389",
    pages = "5303--5320",
    abstract = "Human-translated text displays distinct features from naturally written text in the same language. This phenomena, known as translationese, has been argued to confound the machine translation (MT) evaluation. Yet, we find that existing work on translationese neglects some important factors and the conclusions are mostly correlational but not causal. In this work, we collect CausalMT, a dataset where the MT training data are also labeled with the human translation directions. We inspect two critical factors, the train-test direction match (whether the human translation directions in the training and test sets are aligned), and data-model direction match (whether the model learns in the same direction as the human translation direction in the dataset). We show that these two factors have a large causal effect on the MT performance, in addition to the test-model direction mismatch highlighted by existing work on the impact of translationese. In light of our findings, we provide a set of suggestions for MT training and evaluation. Our code and data are at \url{https://github.com/EdisonNi-hku/CausalMT}",
}
@inproceedings{gladkoff-han-2022-hope,
    title = "{HOPE}: A Task-Oriented and Human-Centric Evaluation Framework Using Professional Post-Editing Towards More Effective {MT} Evaluation",
    author = "Gladkoff, Serge  and
      Han, Lifeng",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.2",
    pages = "13--21",
    abstract = "Traditional automatic evaluation metrics for machine translation have been widely criticized by linguists due to their low accuracy, lack of transparency, focus on language mechanics rather than semantics, and low agreement with human quality evaluation. Human evaluations in the form of MQM-like scorecards have always been carried out in real industry setting by both clients and translation service providers (TSPs). However, traditional human translation quality evaluations are costly to perform and go into great linguistic detail, raise issues as to inter-rater reliability (IRR) and are not designed to measure quality of worse than premium quality translations. In this work, we introduce \textbf{HOPE}, a task-oriented and \textit{ \textbf{h} }uman-centric evaluation framework for machine translation output based \textit{ \textbf{o} }n professional \textit{ \textbf{p} }ost-\textit{ \textbf{e} }diting annotations. It contains only a limited number of commonly occurring error types, and uses a scoring model with geometric progression of error penalty points (EPPs) reflecting error severity level to each translation unit. The initial experimental work carried out on English-Russian language pair MT outputs on marketing content type of text from highly technical domain reveals that our evaluation framework is quite effective in reflecting the MT output quality regarding both overall system-level performance and segment-level transparency, and it increases the IRR for error type interpretation. The approach has several key advantages, such as ability to measure and compare less than perfect MT output from different systems, ability to indicate human perception of quality, immediate estimation of the labor effort required to bring MT output to premium quality, low-cost and faster application, as well as higher IRR. Our experimental data is available at \url{https://github.com/lHan87/HOPE}.",
}
@inproceedings{colman-etal-2022-geco,
    title = "{GECO}-{MT}: The Ghent Eye-tracking Corpus of Machine Translation",
    author = "Colman, Toon  and
      Fonteyne, Margot  and
      Daems, Joke  and
      Dirix, Nicolas  and
      Macken, Lieve",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.4",
    pages = "29--38",
    abstract = "In the present paper, we describe a large corpus of eye movement data, collected during natural reading of a human translation and a machine translation of a full novel. This data set, called GECO-MT (Ghent Eye tracking Corpus of Machine Translation) expands upon an earlier corpus called GECO (Ghent Eye-tracking Corpus) by Cop et al. (2017). The eye movement data in GECO-MT will be used in future research to investigate the effect of machine translation on the reading process and the effects of various error types on reading. In this article, we describe in detail the materials and data collection procedure of GECO-MT. Extensive information on the language proficiency of our participants is given, as well as a comparison with the participants of the original GECO. We investigate the distribution of a selection of important eye movement variables and explore the possibilities for future analyses of the data. GECO-MT is freely available at \url{https://www.lt3.ugent.be/resources/geco-mt}.",
}
@inproceedings{lapshinova-koltunski-etal-2022-dihutra-parallel,
    title = "{D}i{H}u{T}ra: a Parallel Corpus to Analyse Differences between Human Translations",
    author = "Lapshinova-Koltunski, Ekaterina  and
      Popovi{\'c}, Maja  and
      Koponen, Maarit",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.186",
    pages = "1751--1760",
    abstract = "This paper describes a new corpus of human translations which contains both professional and students translations. The data consists of English sources {--} texts from news and reviews {--} and their translations into Russian and Croatian, as well as of the subcorpus containing translations of the review texts into Finnish. All target languages represent mid-resourced and less or mid-investigated ones. The corpus will be valuable for studying variation in translation as it allows a direct comparison between human translations of the same source texts. The corpus will also be a valuable resource for evaluating machine translation systems. We believe that this resource will facilitate understanding and improvement of the quality issues in both human and machine translation. In the paper, we describe how the data was collected, provide information on translator groups and summarise the differences between the human translations at hand based on our preliminary results with shallow features.",
}
@inproceedings{mujadia-sharma-2022-ltrc,
    title = "The {LTRC} {H}indi-{T}elugu Parallel Corpus",
    author = "Mujadia, Vandan  and
      Sharma, Dipti",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.365",
    pages = "3417--3424",
    abstract = "We present the Hindi-Telugu Parallel Corpus of different technical domains such as Natural Science, Computer Science, Law and Healthcare along with the General domain. The qualitative corpus consists of 700K parallel sentences of which 535K sentences were created using multiple methods such as extract, align and review of Hindi-Telugu corpora, end-to-end human translation, iterative back-translation driven post-editing and around 165K parallel sentences were collected from available sources in the public domain. We present the comparative assessment of created parallel corpora for representativeness and diversity. The corpus has been pre-processed for machine translation, and we trained a neural machine translation system using it and report state-of-the-art baseline results on the developed development set over multiple domains and on available benchmarks. With this, we define a new task on Domain Machine Translation for low resource language pairs such as Hindi and Telugu. The developed corpus (535K) is freely available for non-commercial research and to the best of our knowledge, this is the well curated, largest, publicly available domain parallel corpus for Hindi-Telugu.",
}
@inproceedings{poibeau-2022-human,
    title = "On {``}Human Parity{''} and {``}Super Human Performance{''} in Machine Translation Evaluation",
    author = "Poibeau, Thierry",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.647",
    pages = "6018--6023",
    abstract = "In this paper, we reassess claims of human parity and super human performance in machine translation. Although these terms have already been discussed, as well as the evaluation protocols used to achieved these conclusions (human-parity is achieved i) only for a very reduced number of languages, ii) on very specific types of documents and iii) with very literal translations), we show that the terms used are themselves problematic, and that human translation involves much more than what is embedded in automatic systems. We also discuss ethical issues related to the way results are presented and advertised. Finally, we claim that a better assessment of human capacities should be put forward and that the goal of replacing humans by machines is not a desirable one.",
}
@inproceedings{chuklin-etal-2022-clse,
    title = "{CLSE}: Corpus of Linguistically Significant Entities",
    author = "Chuklin, Aleksandr  and
      Zhao, Justin  and
      Kale, Mihir",
    editor = "Bosselut, Antoine  and
      Chandu, Khyathi  and
      Dhole, Kaustubh  and
      Gangal, Varun  and
      Gehrmann, Sebastian  and
      Jernite, Yacine  and
      Novikova, Jekaterina  and
      Perez-Beltrachini, Laura",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.6",
    doi = "10.18653/v1/2022.gem-1.6",
    pages = "78--96",
    abstract = "One of the biggest challenges of natural language generation (NLG) is the proper handling of named entities. Named entities are a common source of grammar mistakes such as wrong prepositions, wrong article handling, or incorrect entity inflection. Without factoring linguistic representation, such errors are often underrepresented when evaluating on a small set of arbitrarily picked argument values, or when translating a dataset from a linguistically simpler language, like English, to a linguistically complex language, like Russian. However, for some applications, broadly precise grammatical correctness is critical {--} native speakers may find entity-related grammar errors silly, jarring, or even offensive. To enable the creation of more linguistically diverse NLG datasets, we release a Corpus of Linguistically Significant Entities (CLSE) annotated by linguist experts. The corpus includes 34 languages and covers 74 different semantic types to support various applications from airline ticketing to video games. To demonstrate one possible use of CLSE, we produce an augmented version of the Schema-Guided Dialog Dataset, SGD-CLSE. Using the CLSE{'}s entities and a small number of human translations, we create a linguistically representative NLG evaluation benchmark in three languages: French (high-resource), Marathi (low-resource), and Russian (highly inflected language). We establish quality baselines for neural, template-based, and hybrid NLG systems and discuss the strengths and weaknesses of each approach.",
}
@inproceedings{freitag-etal-2022-natural,
    title = "A Natural Diet: Towards Improving Naturalness of Machine Translation Output",
    author = "Freitag, Markus  and
      Vilar, David  and
      Grangier, David  and
      Cherry, Colin  and
      Foster, George",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.263",
    doi = "10.18653/v1/2022.findings-acl.263",
    pages = "3340--3353",
    abstract = "Machine translation (MT) evaluation often focuses on accuracy and fluency, without paying much attention to translation style. This means that, even when considered accurate and fluent, MT output can still sound less natural than high quality human translations or text originally written in the target language. Machine translation output notably exhibits lower lexical diversity, and employs constructs that mirror those in the source sentence. In this work we propose a method for training MT systems to achieve a more natural style, i.e. mirroring the style of text originally written in the target language. Our method tags parallel training data according to the naturalness of the target side by contrasting language models trained on natural and translated data. Tagging data allows us to put greater emphasis on target sentences originally written in the target language. Automatic metrics show that the resulting models achieve lexical richness on par with human translations, mimicking a style much closer to sentences originally written in the target language. Furthermore, we find that their output is preferred by human experts when compared to the baseline translations.",
}
@inproceedings{thai-etal-2022-exploring,
    title = "Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature",
    author = "Thai, Katherine  and
      Karpinska, Marzena  and
      Krishna, Kalpesh  and
      Ray, Bill  and
      Inghilleri, Moira  and
      Wieting, John  and
      Iyyer, Mohit",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.672",
    doi = "10.18653/v1/2022.emnlp-main.672",
    pages = "9882--9902",
    abstract = "Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving both training procedures and their overall efficiency. Literary translation is less constrained than more traditional MT settings since translators must balance meaning equivalence, readability, and critical interpretability in the target language. This property, along with the complex discourse-level context present in literary texts, also makes literary MT more challenging to computationally model and evaluate. To explore this task, we collect a dataset (Par3) of non-English language novels in the public domain, each aligned at the paragraph level to both human and automatic English translations. Using Par3, we discover that expert literary translators prefer reference human translations over machine-translated paragraphs at a rate of 84{\%}, while state-of-the-art automatic MT metrics do not correlate with those preferences. The experts note that MT outputs contain not only mistranslations, but also discourse-disrupting errors and stylistic inconsistencies. To address these problems, we train a post-editing model whose output is preferred over normal MT output at a rate of 69{\%} by experts. We publicly release Par3 to spur future research into literary MT.",
}
@inproceedings{volkart-bouillon-2022-studying,
    title = "Studying Post-Editese in a Professional Context: A Pilot Study",
    author = "Volkart, Lise  and
      Bouillon, Pierrette",
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\"\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.10",
    pages = "71--79",
    abstract = "The past few years have seen the multiplication of studies on post-editese, following the massive adoption of post-editing in professional translation workflows. These studies mainly rely on the comparison of post-edited machine translation and human translation on artificial parallel corpora. By contrast, we investigate here post-editese on comparable corpora of authentic translation jobs for the language direction English into French. We explore commonly used scores and also proposes the use of a novel metric. Our analysis shows that post-edited machine translation is not only lexically poorer than human translation, but also less dense and less varied in terms of translation solutions. It also tends to be more prolific than human translation for our language direction. Finally, our study highlights some of the challenges of working with comparable corpora in post-editese research.",
}
@inproceedings{lapshinova-koltunski-etal-2022-dihutra,
    title = "{D}i{H}u{T}ra: a Parallel Corpus to Analyse Differences between Human Translations",
    author = "Lapshinova-Koltunski, Ekaterina  and
      Popovi{\'c}, Maja  and
      Koponen, Maarit",
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\"\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.58",
    pages = "337--338",
    abstract = "This project aimed to design a corpus of parallel human translations (HTs) of the same source texts by professionals and students. The resulting corpus consists of English news and reviews source texts, their translations into Russian and Croatian, and translations of the reviews into Finnish. The corpus will be valuable for both studying variation in translation and evaluating machine translation (MT) systems.",
}
@inproceedings{adebara-etal-2022-linguistically,
    title = "Linguistically-Motivated {Y}or{\`u}b{\'a}-{E}nglish Machine Translation",
    author = "Adebara, Ife  and
      Abdul-Mageed, Muhammad  and
      Silfverberg, Miikka",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.449",
    pages = "5066--5075",
    abstract = "Translating between languages where certain features are marked morphologically in one but absent or marked contextually in the other is an important test case for machine translation. When translating into English which marks (in)definiteness morphologically, from Yor{\`u}b{\'a} which uses bare nouns but marks these features contextually, ambiguities arise. In this work, we perform fine-grained analysis on how an SMT system compares with two NMT systems (BiLSTM and Transformer) when translating bare nouns in Yor{\`u}b{\'a} into English. We investigate how the systems what extent they identify BNs, correctly translate them, and compare with human translation patterns. We also analyze the type of errors each model makes and provide a linguistic description of these errors. We glean insights for evaluating model performance in low-resource settings. In translating bare nouns, our results show the transformer model outperforms the SMT and BiLSTM models for 4 categories, the BiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the NMT models for 1 category.",
}
@inproceedings{ogawa-2022-predicting,
    title = "Predicting the number of errors in human translation using source text and translator characteristics",
    author = "Ogawa, Haruka",
    editor = "Carl, Michael  and
      Yamada, Masaru  and
      Zou, Longui",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Workshop 1: Empirical Translation Process Research)",
    month = sep,
    year = "2022",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-wetpr.4",
    pages = "29--40",
    abstract = "Translation quality and efficiency are of great importance in the language services industry, which is why production duration and error counts are frequently investigated in Translation Process Research. However, a clear picture has not yet emerged as to how these two variables can be optimized or how they relate to one another. In the present study, data from multiple English-Japanese translation sessions is used to predict the number of errors per segment using source text and translator characteristics. An analysis utilizing zero-inflated generalized linear mixed effects models revealed that two source text characteristics (syntactic complexity and the proportion of long words) and three translator characteristics (years of experience, the time translators spent reading a source text before translating, and the time translators spent revising a translation) significantly influenced the number of errors. Furthermore, a lower proportion of long words per source text sentence and more training led to a significantly higher probability of error-free translation. Based on these results, combined with findings from a previous study on production duration, it is concluded that years of experience and the duration of the final revision phase are important factors that have a positive impact on translation efficiency and quality",
}
@inproceedings{anderson-etal-2022-lingua,
    title = "Lingua: Addressing Scenarios for Live Interpretation and Automatic Dubbing",
    author = "Anderson, Nathan  and
      Wilson, Caleb  and
      Richardson, Stephen D.",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.14",
    pages = "202--209",
    abstract = "Lingua is an application developed for the Church of Jesus Christ of Latter-day Saints that performs both real-time interpretation of live speeches and automatic video dubbing (AVD). Like other AVD systems, it can perform synchronized automatic dubbing, given video files and optionally, corresponding text files using a traditional ASR{--}MT{--}TTS pipeline. Lingua{'}s unique contribution is that it can also operate in real-time with a slight delay of a few seconds to interpret live speeches. If no source-language script is provided, the translations are exactly as recognized by ASR and translated by MT. If a script is provided, Lingua matches the recognized ASR segments with script segments and passes the latter to MT for translation and subsequent TTS. If a human translation is also provided, it is passed directly to TTS. Lingua switches between these modes dynamically, enabling translation of off-script comments and different levels of quality for multiple languages. (see extended abstract)",
}
@inproceedings{burda-lassen-2022-ukrainian,
    title = "{U}krainian-To-{E}nglish Folktale Corpus: Parallel Corpus Creation and Augmentation for Machine Translation in Low-Resource Languages",
    author = "Burda-Lassen, Olena",
    editor = "Ortega, John E.  and
      Carpuat, Marine  and
      Chen, William  and
      Kann, Katharina  and
      Lignos, Constantine  and
      Popovic, Maja  and
      Tafreshi, Shabnam",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Workshop 2: Corpus Generation and Corpus Augmentation for Machine Translation)",
    month = sep,
    year = "2022",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-coco4mt.4",
    pages = "28--31",
    abstract = "Folktales are linguistically very rich and culturally significant in understanding the source language. Historically, only human translation has been used for translating folklore. Therefore, the number of translated texts is very sparse, which limits access to knowledge about cultural traditions and customs. We have created a new Ukrainian-To-English parallel corpus of familiar Ukrainian folktales based on available English translations and suggested several new ones. We offer a combined domain-specific approach to building and augmenting this corpus, considering the nature of the domain and differences in the purpose of human versus machine translation. Our corpus is word and sentence-aligned, allowing for the best curation of meaning, specifically tailored for use as training data for machine translation models.",
}
@inproceedings{bizzoni-lapshinova-koltunski-2021-measuring,
    title = "Measuring Translationese across Levels of Expertise: Are Professionals more Surprising than Students?",
    author = "Bizzoni, Yuri  and
      Lapshinova-Koltunski, Ekaterina",
    editor = "Dobnik, Simon  and
      {\O}vrelid, Lilja",
    booktitle = "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may # " 31--2 " # jun,
    year = "2021",
    address = "Reykjavik, Iceland (Online)",
    publisher = {Link{\"o}ping University Electronic Press, Sweden},
    url = "https://aclanthology.org/2021.nodalida-main.6",
    pages = "53--63",
    abstract = "The present paper deals with a computational analysis of translationese in professional and student English-to-German translations belonging to different registers. Building upon an information-theoretical approach, we test translation conformity to source and target language in terms of a neural language model{'}s perplexity over Part of Speech (PoS) sequences. Our primary focus is on register diversification vs. convergence, reflected in the use of constructions eliciting a higher vs. lower perplexity score. Our results show that, against our expectations, professional translations elicit higher perplexity scores from a target language model than students{'} translations. An analysis of the distribution of PoS patterns across registers shows that this apparent paradox is the effect of higher stylistic diversification and register sensitivity in professional translations. Our results contribute to the understanding of human translationese and shed light on the variation in texts generated by different translators, which is valuable for translation studies, multilingual language processing, and machine translation.",
}
@inproceedings{yang-garland-gao-2021-rising,
    title = "A Rising Tide Lifts All Boats? Quality Correlation between Human Translation and Machine Assisted Translation",
    author = "Yang Garland, Evelyn  and
      Gao, Rony",
    editor = "Campbell, Janice  and
      Huyck, Ben  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Users and Providers Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-up.13",
    pages = "166--174",
    abstract = "Does the human who produces the best translation without Machine Translation (MT) also produce the best translation with the assistance of MT? Our empirical study has found a strong correlation between the quality of pure human translation (HT) and that of machine-assisted translation (MAT) produced by the same translator (Pearson correlation coefficient 0.85, p=0.007). Data from the study also indicates a more concentrated distribution of the MAT quality scores than that of the HT scores. Additional insights will also be discussed during the presentation. This study has two prominent features: the participation of professional translators (mostly ATA members, English-into-Chinese) as subjects, and the rigorous quality evaluation by multiple professional translators (all ATA certified) using ATA{'}s time-tested certification exam grading metrics. Despite a major limitation in sample size, our findings provide a strong indication of correlation between HT and MAT quality, adding to the body of evidence in support of further studies on larger scales.",
}
@inproceedings{erofeev-etal-2021-cushlepor,
    title = "cush{LEPOR} uses {LABSE} distilled knowledge to improve correlation with human translation evaluations",
    author = "Erofeev, Gleb  and
      Sorokina, Irina  and
      Han, Lifeng  and
      Gladkoff, Serge",
    editor = "Campbell, Janice  and
      Huyck, Ben  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Users and Providers Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-up.28",
    pages = "421--439",
    abstract = "Automatic MT evaluation metrics are indispensable for MT research. Augmented metrics such as hLEPOR include broader evaluation factors (recall and position difference penalty) in addition to the factors used in BLEU (sentence length, precision), and demonstrated higher accuracy. However, the obstacles preventing the wide use of hLEPOR were the lack of easy portable Python package and empirical weighting parameters that were tuned by manual work. This project addresses the above issues by offering a Python implementation of hLEPOR and automatic tuning of the parameters. We use existing translation memories (TM) as reference set and distillation modeling with LaBSE (Language-Agnostic BERT Sentence Embedding) to calibrate parameters for custom hLEPOR (cushLEPOR). cushLEPOR maximizes the correlation between hLEPOR and the distilling model similarity score towards reference. It can be used quickly and precisely to evaluate MT output from different engines, without need of manual weight tuning for optimization. In this session you will learn how to tune hLEPOR to obtain automatic custom-tuned cushLEPOR metric far more precise than BLEU. The method does not require costly human evaluations, existing TM is taken as a reference translation set, and cushLEPOR is created to select the best MT engine for the reference data-set.",
}
@inproceedings{zhou-waibel-2021-active,
    title = "Active Learning for Massively Parallel Translation of Constrained Text into Low Resource Languages",
    author = "Zhou, Zhong  and
      Waibel, Alex",
    editor = "Ortega, John  and
      Ojha, Atul Kr.  and
      Kann, Katharina  and
      Liu, Chao-Hong",
    booktitle = "Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-loresmt.4",
    pages = "32--43",
    abstract = "We translate a closed text that is known in advance and available in many languages into a new and severely low resource language. Most human translation efforts adopt a portionbased approach to translate consecutive pages/chapters in order, which may not suit machine translation. We compare the portion-based approach that optimizes coherence of the text locally with the random sampling approach that increases coverage of the text globally. Our results show that the random sampling approach performs better. When training on a seed corpus of âˆ¼1,000 lines from the Bible and testing on the rest of the Bible (âˆ¼30,000 lines), random sampling gives a performance gain of +11.0 BLEU using English as a simulated low resource language, and +4.9 BLEU using Eastern Pokomchi, a Mayan language. Furthermore, we compare three ways of updating machine translation models with increasing amount of human post-edited data through iterations. We find that adding newly post-edited data to training after vocabulary update without self-supervision performs the best. We propose an algorithm for human and machine to work together seamlessly to translate a closed text into a severely low resource language.",
}
@inproceedings{fu-nederhof-2021-automatic,
    title = "Automatic Classification of Human Translation and Machine Translation: A Study from the Perspective of Lexical Diversity",
    author = "Fu, Yingxue  and
      Nederhof, Mark-Jan",
    editor = "Bizzoni, Yuri  and
      Teich, Elke  and
      Espa{\~n}a-Bonet, Cristina  and
      van Genabith, Josef",
    booktitle = "Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age",
    month = may,
    year = "2021",
    address = "online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.motra-1.10",
    pages = "91--99",
}
@inproceedings{zhang-etal-2021-chrentranslate,
    title = "{C}hr{E}n{T}ranslate: {C}herokee-{E}nglish Machine Translation Demo with Quality Estimation and Corrective Feedback",
    author = "Zhang, Shiyue  and
      Frey, Benjamin  and
      Bansal, Mohit",
    editor = "Ji, Heng  and
      Park, Jong C.  and
      Xia, Rui",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.33",
    doi = "10.18653/v1/2021.acl-demo.33",
    pages = "272--279",
    abstract = "We introduce ChrEnTranslate, an online machine translation demonstration system for translation between English and an endangered language Cherokee. It supports both statistical and neural translation models as well as provides quality estimation to inform users of reliability, two user feedback interfaces for experts and common users respectively, example inputs to collect human translations for monolingual data, word alignment visualization, and relevant terms from the Cherokee English dictionary. The quantitative evaluation demonstrates that our backbone translation models achieve state-of-the-art translation performance and our quality estimation well correlates with both BLEU and human judgment. By analyzing 216 pieces of expert feedback, we find that NMT is preferable because it copies less than SMT, and, in general, current models can translate fragments of the source sentence but make major mistakes. When we add these 216 expert-corrected parallel texts into the training set and retrain models, equal or slightly better performance is observed, which demonstrates indicates the potential of human-in-the-loop learning.",
}
@inproceedings{srivastava-singh-2020-phinc,
    title = "{PHINC}: A Parallel {H}inglish Social Media Code-Mixed Corpus for Machine Translation",
    author = "Srivastava, Vivek  and
      Singh, Mayank",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wnut-1.7",
    doi = "10.18653/v1/2020.wnut-1.7",
    pages = "41--49",
    abstract = "Code-mixing is the phenomenon of using more than one language in a sentence. In the multilingual communities, it is a very frequently observed pattern of communication on social media platforms. Flexibility to use multiple languages in one text message might help to communicate efficiently with the target audience. But, the noisy user-generated code-mixed text adds to the challenge of processing and understanding natural language to a much larger extent. Machine translation from monolingual source to the target language is a well-studied research problem. Here, we demonstrate that widely popular and sophisticated translation systems such as Google Translate fail at times to translate code-mixed text effectively. To address this challenge, we present a parallel corpus of the 13,738 code-mixed Hindi-English sentences and their corresponding human translation in English. In addition, we also propose a translation pipeline build on top of Google Translate. The evaluation of the proposed pipeline on $PHINC$ demonstrates an increase in the performance of the underlying system. With minimal effort, we can extend the dataset and the proposed approach to other code-mixing language pairs.",
}
@inproceedings{mathur-etal-2020-results,
    title = "Results of the {WMT}20 Metrics Shared Task",
    author = "Mathur, Nitika  and
      Wei, Johnny  and
      Freitag, Markus  and
      Ma, Qingsong  and
      Bojar, Ond{\v{r}}ej",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.77",
    pages = "688--725",
    abstract = "This paper presents the results of the WMT20 Metrics Shared Task. Participants were asked to score the outputs of the translation systems competing in the WMT20 News Translation Task with automatic metrics. Ten research groups submitted 27 metrics, four of which are reference-less {``}metrics{''}. In addition, we computed five baseline metrics, including sentBLEU, BLEU, TER and using the SacreBLEU scorer. All metrics were evaluated on how well they correlate at the system-, document- and segment-level with the WMT20 official human scores. We present an extensive analysis on influence of different reference translations on metric reliability, how well automatic metrics score human translations, and we also flag major discrepancies between metric and human scores when evaluating MT systems. Finally, we investigate whether we can use automatic metrics to flag incorrect human ratings.",
}
@inproceedings{lo-2020-extended,
    title = "Extended Study on Using Pretrained Language Models and {Y}i{S}i-1 for Machine Translation Evaluation",
    author = "Lo, Chi-kiu",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.99",
    pages = "895--902",
    abstract = "We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1{'}s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.",
}
@inproceedings{popovic-etal-2020-neural,
    title = "Neural Machine Translation for translating into {C}roatian and {S}erbian",
    author = "Popovi{\'c}, Maja  and
      Poncelas, Alberto  and
      Brkic, Marija  and
      Way, Andy",
    editor = {Zampieri, Marcos  and
      Nakov, Preslav  and
      Ljube{\v{s}}i{\'c}, Nikola  and
      Tiedemann, J{\"o}rg  and
      Scherrer, Yves},
    booktitle = "Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics (ICCL)",
    url = "https://aclanthology.org/2020.vardial-1.10",
    pages = "102--113",
    abstract = "In this work, we systematically investigate different set-ups for training of neural machine translation (NMT) systems for translation into Croatian and Serbian, two closely related South Slavic languages. We explore English and German as source languages, different sizes and types of training corpora, as well as bilingual and multilingual systems. We also explore translation of English IMDb user movie reviews, a domain/genre where only monolingual data are available. First, our results confirm that multilingual systems with joint target languages perform better. Furthermore, translation performance from English is much better than from German, partly because German is morphologically more complex and partly because the corpus consists mostly of parallel human translations instead of original text and its human translation. The translation from German should be further investigated systematically. For translating user reviews, creating synthetic in-domain parallel data through back- and forward-translation and adding them to a small out-of-domain parallel corpus can yield performance comparable with a system trained on a full out-of-domain corpus. However, it is still not clear what is the optimal size of synthetic in-domain data, especially for forward-translated data where the target language is machine translated. More detailed research including manual evaluation and analysis is needed in this direction.",
}
@inproceedings{yuan-sharoff-2020-sentence,
    title = "Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks",
    author = "Yuan, Yu  and
      Sharoff, Serge",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.229",
    pages = "1858--1865",
    abstract = "This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{bizzoni-etal-2020-human,
    title = "How Human is Machine Translationese? Comparing Human and Machine Translations of Text and Speech",
    author = "Bizzoni, Yuri  and
      Juzek, Tom S  and
      Espa{\~n}a-Bonet, Cristina  and
      Dutta Chowdhury, Koel  and
      van Genabith, Josef  and
      Teich, Elke",
    editor = {Federico, Marcello  and
      Waibel, Alex  and
      Knight, Kevin  and
      Nakamura, Satoshi  and
      Ney, Hermann  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Wu, Dekai  and
      Mariani, Joseph  and
      Yvon, Francois},
    booktitle = "Proceedings of the 17th International Conference on Spoken Language Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.iwslt-1.34",
    doi = "10.18653/v1/2020.iwslt-1.34",
    pages = "280--290",
    abstract = "Translationese is a phenomenon present in human translations, simultaneous interpreting, and even machine translations. Some translationese features tend to appear in simultaneous interpreting with higher frequency than in human text translation, but the reasons for this are unclear. This study analyzes translationese patterns in translation, interpreting, and machine translation outputs in order to explore possible reasons. In our analysis we {--} (i) detail two non-invasive ways of detecting translationese and (ii) compare translationese across human and machine translations from text and speech. We find that machine translation shows traces of translationese, but does not reproduce the patterns found in human translation, offering support to the hypothesis that such patterns are due to the model (human vs machine) rather than to the data (written vs spoken).",
}
@inproceedings{zhao-etal-2020-active,
    title = "Active Learning Approaches to Enhancing Neural Machine Translation",
    author = "Zhao, Yuekai  and
      Zhang, Haoran  and
      Zhou, Shuchang  and
      Zhang, Zhihua",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.162",
    doi = "10.18653/v1/2020.findings-emnlp.162",
    pages = "1796--1806",
    abstract = "Active learning is an efficient approach for mitigating data dependency when training neural machine translation (NMT) models. In this paper, we explore new training frameworks by incorporating active learning into various techniques such as transfer learning and iterative back-translation (IBT) under a limited human translation budget. We design a word frequency based acquisition function and combine it with a strong uncertainty based method. The combined method steadily outperforms all other acquisition functions in various scenarios. As far as we know, we are the first to do a large-scale study on actively training Transformer for NMT. Specifically, with a human translation budget of only 20{\%} of the original parallel corpus, we manage to surpass Transformer trained on the entire parallel corpus in three language pairs.",
}
@inproceedings{graham-etal-2020-assessing,
    title = "Assessing Human-Parity in Machine Translation on the Segment Level",
    author = "Graham, Yvette  and
      Federmann, Christian  and
      Eskevich, Maria  and
      Haddow, Barry",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.375",
    doi = "10.18653/v1/2020.findings-emnlp.375",
    pages = "4199--4207",
    abstract = "Recent machine translation shared tasks have shown top-performing systems to tie or in some cases even outperform human translation. Such conclusions about system and human performance are, however, based on estimates aggregated from scores collected over large test sets of translations and unfortunately leave some remaining questions unanswered. For instance, simply because a system significantly outperforms the human translator on average may not necessarily mean that it has done so for every translation in the test set. Firstly, are there remaining source segments present in evaluation test sets that cause significant challenges for top-performing systems and can such challenging segments go unnoticed due to the opacity of current human evaluation procedures? To provide insight into these issues we carefully inspect the outputs of top-performing systems in the most recent WMT-19 news translation shared task for all language pairs in which a system either tied or outperformed human translation. Our analysis provides a new method of identifying the remaining segments for which either machine or human perform poorly. For example, in our close inspection of WMT-19 English to German and German to English we discover the segments that disjointly proved a challenge for human and machine. For English to Russian, there were no segments included in our sample of translations that caused a significant challenge for the human translator, while we again identify the set of segments that caused issues for the top-performing system.",
}
@inproceedings{weng-etal-2020-towards,
    title = "Towards Enhancing Faithfulness for Neural Machine Translation",
    author = "Weng, Rongxiang  and
      Yu, Heng  and
      Wei, Xiangpeng  and
      Luo, Weihua",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.212",
    doi = "10.18653/v1/2020.emnlp-main.212",
    pages = "2675--2684",
    abstract = "Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.",
}
@inproceedings{fischer-laubli-2020-whats,
    title = "What{'}s the Difference Between Professional Human and Machine Translation? A Blind Multi-language Study on Domain-specific {MT}",
    author = {Fischer, Lukas  and
      L{\"a}ubli, Samuel},
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.23",
    pages = "215--224",
    abstract = "Machine translation (MT) has been shown to produce a number of errors that require human post-editing, but the extent to which professional human translation (HT) contains such errors has not yet been compared to MT. We compile pre-translated documents in which MT and HT are interleaved, and ask professional translators to flag errors and post-edit these documents in a blind evaluation. We find that the post-editing effort for MT segments is only higher in two out of three language pairs, and that the number of segments with wrong terminology, omissions, and typographical problems is similar in HT.",
}
@inproceedings{popovic-2020-differences,
    title = "On the differences between human translations",
    author = "Popovic, Maja",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.39",
    pages = "365--374",
    abstract = "Many studies have confirmed that translated texts exhibit different features than texts originally written in the given language. This work explores texts translated by different translators taking into account expertise and native language. A set of computational analyses was conducted on three language pairs, English-Croatian, German-French and English-Finnish, and the results show that each of the factors has certain influence on the features of the translated texts, especially on sentence length and lexical richness. The results also indicate that for translations used for machine translation evaluation, it is important to specify these factors, especially if comparing machine translation quality with human translation quality is involved.",
}
@inproceedings{zhai-etal-2020-detecting,
    title = "Detecting Non-literal Translations by Fine-tuning Cross-lingual Pre-trained Language Models",
    author = "Zhai, Yuming  and
      Illouz, Gabriel  and
      Vilnat, Anne",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.522",
    doi = "10.18653/v1/2020.coling-main.522",
    pages = "5944--5956",
    abstract = "Human-generated non-literal translations reflect the richness of human languages and are sometimes indispensable to ensure adequacy and fluency. Non-literal translations are difficult to produce even for human translators, especially for foreign language learners, and machine translations are still on the way to simulate human ones on this aspect. In order to foster the study on appropriate and creative non-literal translations, automatically detecting them in parallel corpora is an important step, which can benefit downstream NLP tasks or help to construct materials to teach translation. This article demonstrates that generic sentence representations produced by a pre-trained cross-lingual language model could be fine-tuned to solve this task. We show that there exists a moderate positive correlation between the prediction probability of being human translation and the non-literal translations{'} proportion in a sentence. The fine-tuning experiments show an accuracy of 80.16{\%} when predicting the presence of non-literal translations in a sentence and an accuracy of 85.20{\%} when distinguishing literal and non-literal translations at phrase level. We further conduct a linguistic error analysis and propose directions for future work.",
}
@inproceedings{bhardwaj-etal-2020-human,
    title = "Human or Neural Translation?",
    author = "Bhardwaj, Shivendra  and
      Alfonso Hermelo, David  and
      Langlais, Phillippe  and
      Bernier-Colborne, Gabriel  and
      Goutte, Cyril  and
      Simard, Michel",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.576",
    doi = "10.18653/v1/2020.coling-main.576",
    pages = "6553--6564",
    abstract = "Deep neural models tremendously improved machine translation. In this context, we investigate whether distinguishing machine from human translations is still feasible. We trained and applied 18 classifiers under two settings: a monolingual task, in which the classifier only looks at the translation; and a bilingual task, in which the source text is also taken into consideration. We report on extensive experiments involving 4 neural MT systems (Google Translate, DeepL, as well as two systems we trained) and varying the domain of texts. We show that the bilingual task is the easiest one and that transfer-based deep-learning classifiers perform best, with mean accuracies around 85{\%} in-domain and 75{\%} out-of-domain .",
}
@inproceedings{whitaker-2020-machine,
    title = "The Machine is Blind: Bottom-Up Feedback on the Impact of {MT} on Human Translation Performance",
    author = "Whitaker, Rhett",
    editor = "O'Brien, Sharon  and
      Simard, Michel",
    booktitle = "Workshop on the Impact of Machine Translation (iMpacT 2020)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2020.amta-impact.1",
    pages = "1--17",
}
@inproceedings{kunilovskaya-lapshinova-koltunski-2019-translationese,
    title = "Translationese Features as Indicators of Quality in {E}nglish-{R}ussian Human Translation",
    author = "Kunilovskaya, Maria  and
      Lapshinova-Koltunski, Ekaterina",
    booktitle = "Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "Incoma Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/W19-8706",
    doi = "10.26615/issn.2683-0078.2019_006",
    pages = "47--56",
    abstract = "We use a range of morpho-syntactic features inspired by research in register studies (e.g. Biber, 1995; Neumann, 2013) and translation studies (e.g. Ilisei et al., 2010; Zanettin, 2013; Kunilovskaya and Kutuzov, 2018) to reveal the association between translationese and human translation quality. Translationese is understood as any statistical deviations of translations from non-translations (Baker, 1993) and is assumed to affect the fluency of translations, rendering them foreign-sounding and clumsy of wording and structure. This connection is often posited or implied in the studies of translationese or translational varieties (De Sutter et al., 2017), but is rarely directly tested. Our 45 features include frequencies of selected morphological forms and categories, some types of syntactic structures and relations, as well as several overall text measures extracted from Universal Dependencies annotation. The research corpora include English-to-Russian professional and student translations of informational or argumentative newspaper texts and a comparable corpus of non-translated Russian. Our results indicate lack of direct association between translationese and quality in our data: while our features distinguish translations and non-translations with the near perfect accuracy, the performance of the same algorithm on the quality classes barely exceeds the chance level.",
}
@inproceedings{o-dowd-2019-unreasonable,
    title = "The unreasonable effectiveness of Neural Models in Language Decoding",
    author = "O'Dowd, Tony",
    editor = "Rossi, Laura",
    booktitle = "Proceedings of Machine Translation Summit XVII: Tutorial Abstracts",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/W19-7601",
    abstract = "This tutorial will provide an in-depth look at the experiments, jointly carried out by KantanMT and eBay during 2018, to determine which Neural Model delivers the best translation performance for eBay Customer Service content. It will lay out the timeline, process and mechanisms used to customise Neural MT models and how these were used in conjunction with Human Based evaluations to determine which approach to Neural MT provided the best translation outcomes.The tutorial will cover the following topics and methods:- Structural differences in Neural Networks and how they assist the language decoding process {--} RNN, CNN and TNN will be covered in detailed.- Customisation of Neural MT using the KantanMT Platform- Using MQM Framework for the evaluation and comparison of Translation Outputs and comparison to Human Translation- Collation and analysis of experimental findings in reaching our decision to standardise on Transformer type networks.Participants of the tutorial will get a clear understanding of Neural Model types and the differences, it will also cover how to customise these models and then how to set up a controlled experiment to determine translation performance.",
}
@inproceedings{almazroei-etal-2019-investigating,
    title = "Investigating Correlations Between Human Translation and {MT} Output",
    author = "Almazroei, Samar A.  and
      Ogawa, Haruka  and
      Gilbert, Devin",
    editor = "Carl, Michael  and
      Hansen-Schirra, Silvia",
    booktitle = "Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/W19-7006",
    pages = "11--13",
}
@inproceedings{matusov-etal-2019-customizing,
    title = "Customizing Neural Machine Translation for Subtitling",
    author = "Matusov, Evgeny  and
      Wilken, Patrick  and
      Georgakopoulou, Yota",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5209",
    doi = "10.18653/v1/W19-5209",
    pages = "82--93",
    abstract = "In this work, we customized a neural machine translation system for translation of subtitles in the domain of entertainment. The neural translation model was adapted to the subtitling content and style and extended by a simple, yet effective technique for utilizing inter-sentence context for short sentences such as dialog turns. The main contribution of the paper is a novel subtitle segmentation algorithm that predicts the end of a subtitle line given the previous word-level context using a recurrent neural network learned from human segmentation decisions. This model is combined with subtitle length and duration constraints established in the subtitling industry. We conducted a thorough human evaluation with two post-editors (English-to-Spanish translation of a documentary and a sitcom). It showed a notable productivity increase of up to 37{\%} as compared to translating from scratch and significant reductions in human translation edit rate in comparison with the post-editing of the baseline non-adapted system without a learned segmentation model.",
}
@inproceedings{hashimoto-etal-2019-high,
    title = "A High-Quality Multilingual Dataset for Structured Documentation Translation",
    author = "Hashimoto, Kazuma  and
      Buschiazzo, Raffaella  and
      Bradbury, James  and
      Marshall, Teresa  and
      Socher, Richard  and
      Xiong, Caiming",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5212",
    doi = "10.18653/v1/W19-5212",
    pages = "116--127",
    abstract = "This paper presents a high-quality multilingual dataset for the documentation domain to advance research on localization of structured text. Unlike widely-used datasets for translation of plain text, we collect XML-structured parallel text segments from the online documentation for an enterprise software platform. These Web pages have been professionally translated from English into 16 languages and maintained by domain experts, and around 100,000 text segments are available for each language pair. We build and evaluate translation models for seven target languages from English, with several different copy mechanisms and an XML-constrained beam search. We also experiment with a non-English pair to show that our dataset has the potential to explicitly enable 17 {\mbox{$\times$}} 16 translation settings. Our experiments show that learning to translate with the XML tags improves translation accuracy, and the beam search accurately generates XML structures. We also discuss trade-offs of using the copy mechanisms by focusing on translation of numerical words and named entities. We further provide a detailed human analysis of gaps between the model output and human translations for real-world applications, including suitability for post-editing.",
}
@inproceedings{kalimuthu-etal-2019-incremental,
    title = "Incremental Domain Adaptation for Neural Machine Translation in Low-Resource Settings",
    author = "Kalimuthu, Marimuthu  and
      Barz, Michael  and
      Sonntag, Daniel",
    editor = "El-Hajj, Wassim  and
      Belguith, Lamia Hadrich  and
      Bougares, Fethi  and
      Magdy, Walid  and
      Zitouni, Imed  and
      Tomeh, Nadi  and
      El-Haj, Mahmoud  and
      Zaghouani, Wajdi",
    booktitle = "Proceedings of the Fourth Arabic Natural Language Processing Workshop",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4601",
    doi = "10.18653/v1/W19-4601",
    pages = "1--10",
    abstract = "We study the problem of incremental domain adaptation of a generic neural machine translation model with limited resources (e.g., budget and time) for human translations or model training. In this paper, we propose a novel query strategy for selecting {``}unlabeled{''} samples from a new domain based on sentence embeddings for Arabic. We accelerate the fine-tuning process of the generic model to the target domain. Specifically, our approach estimates the informativeness of instances from the target domain by comparing the distance of their sentence embeddings to embeddings from the generic domain. We perform machine translation experiments (Ar-to-En direction) for comparing a random sampling baseline with our new approach, similar to active learning, using two small update sets for simulating the work of human translators. For the prescribed setting we can save more than 50{\%} of the annotation costs without loss in quality, demonstrating the effectiveness of our approach.",
}
@inproceedings{lo-simard-2019-fully,
    title = "Fully Unsupervised Crosslingual Semantic Textual Similarity Metric Based on {BERT} for Identifying Parallel Data",
    author = "Lo, Chi-kiu  and
      Simard, Michel",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1020",
    doi = "10.18653/v1/K19-1020",
    pages = "206--215",
    abstract = "We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT {--} Bidirectional Encoder Representations from Transformers (Devlin et al., 2019). The goal of crosslingual STS is to measure to what degree two segments of text in different languages express the same meaning. Not only is it a key task in crosslingual natural language understanding (XLU), it is also particularly useful for identifying parallel resources for training and evaluating downstream multilingual natural language processing (NLP) applications, such as machine translation. Most previous crosslingual STS methods relied heavily on existing parallel resources, thus leading to a circular dependency problem. With the advent of massively multilingual context representation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches.",
}
@inproceedings{lapshinova-koltunski-etal-2019-analysing,
    title = "Analysing Coreference in Transformer Outputs",
    author = "Lapshinova-Koltunski, Ekaterina  and
      Espa{\~n}a-Bonet, Cristina  and
      van Genabith, Josef",
    editor = "Popescu-Belis, Andrei  and
      Lo{\'a}iciga, Sharid  and
      Hardmeier, Christian  and
      Xiong, Deyi",
    booktitle = "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6501",
    doi = "10.18653/v1/D19-6501",
    pages = "1--12",
    abstract = "We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra- and cross-sentential anaphoric information. We compare system performance on two different genres: news and TED talks. To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations. We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words. The features of coreference chains in automatic translations are also compared to those of the source texts and human translations. The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.",
}
@inproceedings{toral-etal-2018-attaining,
    title = "Attaining the Unattainable? Reassessing Claims of Human Parity in Neural Machine Translation",
    author = "Toral, Antonio  and
      Castilho, Sheila  and
      Hu, Ke  and
      Way, Andy",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6312",
    doi = "10.18653/v1/W18-6312",
    pages = "113--123",
    abstract = "We reassess a recent study (Hassan et al., 2018) that claimed that machine translation (MT) has reached human parity for the translation of news from Chinese into English, using pairwise ranking and considering three variables that were not taken into account in that previous study: the language in which the source side of the test set was originally written, the translation proficiency of the evaluators, and the provision of inter-sentential context. If we consider only original source text (i.e. not translated from another language, or translationese), then we find evidence showing that human parity has not been achieved. We compare the judgments of professional translators against those of non-experts and discover that those of the experts result in higher inter-annotator agreement and better discrimination between human and machine translations. In addition, we analyse the human translations of the test set and identify important translation issues. Finally, based on these findings, we provide a set of recommendations for future human evaluations of MT.",
}
@inproceedings{zhou-etal-2018-massively,
    title = "Massively Parallel Cross-Lingual Learning in Low-Resource Target Language Translation",
    author = "Zhou, Zhong  and
      Sperber, Matthias  and
      Waibel, Alexander",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6324",
    doi = "10.18653/v1/W18-6324",
    pages = "232--243",
    abstract = "We work on translation from rich-resource languages to low-resource languages. The main challenges we identify are the lack of low-resource language data, effective methods for cross-lingual transfer, and the variable-binding problem that is common in neural systems. We build a translation system that addresses these challenges using eight European language families as our test ground. Firstly, we add the source and the target family labels and study intra-family and inter-family influences for effective cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for English-Swedish translation using eight families compared to the single-family multi-source multi-target baseline. Moreover, we find that training on two neighboring families closest to the low-resource language is often enough. Secondly, we construct an ablation study and find that reasonably good results can be achieved even with considerably less target data. Thirdly, we address the variable-binding problem by building an order-preserving named entity translation model. We obtain 60.6{\%} accuracy in qualitative evaluation where our translations are akin to human translations in a preliminary study.",
}
@inproceedings{steele-specia-2018-vis,
    title = "Vis-Eval Metric Viewer: A Visualisation Tool for Inspecting and Evaluating Metric Scores of Machine Translation Output",
    author = "Steele, David  and
      Specia, Lucia",
    editor = "Liu, Yang  and
      Paek, Tim  and
      Patwardhan, Manasi",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-5015",
    doi = "10.18653/v1/N18-5015",
    pages = "71--75",
    abstract = "Machine Translation systems are usually evaluated and compared using automated evaluation metrics such as BLEU and METEOR to score the generated translations against human translations. However, the interaction with the output from the metrics is relatively limited and results are commonly a single score along with a few additional statistics. Whilst this may be enough for system comparison it does not provide much useful feedback or a means for inspecting translations and their respective scores. VisEval Metric Viewer VEMV is a tool designed to provide visualisation of multiple evaluation scores so they can be easily interpreted by a user. VEMV takes in the source, reference, and hypothesis files as parameters, and scores the hypotheses using several popular evaluation metrics simultaneously. Scores are produced at both the sentence and dataset level and results are written locally to a series of HTML files that can be viewed on a web browser. The individual scored sentences can easily be inspected using powerful search and selection functions and results can be visualised with graphical representations of the scores and distributions.",
}
@inproceedings{he-etal-2018-sequence,
    title = "Sequence to Sequence Mixture Model for Diverse Machine Translation",
    author = "He, Xuanli  and
      Haffari, Gholamreza  and
      Norouzi, Mohammad",
    editor = "Korhonen, Anna  and
      Titov, Ivan",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-1056",
    doi = "10.18653/v1/K18-1056",
    pages = "583--592",
    abstract = "Sequence to sequence (SEQ2SEQ) models lack diversity in their generated translations. This can be attributed to their limitations in capturing lexical and syntactic variations in parallel corpora, due to different styles, genres, topics, or ambiguity of human translation process. In this paper, we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation diversity and quality by adopting a committee of specialized translation models rather than a single translation model. Each mixture component selects its own training dataset via optimization of the marginal log-likelihood, which leads to a soft clustering of the parallel corpus. Experiments on four language pairs demonstrate the superiority of our mixture model compared to SEQ2SEQ model with the standard and diversity encouraged beam search. Our mixture model incurs negligible additional parameters and no extra computation in the decoding time.",
}
@inproceedings{laubli-etal-2018-machine,
    title = "Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation",
    author = {L{\"a}ubli, Samuel  and
      Sennrich, Rico  and
      Volk, Martin},
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1512",
    doi = "10.18653/v1/D18-1512",
    pages = "4791--4796",
    abstract = "Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese{--}English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.",
}
@inproceedings{nishimura-etal-2018-multisource,
    title = "Multi-Source Neural Machine Translation with Data Augmentation",
    author = "Nishimura, Yuta  and
      Sudoh, Katsuhito  and
      Neubig, Graham  and
      Nakamura, Satoshi",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.7",
    pages = "48--53",
    abstract = "Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these systems achieve large gains in accuracy. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these corpora are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.",
}
@inproceedings{ahrenberg-2017-comparing,
    title = "Comparing Machine Translation and Human Translation: A Case Study",
    author = "Ahrenberg, Lars",
    editor = "Temnikova, Irina  and
      Orasan, Constantin  and
      Pastor, Gloria Corpas  and
      Vogel, Stephan",
    booktitle = "Proceedings of the Workshop Human-Informed Translation and Interpreting Technology",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "Association for Computational Linguistics, Shoumen, Bulgaria",
    url = "https://doi.org/10.26615/978-954-452-042-7_003",
    doi = "10.26615/978-954-452-042-7_003",
    pages = "21--28",
    abstract = "As machine translation technology improves comparisons to human performance are often made in quite general and exaggerated terms. Thus, it is important to be able to account for differences accurately. This paper reports a simple, descriptive scheme for comparing translations and applies it to two translations of a British opinion article published in March, 2017. One is a human translation (HT) into Swedish, and the other a machine translation (MT). While the comparison is limited to one text, the results are indicative of current limitations in MT.",
}
@inproceedings{kunilovskaya-kutuzov-2017-universal,
    title = "{U}niversal {D}ependencies-based syntactic features in detecting human translation varieties",
    author = "Kunilovskaya, Maria  and
      Kutuzov, Andrey",
    editor = "Haji{\v{c}}, Jan",
    booktitle = "Proceedings of the 16th International Workshop on Treebanks and Linguistic Theories",
    year = "2017",
    address = "Prague, Czech Republic",
    url = "https://aclanthology.org/W17-7606",
    pages = "27--36",
}
@inproceedings{baskaya-etal-2017-integrating,
    title = "Integrating Meaning into Quality Evaluation of Machine Translation",
    author = {Ba{\c{s}}kaya, Osman  and
      Yildiz, Eray  and
      Tunao{\u{g}}lu, Doruk  and
      Eren, Mustafa Tolga  and
      Do{\u{g}}ru{\"o}z, A. Seza},
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1020",
    pages = "210--219",
    abstract = "Machine translation (MT) quality is evaluated through comparisons between MT outputs and the human translations (HT). Traditionally, this evaluation relies on form related features (e.g. lexicon and syntax) and ignores the transfer of meaning reflected in HT outputs. Instead, we evaluate the quality of MT outputs through meaning related features (e.g. polarity, subjectivity) with two experiments. In the first experiment, the meaning related features are compared to human rankings individually. In the second experiment, combinations of meaning related features and other quality metrics are utilized to predict the same human rankings. The results of our experiments confirm the benefit of these features in predicting human evaluation of translation quality in addition to traditional metrics which focus mainly on form.",
}
@inproceedings{culo-nitzke-2016-patterns,
    title = "Patterns of Terminological Variation in Post-editing and of Cognate Use in Machine Translation in Contrast to Human Translation",
    author = "{\v{C}}ulo, Oliver  and
      Nitzke, Jean",
    booktitle = "Proceedings of the 19th Annual Conference of the {E}uropean Association for Machine Translation",
    year = "2016",
    url = "https://aclanthology.org/W16-3401",
    pages = "106--114",
}
@inproceedings{rubino-etal-2016-information,
    title = "Information Density and Quality Estimation Features as Translationese Indicators for Human Translation Classification",
    author = "Rubino, Raphael  and
      Lapshinova-Koltunski, Ekaterina  and
      van Genabith, Josef",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1110",
    doi = "10.18653/v1/N16-1110",
    pages = "960--970",
}
@inproceedings{fomicheva-bel-2016-using,
    title = "Using Contextual Information for Machine Translation Evaluation",
    author = "Fomicheva, Marina  and
      Bel, N{\'u}ria",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1437",
    pages = "2755--2761",
    abstract = "Automatic evaluation of Machine Translation (MT) is typically approached by measuring similarity between the candidate MT and a human reference translation. An important limitation of existing evaluation systems is that they are unable to distinguish candidate-reference differences that arise due to acceptable linguistic variation from the differences induced by MT errors. In this paper we present a new metric, UPF-Cobalt, that addresses this issue by taking into consideration the syntactic contexts of candidate and reference words. The metric applies a penalty when the words are similar but the contexts in which they occur are not equivalent. In this way, Machine Translations (MTs) that are different from the human translation but still essentially correct are distinguished from those that share high number of words with the reference but alter the meaning of the sentence due to translation errors. The results show that the method proposed is indeed beneficial for automatic MT evaluation. We report experiments based on two different evaluation tasks with various types of manual quality assessment. The metric significantly outperforms state-of-the-art evaluation systems in varying evaluation settings.",
}
@inproceedings{matsuzaki-etal-2016-translation,
    title = "Translation Errors and Incomprehensibility: a Case Study using Machine-Translated Second Language Proficiency Tests",
    author = "Matsuzaki, Takuya  and
      Fujita, Akira  and
      Todo, Naoya  and
      Arai, Noriko H.",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1440",
    pages = "2771--2776",
    abstract = "This paper reports on an experiment where 795 human participants answered to the questions taken from second language proficiency tests that were translated to their native language. The output of three machine translation systems and two different human translations were used as the test material. We classified the translation errors in the questions according to an error taxonomy and analyzed the participants{'} response on the basis of the type and frequency of the translation errors. Through the analysis, we identified several types of errors that deteriorated most the accuracy of the participants{'} answers, their confidence on the answers, and their overall evaluation of the translation quality.",
}
@inproceedings{couto-vale-etal-2016-automatic,
    title = "Automatic Recognition of Linguistic Replacements in Text Series Generated from Keystroke Logs",
    author = "Couto-Vale, Daniel  and
      Neumann, Stella  and
      Niemietz, Paula",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1574",
    pages = "3617--3623",
    abstract = "This paper introduces a toolkit used for the purpose of detecting replacements of different grammatical and semantic structures in ongoing text production logged as a chronological series of computer interaction events (so-called keystroke logs). The specific case we use involves human translations where replacements can be indicative of translator behaviour that leads to specific features of translations that distinguish them from non-translated texts. The toolkit uses a novel CCG chart parser customised so as to recognise grammatical words independently of space and punctuation boundaries. On the basis of the linguistic analysis, structures in different versions of the target text are compared and classified as potential equivalents of the same source text segment by {`}equivalence judges{'}. In that way, replacements of grammatical and semantic structures can be detected. Beyond the specific task at hand the approach will also be useful for the analysis of other types of spaceless text such as Twitter hashtags and texts in agglutinative or spaceless languages like Finnish or Chinese.",
}
@inproceedings{yuan-etal-2016-mobil,
    title = "{M}o{B}i{L}: A Hybrid Feature Set for Automatic Human Translation Quality Assessment",
    author = "Yuan, Yu  and
      Sharoff, Serge  and
      Babych, Bogdan",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1581",
    pages = "3663--3670",
    abstract = "In this paper we introduce MoBiL, a hybrid Monolingual, Bilingual and Language modelling feature set and feature selection and evaluation framework. The set includes translation quality indicators that can be utilized to automatically predict the quality of human translations in terms of content adequacy and language fluency. We compare MoBiL with the QuEst baseline set by using them in classifiers trained with support vector machine and relevance vector machine learning algorithms on the same data set. We also report an experiment on feature selection to opt for fewer but more informative features from MoBiL. Our experiments show that classifiers trained on our feature set perform consistently better in predicting both adequacy and fluency than the classifiers trained on the baseline feature set. MoBiL also performs well when used with both support vector machine and relevance vector machine algorithms.",
}
@inproceedings{li-etal-2015-machine,
    title = "A Machine Learning Method to Distinguish Machine Translation from Human Translation",
    author = "Li, Yitong  and
      Wang, Rui  and
      Zhao, Hai",
    editor = "Zhao, Hai",
    booktitle = "Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",
    month = oct,
    year = "2015",
    address = "Shanghai, China",
    url = "https://aclanthology.org/Y15-2041",
    pages = "354--360",
}
@inproceedings{geng-2015-improving,
    title = "On Improving the Human Translation Process by Using {MT} Technologies under a Cognitive Framework",
    author = "Geng, Xinhui",
    editor = "Babych, Bogdan  and
      Eberle, Kurt  and
      Lambert, Patrik  and
      Rapp, Reinhard  and
      Banchs, Rafael E.  and
      Costa-juss{\`a}, Marta R.",
    booktitle = "Proceedings of the Fourth Workshop on Hybrid Approaches to Translation ({H}y{T}ra)",
    month = jul,
    year = "2015",
    address = "Beijing",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4111",
    doi = "10.18653/v1/W15-4111",
    pages = "63",
}
@inproceedings{kumar-etal-2015-machine,
    title = "A machine-assisted human translation system for technical documents",
    author = "Kumar, Vishwajeet  and
      Kulkarni, Ashish  and
      Singh, Pankaj  and
      Ramakrishnan, Ganesh  and
      Arnaal, Ganesh",
    booktitle = "Proceedings of Machine Translation Summit XV: User Track",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-users.20",
}
@inproceedings{alabau-leiva-2014-proofreading,
    title = "Proofreading Human Translations with an {E}-pen",
    author = "Alabau, Vicent  and
      Leiva, Luis A.",
    editor = "Germann, Ulrich  and
      Carl, Michael  and
      Koehn, Philipp  and
      Sanchis-Trilles, Germ{\'a}n  and
      Casacuberta, Francisco  and
      Hill, Robin  and
      O{'}Brien, Sharon",
    booktitle = "Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-0302",
    doi = "10.3115/v1/W14-0302",
    pages = "10--15",
}
@inproceedings{turchi-negri-2014-automatic,
    title = "Automatic Annotation of Machine Translation Datasets with Binary Quality Judgements",
    author = "Turchi, Marco  and
      Negri, Matteo",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/473_Paper.pdf",
    pages = "1788--1792",
    abstract = "The automatic estimation of machine translation (MT) output quality is an active research area due to its many potential applications (e.g. aiding human translation and post-editing, re-ranking MT hypotheses, MT system combination). Current approaches to the task rely on supervised learning methods for which high-quality labelled data is fundamental. In this framework, quality estimation (QE) has been mainly addressed as a regression problem where models trained on (source, target) sentence pairs annotated with continuous scores (in the [0-1] interval) are used to assign quality scores (in the same interval) to unseen data. Such definition of the problem assumes that continuous scores are informative and easily interpretable by different users. These assumptions, however, conflict with the subjectivity inherent to human translation and evaluation. On one side, the subjectivity of human judgements adds noise and biases to annotations based on scaled values. This problem reduces the usability of the resulting datasets, especially in application scenarios where a sharp distinction between Â“goodÂ” and Â“badÂ” translations is needed. On the other side, continuous scores are not always sufficient to decide whether a translation is actually acceptable or not. To overcome these issues, we present an automatic method for the annotation of (source, target) pairs with binary judgements that reflect an empirical, and easily interpretable notion of quality. The method is applied to annotate with binary judgements three QE datasets for different language combinations. The three datasets are combined in a single resource, called BinQE, which can be freely downloaded from \url{http://hlt.fbk.eu/technologies/binqe}.",
}
@inproceedings{more-climent-2014-machine,
    title = "Machine Translationness: Machine-likeness in Machine Translation Evaluation",
    author = "Mor{\'e}, Joaquim  and
      Climent, Salvador",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/506_Paper.pdf",
    pages = "54--61",
    abstract = "Machine translationness (MTness) is the linguistic phenomena that make machine translations distinguishable from human translations. This paper intends to present MTness as a research object and suggests an MT evaluation method based on determining whether the translation is machine-like instead of determining its human-likeness as in evaluation current approaches. The method rates the MTness of a translation with a metric, the MTS (Machine Translationness Score). The MTS calculation is in accordance with the results of an experimental study on machine translation perception by common people. MTS proved to correlate well with human ratings on translation quality. Besides, our approach allows the performance of cheap evaluations since expensive resources (e.g. reference translations, training corpora) are not needed. The paper points out the challenge of dealing with MTness as an everyday phenomenon caused by the massive use of MT.",
}
@inproceedings{ocurran-2014-translation,
    title = "Translation quality in post-edited versus human-translated segments: a case study",
    author = "O{'}Curran, Elaine",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-wptp.9",
    pages = "113--118",
    abstract = "We analyze the linguistic quality results for a post-editing productivity test that contains a 3:1 ratio of post-edited segments versus human-translated segments, in order to assess if there is a difference in the final translation quality of each segment type and also to investigate the type of errors that are found in each segment type. Overall, we find that the human-translated segments contain more errors per word than the post-edited segments and although the error categories logged are similar across the two segment types, the most notable difference is that the number of stylistic errors in the human translations is 3 times higher than in the post-edited translations.",
}
@inproceedings{specia-shah-2014-predicting,
    title = "Predicting human translation quality",
    author = "Specia, Lucia  and
      Shah, Kashif",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.22",
    pages = "288--300",
    abstract = "We present a first attempt at predicting the quality of translations produced by human, professional translators. We examine datasets annotated for quality at sentence- and word-level for four language pairs and provide experiments with prediction models for these datasets. We compare the performance of such models against that of models built from machine translations, highlighting a number of challenges in estimating quality and detecting errors in human translations.",
}
@inproceedings{stymne-etal-2012-eye,
    title = "Eye Tracking as a Tool for Machine Translation Error Analysis",
    author = "Stymne, Sara  and
      Danielsson, Henrik  and
      Bremin, Sofia  and
      Hu, Hongzhan  and
      Karlsson, Johanna  and
      Lillkull, Anna Prytz  and
      Wester, Martin",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/192_Paper.pdf",
    pages = "1121--1126",
    abstract = "We present a preliminary study where we use eye tracking as a complement to machine translation (MT) error analysis, the task of identifying and classifying MT errors. We performed a user study where subjects read short texts translated by three MT systems and one human translation, while we gathered eye tracking data. The subjects were also asked comprehension questions about the text, and were asked to estimate the text quality. We found that there are a longer gaze time and a higher number of fixations on MT errors, than on correct parts. There are also differences in the gaze time of different error types, with word order errors having the longest gaze time. We also found correlations between eye tracking data and human estimates of text quality. Overall our study shows that eye tracking can give complementary information to error analysis, such as aiding in ranking error types for seriousness.",
}
@inproceedings{carl-2012-translog,
    title = "Translog-{II}: a Program for Recording User Activity Data for Empirical Reading and Writing Research",
    author = "Carl, Michael",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/614_Paper.pdf",
    pages = "4108--4112",
    abstract = "This paper presents a novel implementation of Translog-II. Translog-II is a Windows-oriented program to record and study reading and writing processes on a computer. In our research, it is an instrument to acquire objective, digital data of human translation processes. As their predecessors, Translog 2000 and Translog 2006, also Translog-II consists of two main components: Translog-II Supervisor and Translog-II User, which are used to create a project file, to run a text production experiments (a user reads, writes or translates a text) and to replay the session. Translog produces a log files which contains all user activity data of the reading, writing, or translation session, and which can be evaluated by external tools. While there is a large body of translation process research based on Translog, this paper gives an overview of the Translog-II functions and its data visualization options.",
}
@inproceedings{stuker-etal-2012-kit,
    title = "The {KIT} Lecture Corpus for Speech Translation",
    author = {St{\"u}ker, Sebastian  and
      Kraft, Florian  and
      Mohr, Christian  and
      Herrmann, Teresa  and
      Cho, Eunah  and
      Waibel, Alex},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/1121_Paper.pdf",
    pages = "3409--3414",
    abstract = "Academic lectures offer valuable content, but often do not reach their full potential audience due to the language barrier. Human translations of lectures are too expensive to be widely used. Speech translation technology can be an affordable alternative in this case. State-of-the-art speech translation systems utilize statistical models that need to be trained on large amounts of in-domain data. In order to support the KIT lecture translation project in its effort to introduce speech translation technology in KIT's lecture halls, we have collected a corpus of German lectures at KIT. In this paper we describe how we recorded the lectures and how we annotated them. We further give detailed statistics on the types of lectures in the corpus and its size. We collected the corpus with the purpose in mind that it should not just be suited for training a spoken language translation system the traditional way, but should also enable us to research techniques that enable the translation system to automatically and autonomously adapt itself to the varying topics and speakers of lectures",
}
@inproceedings{carl-2012-critt,
    title = "The {CRITT} {TPR}-{DB} 1.0: A Database for Empirical Human Translation Process Research",
    author = "Carl, Michael",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Workshop on Post-Editing Technology and Practice",
    month = oct # " 28",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-wptp.1",
    abstract = "This paper introduces a publicly available database of recorded translation sessions for Translation Process Research (TPR). User activity data (UAD) of translators behavior was collected over the past 5 years in several translation studies with Translog 1 , a data acquisition software which logs keystrokes and gaze data during text reception and production. The database compiles this data into a consistent format which can be processed by various visualization and analysis tools.",
}
@inproceedings{parton-etal-2012-lost,
    title = "Lost {\&} Found in Translation: Impact of Machine Translated Results on Translingual Information Retrieval",
    author = "Parton, Kristen  and
      Habash, Nizar  and
      McKeown, Kathleen",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.12",
    abstract = "In an ideal cross-lingual information retrieval (CLIR) system, a user query would generate a search over documents in a different language and the relevant results would be presented in the user{'}s language. In practice, CLIR systems are typically evaluated by judging result relevance in the document language, to factor out the effects of translating the results using machine translation (MT). In this paper, we investigate the influence of four different approaches for integrating MT and CLIR on both retrieval accuracy and user judgment of relevancy. We create a corpus with relevance judgments for both human and machine translated results, and use it to quantify the effect that MT quality has on end-to-end relevance. We find that MT errors result in a 16-39{\%} decrease in mean average precision over the ground truth system that uses human translations. MT errors also caused relevant sentences to appear irrelevant {--} 5-19{\%} of sentences were relevant in human translation, but were judged irrelevant in MT. To counter this degradation, we present two hybrid retrieval models and two automatic MT post-editing techniques and show that these approaches substantially mitigate the errors and improve the end-to-end relevance.",
}
@inproceedings{egan-2012-machine,
    title = "Machine Translation Revisited: An Operational Reality Check",
    author = "Egan, Kathleen",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Government MT User Program",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-government.5",
    abstract = "The government and the research community have strived for the past few decades to develop machine translation capabilities. Historically, DARPA took the lead in the grand challenge aiming at surpassing human translation quality. While we have made strides from rule based, to statistical and hybrid machine translation engines, we cannot rely solely on machine translation to overcome the language barrier and accomplish the mission. Machine Translation is often misunderstood or misplaced in the operational settings as expectations are unrealistic and optimization not achieved. With the increase in volume, variety and velocity of data, new paradigms are needed when choosing machine translation software and embedding it into a business process so as to achieve the operational goals. The talk will focus on the operational requirements and frame where, when and how to use machine translation. We will also outline some gaps and suggest new areas for research, development, and implementation.",
}
@inproceedings{marcu-2012-new,
    title = "A New Method for Automatic Translation Scoring-{H}y{TER}",
    author = "Marcu, Daniel",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Government MT User Program",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-government.9",
    abstract = "It is common knowledge that translation is an ambiguous, 1-to-n mapping process, but to date, our community has produced no empirical estimates of this ambiguity. We have developed an annotation tool that enables us to create representations that compactly encode an exponential number of correct translations for a sentence. Our findings show that naturally occurring sentences have billions of translations. Having access to such large sets of meaning-equivalent translations enables us to develop a new metric, HyTER, for translation accuracy. We show that our metric provides better estimates of machine and human translation accuracy than alternative evaluation metrics using data from the most recent Open MT NIST evaluation and we discuss how HyTER representations can be used to inform a data-driven inquiry into natural language semantics.",
}
@inproceedings{lin-etal-2010-composing,
    title = "Composing Human and Machine Translation Services: Language Grid for Improving Localization Processes",
    author = "Lin, Donghui  and
      Murakami, Yoshiaki  and
      Ishida, Toru  and
      Murakami, Yohei  and
      Tanaka, Masahiro",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/317_Paper.pdf",
    abstract = "With the development of the Internet environments, more and more language services become accessible for common people. However, the gap between human translators and machine translators remains huge especially for the domain of localization processes that requires high translation quality. Although efforts of combining human and machine translators for supporting multilingual communication have been reported in previous research, how to apply such approaches for improving localization processes are rarely discussed. In this paper, we aim at improving localization processes by composing human and machine translation services based on the Language Grid, which is a language service platform that we have developed. Further, we conduct experiments to compare the translation quality and translation cost using several translation processes, including absolute machine translation processes, absolute human translation processes and translation processes by human and machine translation services. The experiment results show that composing monolingual roles and dictionary services improves the translation quality of machine translators, and that collaboration of human and machine translators is possible to reduce the cost comparing with the absolute bilingual human translation. We also discuss the generality of the experimental results and further challenging issues of the proposed localization processes.",
}
@inproceedings{carl-2010-computational,
    title = "A computational framework for a cognitive model of human translation processes",
    author = "Carl, Michael",
    booktitle = "Proceedings of Translating and the Computer 32",
    month = nov # " 18-19",
    year = "2010",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2010.tc-1.10",
}
@inproceedings{wendt-2010-better,
    title = "Better translations with user collaboration {--} Integrated {MT} at {M}icrosoft",
    author = "Wendt, Chris",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Commercial MT User Program",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-commercial.10",
    abstract = "This paper outlines the methodologies Microsoft has deployed for seamless integration of human translation into the translation workflow, and describes a variety of methods to gather and collect human translation data. Increased amounts of parallel training data help to enhance the translation quality of the statistical MT system in use at Microsoft. The presentation covers the theory, the technical methodology as well as the experiences Microsoft has with the implementation, and practical use of such a system. Included is a discussion of the factors influencing the translation quality of a statistical MT system, a short description of the feedback collection mechanism in use at Microsoft, and the metrics it observed on its MT deployments.",
}
@inproceedings{koehn-2009-human,
    title = "Human translation and machine translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of the 6th International Workshop on Spoken Language Translation: Plenaries",
    month = dec # " 1-2",
    year = "2009",
    address = "Tokyo, Japan",
    url = "https://aclanthology.org/2009.iwslt-keynotes.1",
}
@inproceedings{glenn-etal-2008-management,
    title = "Management of Large Annotation Projects Involving Multiple Human Judges: a Case Study of {GALE} Machine Translation Post-editing",
    author = "Glenn, Meghan Lammie  and
      Strassel, Stephanie  and
      Friedman, Lauren  and
      Lee, Haejoong  and
      Medero, Shawn",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/751_paper.pdf",
    abstract = "Managing large groups of human judges to perform any annotation task is a challenge. Linguistic Data Consortium coordinated the creation of manual machine translation post-editing results for the DARPA Global Autonomous Language Exploration Program. Machine translation is one of three core technology components for GALE, which includes an annual MT evaluation administered by National Institute of Standards and Technology. Among the training and test data LDC creates for the GALE program are gold standard translations for system evaluation. The GALE machine translation system evaluation metric is edit distance, measured by HTER (human translation edit rate), which calculates the minimum number of changes required for highly-trained human editors to correct MT output so that it has the same meaning as the reference translation. LDC has been responsible for overseeing the post-editing process for GALE. We describe some of the accomplishments and challenges of completing the post-editing effort, including developing a new web-based annotation workflow system, and recruiting and training human judges for the task. In addition, we suggest that the workflow system developed for post-editing could be ported efficiently to other annotation efforts.",
}
@inproceedings{song-strassel-2008-entity,
    title = "Entity Translation and Alignment in the {ACE}-07 {ET} Task",
    author = "Song, Zhiyi  and
      Strassel, Stephanie",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/551_paper.pdf",
    abstract = "Entities - people, organizations, locations and the like - have long been a central focus of natural language processing technology development, since entities convey essential content in human languages. For multilingual systems, accurate translation of named entities and their descriptors is critical. LDC produced Entity Translation pilot data to support the ACE ET 2007 Evaluation and the current paper delves more deeply into the entity alignment issue across languages, combining the automatic alignment techniques developed for ACE-07 with manual alignment. Altogether 84{\%} of the Chinese-English entity mentions and 74{\%} of the Arabic-English entity mentions are perfect aligned. The results of this investigation offer several important insights. Automatic alignment algorithms predicted that perfect alignment for the ET corpus was likely to be no greater than 55{\%}; perfect alignment on the 15 pilot documents was predicted at 62.5{\%}. Our results suggest the actual perfect alignment rate is substantially higher (82{\%} average, 92{\%} for NAM entities). The careful analysis of alignment errors also suggests strategies for human translation to support the ET task; for instance, translators might be given additional guidance about preferred treatments of name versus nominal translation. These results can also contribute to refined methods of evaluating ET systems.",
}
@inproceedings{hamon-mostefa-2008-experimental,
    title = "An Experimental Methodology for an End-to-End Evaluation in Speech-to-Speech Translation",
    author = "Hamon, Olivier  and
      Mostefa, Djamel",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/900_paper.pdf",
    abstract = "This paper describes the evaluation methodology used to evaluate the TC-STAR speech-to-speech translation (SST) system and the results from the third year of the project. It follows the results presented in Hamon (2007), dealing with the first end-to-end evaluation of the project. In this paper, we try to experiment with the methodology and the protocol during a second end-to-end evaluation, by comparing outputs from the TC-STAR system with interpreters from the European parliament. For this purpose, we test different criteria of evaluation and type of questions within a comprehension test. The results show that interpreters do not translate all the information (as opposed to the automatic system), but the quality of SST is still far from that of human translation. The experimental comprehension test used provides new information to study the quality of automatic systems, but without settling the issue of which protocol is the best. This depends on what the evaluator wants to know about the SST: either to have a subjective end-user evaluation or a more objective one.",
}
@inproceedings{abekawa-kageura-2008-constructing,
    title = "Constructing a Corpus that Indicates Patterns of Modification between Draft and Final Translations by Human Translators",
    author = "Abekawa, Takeshi  and
      Kageura, Kyo",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/512_paper.pdf",
    abstract = "In human translation, translators first make draft translations and then modify and edit them. In the case of experienced translators, this process involves the use of wide-ranging expert knowledge, which has mostly remained implicit so far. Describing the difference between draft and final translations, therefore, should contribute to making this knowledge explicit. If we could clarify the expert knowledge of translators, hopefully in a computationally tractable way, we would be able to contribute to the automatic notification of awkward translations to assist inexperienced translators, improving the quality of MT output, etc. Against this backdrop, we have started constructing a corpus that indicates patterns of modification between draft and final translations made by human translators. This paper reports on our progress to date.",
}
@inproceedings{madnani-etal-2008-multiple,
    title = "Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization",
    author = "Madnani, Nitin  and
      Resnik, Philip  and
      Dorr, Bonnie J.  and
      Schwartz, Richard",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-papers.13",
    pages = "143--152",
    abstract = "Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases.",
}
@inproceedings{friedman-strassel-2008-identifying,
    title = "Identifying Common Challenges for Human and Machine Translation: A Case Study from the {GALE} Program",
    author = "Friedman, Lauren  and
      Strassel, Stephanie",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-govandcom.10",
    pages = "364--369",
    abstract = "The dramatic improvements shown by statistical machine translation systems in recent years clearly demonstrate the benefits of having large quantities of manually translated parallel text for system training and development. And while many competing evaluation metrics exist to evaluate MT technology, most of those methods also crucially rely on the existence of one or more high quality human translations to benchmark system performance. Given the importance of human translations in this framework, understanding the particular challenges of human translation-for-MT is key, as is comprehending the relative strengths and weaknesses of human versus machine translators in the context of an MT evaluation. Vanni (2000) argued that the metric used for evaluation of competence in human language learners may be applicable to MT evaluation; we apply similar thinking to improve the prediction of MT performance, which is currently unreliable. In the current paper we explore an alternate model based upon a set of genre-defining features that prove to be consistently challenging for both humans and MT systems.",
}
@inproceedings{powell-blodgett-2008-use,
    title = "The Use of Machine-generated Transcripts during Human Translation",
    author = "Powell, Allison L.  and
      Blodgett, Allison",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-govandcom.19",
    pages = "427--434",
    abstract = "At the request of the USG National Virtual Translation Center, the University of Maryland Center for Advanced Study of Language conducted a study that assessed the role of several factors mediating transcript usefulness during translation tasks. These factors included source language (Mandarin or Modern Standard Arabic), native speaker status of the translators, transcript quality (low or moderate word error rate), and transcript functionality (static or dynamic). Using 54 Mandarin and 54 Arabic translators (half native speakers in each language) and broadcast news clips for input, the study demonstrated that translation environments that provide dynamic transcripts with low or moderate word error rates are likely to improve performance (measured as integrated speed and accuracy scores) among non-native speakers without decreasing performance among native speakers.",
}
@inproceedings{van-ess-dykema-etal-2008-embedding,
    title = "Embedding Technology at the front end of the Human Translation Workflow: An {NVTC} Vision",
    author = "van Ess-Dykema, Carol  and
      Gigley, Helen G.  and
      Lewis, Stephen  and
      Vancho Bannister, Emily",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-govandcom.24",
    pages = "457--463",
    abstract = "This paper describes the strategic vision for a new translation management workflow for the US Government{'}s National Virtual Translation Center (NVTC). The paper also describes past, current, and planned experiments validating the vision, along with experiment results to-date. The most salient features of the new workflow include the embedding of translation technology at the front end of the workflow (e.g., translation memory technology, specialized lexicons, and machine translation), technology-generated {``}seed translation{''}, a new human work role called {``}paralinguist{''} to assess the {``}seed translation{''} and assign an appropriate translator/post-editor, and new human translation strategies including federated search of online dictionaries and collaborative translation.",
}
@inproceedings{cyrus-2006-building,
    title = "Building a resource for studying translation shifts",
    author = "Cyrus, Lea",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)",
    month = may,
    year = "2006",
    address = "Genoa, Italy",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2006/pdf/622_pdf.pdf",
    abstract = "This paper describes an interdisciplinary approach which brings together the fields of corpus linguistics and translation studies. It presents ongoing work on the creation of a corpus resource in which translation shifts are explicitly annotated. Translation shifts denote departures from formal correspondence between source and target text, i.e. deviations that have occurred during the translation process. A resource in which such shifts are annotated in a systematic way will make it possible to study those phenomena that need to be addressed if machine translation output is to resemble human translation. The resource described in this paper contains English source texts (parliamentary proceedings) and their German translations. The shift annotation is based on predicate-argument structures and proceeds in two steps: first, predicates and their arguments are annotated monolingually in a straightforward manner. Then, the corresponding English and German predicates and arguments are aligned with each other. Whenever a shift - mainly grammatical or semantic - has occurred, the alignment is tagged accordingly.",
}
@inproceedings{uszkoreit-2005-ontologies,
    title = "Ontologies for Crosslingual Applications",
    author = "Uszkoreit, Hans",
    booktitle = "Workshop on Semantic Web technologies for machine translation",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-swtmt.1",
    abstract = "Human translation is based on linguistic and extralinguistic knowledge. Despite promising pioneering advances, knowledge-based machine translation has remained a tempting vision. The bottleneck has been the engineering of sufficiently comprehensive bodies of relevant knowledge The Semantic Web offers opportunities for the gradual evolution of a global heterogeneous knowledge base. The immediate target has been the modelling of certain knowledge domains by practical ontologies. In the talk we will demonstrate the utilization of ontological knowledge indifferent crosslingual applications reaching from crosslingual document retrieval via crosslingual question answering to complex information services involving several crosslingual functionalities, including machine translation. We will then discuss the ramifications of this development and of the evolution of the World Wide Web for future directions in both statistical and rule-based machine translation.",
}
@inproceedings{majithia-etal-2005-rapid,
    title = "Rapid Ramp-up for Statistical Machine Translation: Minimal Training for Maximal Coverage",
    author = "Majithia, Hemali  and
      Rennart, Philip  and
      Tzoukermann, Evelyne",
    booktitle = "Proceedings of Machine Translation Summit X: Posters",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-posters.17",
    pages = "438--444",
    abstract = "This paper investigates optimal ways to get maximal coverage from minimal input training corpus. In effect, it seems antagonistic to think of minimal input training with a statistical machine translation system. Since statistics work well with repetition and thus capture well highly occurring words, one challenge has been to figure out the optimal number of {``}new{''} words that the system needs to be appropriately trained. Additionally, the goal is to minimize the human translation time for training a new language. In order to account for rapid ramp-up translation, we ran several experiments to figure out the minimal amount of data to obtain optimal translation results.",
}
@inproceedings{zhang-etal-2005-building-annotated,
    title = "Building an Annotated {J}apanese-{C}hinese Parallel Corpus {--} A Part of {NICT} Multilingual Corpora",
    author = "Zhang, Yujie  and
      Uchimoto, Kiyotaka  and
      Ma, Qing  and
      Isahara, Hitoshi",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.10",
    pages = "71--78",
    abstract = "We are constricting a Japanese-Chinese parallel corpus, which is a part of the NICT Multilingual Corpora. The corpus is general domain, of large scale of about 40,000 sentence pairs, long sentences, annotated with detailed information and high quality. To the best of our knowledge, this will be the first annotated Japanese-Chinese parallel corpus in the world. We created the corpus by selecting Japanese sentences from Mainichi Newspaper and then manually translating them into Chinese. We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels. This paper describes the specification in human translation and detailed information annotation, and the tools we developed in the project. The experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction.",
}
@inproceedings{jelinek-2004-modern,
    title = "{M}odern {MT} Systems and the Myth of Human Translation: Real World Status Quo",
    author = "Jelinek, Richard",
    booktitle = "Proceedings of Translating and the Computer 26",
    month = nov # " 18-19",
    year = "2004",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2004.tc-1.2",
}
@inproceedings{elliott-etal-2004-fluency,
    title = "A fluency error categorization scheme to guide automated machine translation evaluation",
    author = "Elliott, Debbie  and
      Hartley, Anthony  and
      Atwell, Eric",
    editor = "Frederking, Robert E.  and
      Taylor, Kathryn B.",
    booktitle = "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = sep # " 28 - " # oct # " 2",
    year = "2004",
    address = "Washington, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/978-3-540-30194-3_8",
    pages = "64--73",
    abstract = "Existing automated MT evaluation methods often require expert human translations. These are produced for every language pair evaluated and, due to this expense, subsequent evaluations tend to rely on the same texts, which do not necessarily reflect real MT use. In contrast, we are designing an automated MT evaluation system, intended for use by post-editors, purchasers and developers, that requires nothing but the raw MT output. Furthermore, our research is based on texts that reflect corporate use of MT. This paper describes our first step in system design: a hierarchical classification scheme of fluency errors in English MT output, to enable us to identify error types and frequencies, and guide the selection of errors for automated detection. We present results from the statistical analysis of 20,000 words of MT output, manually annotated using our classification scheme, and describe correlations between error frequencies and human scores for fluency and adequacy.",
}
@inproceedings{reeder-2004-investigation,
    title = "Investigation of intelligibility judgments",
    author = "Reeder, Florence",
    editor = "Frederking, Robert E.  and
      Taylor, Kathryn B.",
    booktitle = "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = sep # " 28 - " # oct # " 2",
    year = "2004",
    address = "Washington, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/978-3-540-30194-3_25",
    pages = "227--235",
    abstract = "This paper describes an intelligibility snap-judgment test. In this exercise, participants are shown a series of human translations and machine translations and are asked to determine whether the author was human or machine. The experiment shows that snap judgments on intelligibility are made successfully and that system rankings on snap judgments are consistent with more detailed intelligibility measures. In addition to demonstrating a quick intelligibility judgment, representing on a few minutes time of each participant, it details the types of errors which led to the snap judgments.",
}
@inproceedings{culy-riehemann-2003-limits,
    title = "The limits of n-gram translation evaluation metrics",
    author = "Culy, Christopher  and
      Riehemann, Susanne Z.",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.10",
    abstract = "N-gram measures of translation quality, such as BLEU and the related NIST metric, are becoming increasingly important in machine translation, yet their behaviors are not fully understood. In this paper we examine the performance of these metrics on professional human translations into German of two literary genres, the Bible and Tom Sawyer. The most surprising result is that some machine translations outscore some professional human translations. In addition, it can be difficult to distinguish some other human translations from machine translations with only two reference translations; with four reference translations it is much easier. Our results lead us to conclude that much care must be taken in using n-gram measures in formal evaluations of machine translation quality, though they are still valuable as part of the iterative development cycle.",
}
@inproceedings{karagol-ayan-etal-2003-acquisition,
    title = "Acquisition of bilingual {MT} lexicons from {OCR}ed dictionaries",
    author = "Karagol-Ayan, Burcu  and
      Doermann, David  and
      Dorr, Bonnie J.",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.28",
    abstract = "This paper describes an approach to analyzing the lexical structure of OCRed bilingual dictionaries to construct resources suited for machine translation of low-density languages, where online resources are limited. A rule-based, an HMM-based, and a post-processed HMM-based method are used for rapid construction of MT lexicons based on systematic structural clues provided in the original dictionary. We evaluate the effectiveness of our techniques, concluding that: (1) the rule-based method performs better with dictionaries where the font is not an important distinguishing feature for determining information types; (2) the post-processed stochastic method improves the results of the stochastic method for phrasal entries; and (3) Our resulting bilingual lexicons are comprehensive enough to provide the basis for reasonable translation results when compared to human translations.",
}
@inproceedings{popescu-belis-2003-experiment,
    title = "An experiment in comparative evaluation: humans vs. computers",
    author = "Popescu-Belis, Andrei",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.41",
    abstract = "This paper reports results from an experiment that was aimed at comparing evaluation metrics for machine translation. Implemented as a workshop at a major conference in 2002, the experiment defined an evaluation task, description of the metrics, as well as test data consisting of human and machine translations of two texts. Several metrics, either applicable by human judges or automated, were used, and the overall results were analyzed. It appeared that most human metrics and automated metrics provided in general consistent rankings of the various candidate translations; the ranking of the human translations matched the one provided by translation professionals; and human translations were distinguished from machine translations.",
}
@inproceedings{reeder-2001-one,
    title = "In one hundred words or less",
    author = "Reeder, Florence",
    editor = "Hovy, Eduard  and
      King, Margaret  and
      Manzi, Sandra  and
      Reeder, Florence",
    booktitle = "Workshop on MT Evaluation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-eval.7",
    abstract = "This paper reports on research which aims to test the efficacy of applying automated evaluation techniques, originally designed for human second language learners, to machine translation (MT) system evaluation. We believe that such evaluation techniques will provide insight into MT evaluation, MT development, the human translation process and the human language learning process. The experiment described here looks only at the intelligibility of MT output. The evaluation technique is derived from a second language acquisition experiment that showed that assessors can differentiate native from non-native language essays in less than 100 words. Particularly illuminating for our purposes is the set of factor on which the assessors made their decisions. We duplicated this experiment to see if similar criteria could be elicited from duplicating the test using both human and machine translation outputs in the decision set. The encouraging results of this experiment, along with an analysis of language factors contributing to the successful outcomes, is presented here.",
}
@inproceedings{white-forner-2001-predicting,
    title = "Predicting {MT} fidelity from noun-compound handling",
    author = "White, John  and
      Forner, Monika",
    editor = "Hovy, Eduard  and
      King, Margaret  and
      Manzi, Sandra  and
      Reeder, Florence",
    booktitle = "Workshop on MT Evaluation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-eval.11",
    abstract = "Approaches to the automation of machine translation (MT) evaluation have attempted, or presumed, to connect some rapidly measurable phenomenon with general attributes of the MT output and/or system. In particular, measurements of the fluency of output are often asserted to be predictive of the usefulness of MT output in information-intensive, downstream tasks. The connections between the fluency ({``}intelligibility{''}) of translation and its informational adequacy ({``}fidelity{''}) are not actually straightforward. This paper discussed a small experiment in isolating a particular contrastive linguistic phenomena common to both French-English and Spanish-English pairs, and attempts to associate that behavior in machine and human translations with known fidelity properties of those translations. Our results show a definite correlative trend.",
}
@inproceedings{king-1999-transrouter,
    title = "{T}rans{R}outer : a decision support tool for translation managers",
    author = "King, Margaret",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.40",
    pages = "272--277",
    abstract = "Translation managers often have to decide on the most appropriate way to deal with a translation project. Possible options may include human translation, translation using a specific terminology resource, translation in interaction with a translation memory system, and machine translation. The decision making involved is complex, and it is not always easy to decide by inspection whether a specific text lends itself to certain kinds of treatment. TransRouter supports the decision making by offering a suite of computer based tools which can be used to analyse the text to be translated. Some tools, such as the word counter, the repetition detector, the sentence length estimator and the sentence simplicity checker look at characteristics of the text itself. A version comparison tool compares the new text to previously translated texts. Other tools, such as the unknown terms detector and the translation memory coverage estimator, estimate overlap between the text and a set of known resources. The information gained, combined with further information provided by the user, is input to a decision kernel which calculates possible routes towards achieving the translation together with their cost and consequences on translation quality. The user may influence the kernel by, for example, specifying particular resources or refining routes under investigation. The final decision on how to treat the project rests with the translation manager.",
}
@inproceedings{cote-1998-system,
    title = "System description/demo of {A}lis {T}ranslation {S}olutions: overview",
    author = "C{\^o}t{\'e}, Nathalie",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: System Descriptions",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_44",
    pages = "494--497",
    abstract = "Part software, part process, Alis Translation Solutions (ATS) address the language barrier by tightly integrating a variety of language tools and services which include machine and human translation, on-line dictionaries, search engines, workflow and management tools. During the AMTA-98 conference, Alis Technologies is demonstrating various applications of ATS: Web and Intranet Publishing, Web Browsing, Company Document Circulation, E-mail Communication and Multilingual Site Search.",
}
@inproceedings{westfall-1998-integrating,
    title = "Integrating tools with the translation process",
    author = "Westfall, Edith R.",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: System Descriptions",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_46",
    pages = "501--505",
    abstract = "Translation tools can be integrated with the translation process with the goal and result of increasing consistency, reusing previous translations, and decreasing the amount of time needed to put a product on the market. This system demonstration will follow a document through the translation cycle utilizing a combination of TRADOS Translator{'}s Workbench 2.0 (translation memory), machine translation, and human translation.",
}
@inproceedings{godden-1998-machine,
    title = "Machine translation in context",
    author = "Godden, Kurt",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_15",
    pages = "158--163",
    abstract = "The Controlled Automotive Service Language project at General Motors is combining machine translation (MT) with a variety of other language technologies into an existing translation environment. In keeping with the theme of this conference, this report elaborates on the elements of this mixture, and how they are being blended together to form a coordinated whole. The primary concept is that machine translation cannot be viewed independently of the context in which it will be used. That entire context must be prepared and managed in order to accommodate MT without undue business risk. Further, until high-quality MT is available in a much wider variety of languages, any MT production application is likely to co-exist with traditional human translation, which requires additional considerations.",
}
@inproceedings{loehr-1998-simultaneous,
    title = "Can simultaneous interpretation help machine translation?",
    author = "Loehr, Dan",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_20",
    pages = "213--224",
    abstract = "It is well known that Machine Translation (MT) has not approached the quality of human translations. It has also been noted that MT research has largely ignored the work of professionals and researchers in the field of translation, and that MT might benefit from collaboration with this field. In this paper, I look at a specialized type of translation, Simultaneous Interpretation (SI), in the light of possible applications to MT. I survey the research and practice of SI, and note that explanatory analyses of SI do not yet exist. However, descriptive analyses do, arrived at through anecdotal, empirical, and model-based methods. These descriptive analyses include {``}techniques{''} humans use for interpreting, and I suggest possible ways MT might use these techniques. I conclude by noting further questions which must be answered before we can fully understand SI, and how it might help MT.",
}
@inproceedings{taylor-white-1998-predicting,
    title = "Predicting what {MT} is good for: user judgments and task performance",
    author = "Taylor, Kathryn  and
      White, John",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_33",
    pages = "364--373",
    abstract = "As part of the Machine Translation (MT) Proficiency Scale project at the US Federal Intelligent Document Understanding Laboratory (FIDUL), Litton PRC is developing a method to measure MT systems in terms of the tasks for which their output may be successfully used. This paper describes the development of a task inventory, i.e., a comprehensive list of the tasks analysts perform with translated material and details the capture of subjective user judgments and insights about MT samples. Also described are the user exercises conducted using machine and human translation samples and the assessment of task performance. By analyzing translation errors, user judgments about errors that interfere with task performance, and user task performance results, we isolate source language patterns which produce output problems. These patterns can then be captured in a single diagnostic test set, to be easily applied to any new Japanese-English system to predict the utility of its output.",
}
@inproceedings{nagao-1997-machine,
    title = "Machine Translation Through Language Understanding",
    author = "Nagao, Makoto",
    editor = "Teller, Virginia  and
      Sundheim, Beth",
    booktitle = "Proceedings of Machine Translation Summit VI: Plenaries",
    month = oct # " 29 {--} " # nov # " 1",
    year = "1997",
    address = "San Diego, California",
    url = "https://aclanthology.org/1997.mtsummit-plenaries.11",
    pages = "41--49",
    abstract = "In this paper is described a general framework of a next generation machine translation system which translates a text not sentence by sentence but by considering inter-sentential discourse. The method is a step closer to human translation than the present-day machine translation systems. Particularly important are a detailed discourse analysis and a flexible text generation by using information obtained from the discourse analysis.",
}
@inproceedings{bedard-1996-juggling,
    title = "Juggling with words: some insights from the human translation process",
    author = "B{\'e}dard, Claude",
    booktitle = "Conference of the Association for Machine Translation in the Americas",
    month = oct # " 2-5",
    year = "1996",
    address = "Montreal, Canada",
    url = "https://aclanthology.org/1996.amta-1.3",
}
@inproceedings{macklovitch-1995-transcheck,
    title = "{T}rans{C}heck: The automatic validation of human translations",
    author = "Macklovitch, Elliott",
    booktitle = "Proceedings of Machine Translation Summit V",
    month = jul # " 10-13",
    year = "1995",
    address = "Luxembourg, Luxembourg",
    url = "https://aclanthology.org/1995.mtsummit-1.36",
}
@inproceedings{isabelle-1993-machine,
    title = "Machine-Aided Human Translation and the Paradigm Shift",
    author = "Isabelle, Pierre",
    booktitle = "Proceedings of Machine Translation Summit IV",
    month = jul # " 19-22",
    year = "1993",
    address = "Kobe, Japan",
    url = "https://aclanthology.org/1993.mtsummit-1.16",
    pages = "177--180",
}
@inproceedings{boje-1988-disambiguering,
    title = "Disambiguering i human overs{\ae}ttelse og i maskinovers{\ae}ttelse (Disambiguation in human translation and in machine translation) [In {D}anish]",
    author = "Boje, Frede",
    booktitle = "Proceedings of the 6th Nordic Conference of Computational Linguistics ({NODALIDA} 1987)",
    month = mar,
    year = "1988",
    address = "Copenhagen, Denmark",
    publisher = "Institut for Datalingvistik, Handelsh{\o}jskolen i K{\o}benhavn, Denmark",
    url = "https://aclanthology.org/W87-0111",
    pages = "170--186",
}
@inproceedings{ceccato-zonta-1961-human,
    title = "Human translation and translation by machine",
    author = "Ceccato, Silvio  and
      Zonta, Bruna",
    booktitle = "Proceedings of the International Conference on Machine Translation and Applied Language Analysis",
    month = "5-8 " # sep,
    year = "1961",
    address = "National Physical Laboratory, Teddington, UK",
    url = "https://aclanthology.org/1961.earlymt-1.14",
}
@inproceedings{glasersfeld-1961-human,
    title = "Human translation and translation by machine {II}",
    author = "Glasersfeld, E. V.  and
      Perschke, Sergei  and
      Samet, Elsa",
    booktitle = "Proceedings of the International Conference on Machine Translation and Applied Language Analysis",
    month = "5-8 " # sep,
    year = "1961",
    address = "National Physical Laboratory, Teddington, UK",
    url = "https://aclanthology.org/1961.earlymt-1.26",
}
@inproceedings{dostert-1952-human,
    title = "Human translation versus machine translation",
    author = "Dostert, Leon",
    booktitle = "Proceedings of the Conference on Mechanical Translation",
    month = "17-20 " # jun,
    year = "1952",
    address = "Massachusetts Institute of Technology",
    url = "https://aclanthology.org/1952.earlymt-1.6",
}