@inproceedings{koretaka-etal-2023-mitigating,
    title = "Mitigating Domain Mismatch in Machine Translation via Paraphrasing",
    author = "Koretaka, Hyuga  and
      Kajiwara, Tomoyuki  and
      Fujita, Atsushi  and
      Ninomiya, Takashi",
    editor = "Nakazawa, Toshiaki  and
      Kinugawa, Kazutaka  and
      Mino, Hideya  and
      Goto, Isao  and
      Dabre, Raj  and
      Higashiyama, Shohei  and
      Parida, Shantipriya  and
      Morishita, Makoto  and
      Bojar, Ondrej  and
      Eriguchi, Akiko  and
      Oda, Yusuke  and
      Eriguchi, Akiko  and
      Chu, Chenhui  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 10th Workshop on Asian Translation",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.wat-1.2",
    pages = "29--40",
    abstract = "Quality of machine translation (MT) deteriorates significantly when translating texts having characteristics that differ from the training data, such as content domain. Although previous studies have focused on adapting MT models on a bilingual parallel corpus in the target domain, this approach is not applicable when no parallel data are available for the target domain or when utilizing black-box MT systems. To mitigate problems caused by such domain mismatch without relying on any corpus in the target domain, this study proposes a method to search for better translations by paraphrasing input texts of MT. To obtain better translations even for input texts from unforeknown domains, we generate their multiple paraphrases, translate each, and rerank the resulting translations to select the most likely one. Experimental results on Japanese-to-English translation reveal that the proposed method improves translation quality in terms of BLEU score for input texts from specific domains.",
}
@article{brannon-etal-2023-dubbing,
    title = "Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing",
    author = "Brannon, William  and
      Virkar, Yogesh  and
      Thompson, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.25",
    doi = "10.1162/tacl_a_00551",
    pages = "419--435",
    abstract = "We investigate how humans perform the task of dubbing video content from one language into another, leveraging a novel corpus of 319.57 hours of video from 54 professionally produced titles. This is the first such large-scale study we are aware of. The results challenge a number of assumptions commonly made in both qualitative literature on human dubbing and machine-learning literature on automatic dubbing, arguing for the importance of vocal naturalness and translation quality over commonly emphasized isometric (character length) and lip-sync constraints, and for a more qualified view of the importance of isochronic (timing) constraints. We also find substantial influence of the source-side audio on human dubs through channels other than the words of the translation, pointing to the need for research on ways to preserve speech characteristics, as well as transfer of semantic properties such as emphasis and emotion, in automatic dubbing systems.",
}
@inproceedings{pramodya-2023-exploring,
    title = "Exploring Low-resource Neural Machine Translation for {S}inhala-{T}amil Language Pair",
    author = "Pramodya, Ashmari",
    editor = "Hardalov, Momchil  and
      Kancheva, Zara  and
      Velichkov, Boris  and
      Nikolova-Koleva, Ivelina  and
      Slavcheva, Milena",
    booktitle = "Proceedings of the 8th Student Research Workshop associated with the International Conference Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-stud.10",
    pages = "87--97",
    abstract = "At present, Neural Machine Translation is a promising approach for machine translation. Transformer-based deep learning architectures in particular show a substantial performance increase in translating between various language pairs. However, many low-resource language pairs still struggle to lend themselves to Neural Machine Translation due to their data-hungry nature. In this article, we investigate methods of expanding the parallel corpus to enhance translation quality within a model training pipeline, starting from the initial collection of parallel data to the training process of baseline models. Grounded on state-of-the-art Neural Machine Translation approaches such as hyper-parameter tuning, and data augmentation with forward and backward translation, we define a set of best practices for improving Tamil-to-Sinhala machine translation and empirically validate our methods using standard evaluation metrics. Our results demonstrate that the Neural Machine Translation models trained on larger amounts of back-translated data outperform other synthetic data generation approaches in Transformer base training settings. We further demonstrate that, even for language pairs with limited resources, Transformer models are able to tune to outperform existing state-of-the-art Statistical Machine Translation models by as much as 3.28 BLEU points in the Tamil to Sinhala translation scenarios.",
}
@inproceedings{gete-etchegoyhen-2023-evaluation,
    title = "An Evaluation of Source Factors in Concatenation-Based Context-Aware Neural Machine Translation",
    author = "Gete, Harritxu  and
      Etchegoyhen, Thierry",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.45",
    pages = "399--407",
    abstract = "We explore the use of source factors in context-aware neural machine translation, specifically concatenation-based models, to improve the translation quality of inter-sentential phenomena. Context sentences are typically concatenated to the sentence to be translated, with string-based markers to separate the latter from the former. Although previous studies have measured the impact of prefixes to identify and mark context information, the use of learnable factors has only been marginally explored. In this study, we evaluate the impact of single and multiple source context factors in English-German and Basque-Spanish contextual translation. We show that this type of factors can significantly enhance translation accuracy for phenomena such as gender and register coherence in Basque-Spanish, while also improving BLEU results in some scenarios. These results demonstrate the potential of factor-based context identification to improve context-aware machine translation in future research.",
}
@inproceedings{gladkoff-etal-2023-students,
    title = "Student{'}s t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce",
    author = "Gladkoff, Serge  and
      Han, Lifeng  and
      Nenadic, Goran",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.47",
    pages = "419--428",
    abstract = "In natural language processing (NLP) we always rely on human judgement as the golden quality evaluation method. However, there has been an ongoing debate on how to better evaluate inter-rater reliability (IRR) levels for certain evaluation tasks, such as translation quality evaluation (TQE), especially when the data samples (observations) are very scarce. In this work, we first introduce the study on how to estimate the confidence interval for the measurement value when only one data (evaluation) point is available. Then, this leads to our example with two human-generated observational scores, for which, we introduce {``}Student{'}s \textit{t}-Distribution{''} method and explain how to use it to measure the IRR score using only these two data points, as well as the confidence intervals (CIs) of the quality evaluation. We give a quantitative analysis of how the evaluation confidence can be greatly improved by introducing more observations, even if only one extra observation. We encourage researchers to report their IRR scores in all possible means, e.g. using Student{'}s \textit{t}-Distribution method whenever possible; thus making the NLP evaluation more meaningful, transparent, and trustworthy. This \textit{t}-Distribution method can be also used outside of NLP fields to measure IRR level for trustworthy evaluation of experimental investigations, whenever the observational data is scarce.",
}
@inproceedings{stenlund-etal-2023-improving,
    title = "Improving Translation Quality for Low-Resource {I}nuktitut with Various Preprocessing Techniques",
    author = "Stenlund, Mathias Hans Erik  and
      Nanni, Mathilde  and
      Bruton, Micaella  and
      Beloucif, Meriem",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.53",
    pages = "475--479",
    abstract = "Neural machine translation has been shown to outperform all other machine translation paradigms when trained in a high-resource setting. However, it still performs poorly when dealing with low-resource languages, for which parallel data for training is scarce. This is especially the case for morphologically complex languages such as Turkish, Tamil, Uyghur, etc. In this paper, we investigate various preprocessing methods for Inuktitut, a low-resource indigenous language from North America, without a morphological analyzer. On both the original and romanized scripts, we test various preprocessing techniques such as Byte-Pair Encoding, random stemming, and data augmentation using Hungarian for the Inuktitut-to-English translation task. We found that there are benefits to retaining the original script as it helps to achieve higher BLEU scores than the romanized models.",
}
@inproceedings{temnikova-etal-2023-looking,
    title = "Looking for Traces of Textual Deepfakes in {B}ulgarian on Social Media",
    author = "Temnikova, Irina  and
      Marinova, Iva  and
      Gargova, Silvia  and
      Margova, Ruslana  and
      Koychev, Ivan",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.122",
    pages = "1151--1161",
    abstract = "Textual deepfakes can cause harm, especially on social media. At the moment, there are models trained to detect deepfake messages mainly for the English language, but no research or datasets currently exist for detecting them in most low-resource languages, such as Bulgarian. To address this gap, we explore three approaches. First, we machine translate an English-language social media dataset with bot messages into Bulgarian. However, the translation quality is unsatisfactory, leading us to create a new Bulgarian-language dataset with real social media messages and those generated by two language models (a new Bulgarian GPT-2 model {--} GPT-WEB-BG, and ChatGPT). We machine translate it into English and test existing English GPT-2 and ChatGPT detectors on it, achieving only 0.44-0.51 accuracy. Next, we train our own classifiers on the Bulgarian dataset, obtaining an accuracy of 0.97. Additionally, we apply the classifier with the highest results to a recently released Bulgarian social media dataset with manually fact-checked messages, which successfully identifies some of the messages as generated by Language Models (LM). Our results show that the use of machine translation is not suitable for textual deepfakes detection. We conclude that combining LM text detection with fact-checking is the most appropriate method for this task, and that identifying Bulgarian textual deepfakes is indeed possible.",
}
@inproceedings{boggia-etal-2023-dozens,
    title = "Dozens of Translation Directions or Millions of Shared Parameters? Comparing Two Types of Multilinguality in Modular Machine Translation",
    author = {Boggia, Michele  and
      Gr{\"o}nroos, Stig-Arne  and
      Loppi, Niki  and
      Mickus, Timothee  and
      Raganato, Alessandro  and
      Tiedemann, J{\"o}rg  and
      V{\'a}zquez, Ra{\'u}l},
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.24",
    pages = "238--247",
    abstract = "There are several ways of implementing multilingual NLP systems but little consensus as to whether different approaches exhibit similar effects. Are the trends that we observe when adding more languages the same as those we observe when sharing more parameters? We focus on encoder representations drawn from modular multilingual machine translation systems in an English-centric scenario, and study their quality from multiple aspects: how adequate they are for machine translation, how independent of the source language they are, and what semantic information they convey. Adding translation directions in English-centric scenarios does not conclusively lead to an increase in translation quality. Shared layers increase performance on zero-shot translation pairs and lead to more language-independent representations, but these improvements do not systematically align with more semantically accurate representations, from a monolingual standpoint.",
}
@inproceedings{zhang-2023-exploring,
    title = "Exploring undergraduate translation students{'} perceptions towards machine translation: A qualitative questionnaire survey",
    author = "Zhang, Jia",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.1",
    pages = "1--10",
    abstract = "Machine translation (MT) has relatively recently been introduced in higher education institutions, with specialised courses provided for students. However, such courses are often offered at the postgraduate level or towards the last year of an undergraduate programme (e.g., Arenas {\&} Moorkens, 2019; Doherty et al., 2012). Most previous studies have focussed on postgraduate students or undergraduate students in the last year of their programme and surveyed their perceptions or attitudes towards MT with quantitative questionnaires (e.g., Liu et al., 2022; Yang et al., 2021), yet undergraduate students earlier in their translation education remain overlooked. As such, not much is known about how they perceive and use MT and what their training needs may be. This study investigates the perceptions towards MT of undergraduate students at the early stage of translator training via qualitative questionnaires. Year-two translation students with little or no MT knowledge and no real-life translation experience (n=20) were asked to fill out a questionnaire with open-ended questions. Their answers were manually analysed by the researcher using NVivo to identify themes and arguments. It was revealed that even without proper training, the participants recognised MT{'}s potential advantages and disadvantages to a certain degree. MT is more often engaged as an instrument to learn language and translation rather than straightforwardly a translation tool. None of the students reported post-editing machine-generated translation in their translation assignments. Instead, they referenced MT output to understand terms, slang, fixed combinations and complicated sentences and to produce accurate, authentic and diversified phrases and sentences. They held a positive attitude towards MT quality and agreed that MT increased their translation quality, and they felt more confident with the tasks. While they were willing to experiment with MT as a translation tool and perform post-editing in future tasks, they were doubtful that MT could be introduced in the classroom at their current stage of translation learning. They feared that MT would impact their independent and critical thinking. Students did not mention any potential negative impacts of MT on the development of their language proficiency or translation competency. It is hoped that the findings will make an evidence-based contribution to the design of MT curricula and teaching pedagogies. Keywords: machine translation, post-editing, translator training, perception, attitudes, teaching pedagogy References: Arenas, A. G., {\&} Moorkens, J. (2019). Machine translation and post-editing training as part of a master{'}s programme. Journal of Specialised Translation, 31, 217{--}238. Doherty, S., Kenny, D., {\&} Way, A. (2012). Taking statistical machine translation to the student translator. Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Commercial MT User Program. Liu, K., Kwok, H. L., Liu, J., {\&} Cheung, A. K. (2022). Sustainability and influence of machine translation: Perceptions and attitudes of translation instructors and learners in Hong Kong. Sustainability, 14(11), 6399. Yang, Y., Wang, X., {\&} Yuan, Q. (2021). Measuring the usability of machine translation in the classroom context. Translation and Interpreting Studies, 16(1), 101{--}123.",
}
@inproceedings{zhang-etal-2023-leveraging,
    title = "Leveraging Multilingual Knowledge Graph to Boost Domain-specific Entity Translation of {C}hat{GPT}",
    author = "Zhang, Min  and
      Liu, Limin  and
      Yanqing, Zhao  and
      Qiao, Xiaosong  and
      Chang, Su  and
      Zhao, Xiaofeng  and
      Zhu, Junhao  and
      Zhu, Ming  and
      Peng, Song  and
      Li, Yinglu  and
      Liu, Yilun  and
      Ma, Wenbing  and
      Piao, Mengyao  and
      Tao, Shimin  and
      Yang, Hao  and
      Jiang, Yanfei",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.7",
    pages = "77--87",
    abstract = "Recently, ChatGPT has shown promising results for Machine Translation (MT) in general domains and is becoming a new paradigm for translation. In this paper, we focus on how to apply ChatGPT to domain-specific translation and propose to leverage Multilingual Knowledge Graph (MKG) to help ChatGPT improve the domain entity translation quality. To achieve this, we extract the bilingual entity pairs from MKG for the domain entities that are recognized from source sentences. We then introduce these pairs into translation prompts, instructing ChatGPT to use the correct translations of the domain entities. To evaluate the novel MKG method for ChatGPT, we conduct comparative experiments on three Chinese-English (zh-en) test datasets constructed from three specific domains, of which one domain is from biomedical science, and the other two are from the Information and Communications Technology (ICT) industry {---} Visible Light Communication (VLC) and wireless domains. Experimental results demonstrate that both the overall translation quality of ChatGPT (+6.21, +3.13 and +11.25 in BLEU scores) and the translation accuracy of domain entities (+43.2{\%}, +30.2{\%} and +37.9{\%} absolute points) are significantly improved with MKG on the three test datasets.",
}
@inproceedings{zhang-qian-2023-impact,
    title = "The impact of machine translation on the translation quality of undergraduate translation students",
    author = "Zhang, Jia  and
      Qian, Hong",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.9",
    pages = "99--108",
    abstract = "Post-editing (PE) refers to checking, proofreading, and revising the translation output of any automated translation (Gouadec, 2007, p. 25). It is needed because the meaning of a text can yet be accurately and fluently conveyed by machine translation (MT). The importance of PE and, accordingly, PE training has been widely acknowledged, and specialised courses have recently been introduced across universities and other organisations worldwide. However, scant consideration is given to when PE skills should be introduced in translation training. PE courses are usually offered to advanced translation learners, i.e., those at the postgraduate level or in the last year of an undergraduate program. Also, existing empirical studies most often investigate the impact of MT on postgraduate students or undergraduate students in the last year of their study. This paper reports on a study that aims to determine the possible effects of MT and PE on the translation quality of students at the early stage of translator training, i.e., undergraduate translation students with only basic translation knowledge. Methodologically, an experiment was conducted to compare students{'} (n=10) PEMT-based translations and from-scratch translations without the assistance of machine translation. Second-year students of an undergraduate translation programme were invited to translate two English texts with similar difficulties into Chinese. One of the texts was translated directly, while the other one was done with reference to machine-generated translation. Translation quality can be dynamic. When examined from different perspectives using different methods, the quality of a translation can vary. Several methods of translation quality assessment were adopted in this project, including rubrics-based scoring, error analysis and fixed-point translation analysis. It was found that the quality of students{'} PE translations was compromised compared with that of from-scratch translations. In addition, errors were more homogenised in the PEMT-based translations. It is hoped that this study can shed some light on the role of PEMT in translator training and contribute to the curricula and course design of post-editing for translator education. Reference: Gouadec, D. (2007). Translation as a Profession. John Benjamins Publishing. Keywords: machine translation, post-editing, translator training, translation quality assessment, error analysis, undergraduate students",
}
@inproceedings{zhang-etal-2023-leveraging-latent,
    title = "Leveraging Latent Topic Information to Improve Product Machine Translation",
    author = "Zhang, Bryan  and
      Walter, Stephan  and
      Misra, Amita  and
      Tan, Liling",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.10",
    pages = "109--118",
    abstract = "Meeting the expectations of e-commerce customers involves offering a seamless online shopping experience in their preferred language. To achieve this, modern e-commerce platforms rely on machine translation systems to provide multilingual product information on a large scale. However, maintaining high-quality machine translation that can keep up with the ever-expanding volume of product data remains an open challenge for industrial machine translation systems. In this context, topical clustering emerges as a valuable approach, leveraging latent signals and interpretable textual patterns to potentially enhance translation quality and facilitate industry-scale translation data discovery. This paper proposes two innovative methods: topic-based data selection and topic-signal augmentation, both utilizing latent topic clusters to improve the quality of machine translation in e-commerce. Furthermore, we present a data discovery workflow that utilizes topic clusters to effectively manage the growing multilingual product catalogs, addressing the challenges posed by their expansion.",
}
@inproceedings{zhu-etal-2023-kg,
    title = "{KG}-{IQES}: An Interpretable Quality Estimation System for Machine Translation Based on Knowledge Graph",
    author = "Zhu, Junhao  and
      Zhang, Min  and
      Yang, Hao  and
      Peng, Song  and
      Wu, Zhanglin  and
      Jiang, Yanfei  and
      Qiu, Xijun  and
      Pan, Weiqiang  and
      Zhu, Ming  and
      Miaomiao, Ma  and
      Zhang, Weidong",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.15",
    pages = "162--170",
    abstract = "The widespread use of machine translation (MT) has driven the need for effective automatic quality estimation (AQE) methods. How to enhance the interpretability of MT output quality estimation is well worth exploring in the industry. From the perspective of the alignment of named entities (NEs) in the source and translated sentences, we construct a multilingual knowledge graph (KG) consisting of domain-specific NEs, and design a KG-based interpretable quality estimation (QE) system for machine translations (KG-IQES). KG-IQES effectively estimates the translation quality without relying on reference translations. Its effectiveness has been verified in our business scenarios.",
}
@inproceedings{yamada-2023-optimizing,
    title = "Optimizing Machine Translation through Prompt Engineering: An Investigation into {C}hat{GPT}{'}s Customizability",
    author = "Yamada, Masaru",
    editor = "Yamada, Masaru  and
      do Carmo, Felix",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 2: Users Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-users.19",
    pages = "195--204",
    abstract = "This paper explores the influence of integrating the purpose of the translation and the target audience into prompts on the quality of translations produced by ChatGPT. Drawing on previous translation studies, industry practices, and ISO standards, the research underscores the significance of the pre-production phase in the translation process. The study reveals that the inclusion of suitable prompts in large-scale language models like ChatGPT can yield flexible translations, a feat yet to be realized by conventional Ma-chine Translation (MT). The research scrutinizes the changes in translation quality when prompts are used to generate translations that meet specific conditions. The evaluation is conducted from a practicing translator{'}s viewpoint, both subjectively and qualitatively, supplemented by the use of OpenAI{'}s word embedding API for cosine similarity calculations. The findings suggest that the integration of the purpose and target audience into prompts can indeed modify the generated translations, generally enhancing the translation quality by industry standards. The study also demonstrates the practical application of the {``}good translation{''} concept, particularly in the context of marketing documents and culturally dependent idioms.",
}
@inproceedings{liu-etal-2023-multiloop,
    title = "Multiloop Incremental Bootstrapping for Low-Resource Machine Translation",
    author = "Liu, Wuying  and
      Li, Wei  and
      Wang, Lin",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.1",
    pages = "1--11",
    abstract = "Due to the scarcity of high-quality bilingual sentence pairs, some deep-learning-based machine translation algorithms cannot achieve better performance in low-resource machine translation. On this basis, we are committed to integrating the ideas of machine learning algorithm improvement and data augmentation, propose a novel multiloop incremental bootstrapping framework, and design the corresponding semi-supervised learning algorithm. This framework is a meta-frame independent of specific machine translation algorithms. This algorithm makes full use of bilingual seed data of appropriate scale and super-large-scale monolingual data to expand bilingual sentence pair data incrementally, and trains machine translation models step by step to improve the translation quality. The experimental results of neural machine translation on multiple language pairs prove that our proposed framework can make use of continuous monolingual data to raise itself. Its effectiveness is not only reflected in the easy implementation of state-of-the-art low-resource machine translation, but also in the practical option to quickly establish precise domain machine translation systems.",
}
@inproceedings{araabi-etal-2023-joint,
    title = "Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables",
    author = "Araabi, Ali  and
      Niculae, Vlad  and
      Monz, Christof",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.2",
    pages = "12--25",
    abstract = "Despite the tremendous success of Neural Machine Translation (NMT), its performance on low- resource language pairs still remains subpar, partly due to the limited ability to handle previously unseen inputs, i.e., generalization. In this paper, we propose a method called Joint Dropout, that addresses the challenge of low-resource neural machine translation by substituting phrases with variables, resulting in significant enhancement of compositionality, which is a key aspect of generalization. We observe a substantial improvement in translation quality for language pairs with minimal resources, as seen in BLEU and Direct Assessment scores. Furthermore, we conduct an error analysis, and find Joint Dropout to also enhance generalizability of low-resource NMT in terms of robustness and adaptability across different domains.",
}
@inproceedings{tang-lepage-2023-dual,
    title = "A Dual Reinforcement Method for Data Augmentation using Middle Sentences for Machine Translation",
    author = "Tang, Wenyi  and
      Lepage, Yves",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.5",
    pages = "48--58",
    abstract = "This paper presents an approach to enhance the quality of machine translation by leveraging middle sentences as pivot points and employing dual reinforcement learning. Conventional methods for generating parallel sentence pairs for machine translation rely on parallel corpora, which may be scarce, resulting in limitations in translation quality. In contrast, our proposed method entails training two machine translation models in opposite directions, utilizing the middle sentence as a bridge for a virtuous feedback loop between the two models. This feedback loop resembles reinforcement learning, facilitating the models to make informed decisions based on mutual feedback. Experimental results substantiate that our proposed method significantly improves machine translation quality.",
}
@inproceedings{tran-etal-2023-improving,
    title = "Improving Embedding Transfer for Low-Resource Machine Translation",
    author = "Tran, Van Hien  and
      Ding, Chenchen  and
      Tanaka, Hideki  and
      Utiyama, Masao",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.11",
    pages = "123--134",
    abstract = "Low-resource machine translation (LRMT) poses a substantial challenge due to the scarcity of parallel training data. This paper introduces a new method to improve the transfer of the embedding layer from the Parent model to the Child model in LRMT, utilizing trained token embeddings in the Parent model{'}s high-resource vocabulary. Our approach involves projecting all tokens into a shared semantic space and measuring the semantic similarity between tokens in the low-resource and high-resource languages. These measures are then utilized to initialize token representations in the Child model{'}s low-resource vocabulary. We evaluated our approach on three benchmark datasets of low-resource language pairs: Myanmar-English, Indonesian-English, and Turkish-English. The experimental results demonstrate that our method outperforms previous methods regarding translation quality. Additionally, our approach is computationally efficient, leading to reduced training time compared to prior works.",
}
@inproceedings{dabre-etal-2023-study,
    title = "A Study on the Effectiveness of Large Language Models for Translation with Markup",
    author = "Dabre, Raj  and
      Buschbeck, Bianka  and
      Exel, Miriam  and
      Tanaka, Hideki",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.13",
    pages = "148--159",
    abstract = "In this paper we evaluate the utility of large language models (LLMs) for translation of text with markup in which the most important and challenging aspect is to correctly transfer markup tags while ensuring that the content, both, inside and outside tags is correctly translated. While LLMs have been shown to be effective for plain text translation, their effectiveness for structured document translation is not well understood. To this end, we experiment with BLOOM and BLOOMZ, which are open-source multilingual LLMs, using zero, one and few-shot prompting, and compare with a domain-specific in-house NMT system using a detag-and-project approach for markup tags. We observe that LLMs with in-context learning exhibit poorer translation quality compared to the domain-specific NMT system, however, they are effective in transferring markup tags, especially the large BLOOM model (176 billion parameters). This is further confirmed by our human evaluation which also reveals the types of errors of the different tag transfer techniques. While LLM-based approaches come with the risk of losing, hallucinating and corrupting tags, they excel at placing them correctly in the translation.",
}
@inproceedings{neumannova-bojar-2023-role,
    title = "The Role of Compounds in Human vs. Machine Translation Quality",
    author = "Neumannova, Kristyna  and
      Bojar, Ond{\v{r}}ej",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.21",
    pages = "248--260",
    abstract = "We focus on the production of German compounds in English-to-German manual and automatic translation. On the example of WMT21 news translation test set, we observe that even the best MT systems produce much fewer compounds compared to three independent manual translations. Despite this striking difference, we observe that this insufficiency is not apparent in manual evaluation methods that target the overall translation quality (DA and MQM). Simple automatic methods like BLEU somewhat surprisingly provide a better indication of this quality aspect. Our manual analysis of system outputs, including our freshly trained Transformer models, confirms that current deep neural systems operating at the level of subword units are capable of constructing novel words, including novel compounds. This effect however cannot be measured using static dictionaries of compounds such as GermaNet. German compounds thus pose an interesting challenge for future development of MT systems.",
}
@inproceedings{honda-etal-2023-context,
    title = "Context-aware Neural Machine Translation for {E}nglish-{J}apanese Business Scene Dialogues",
    author = "Honda, Sumire  and
      Fernandes, Patrick  and
      Zerva, Chrysoula",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.23",
    pages = "272--285",
    abstract = "Despite the remarkable advancements in machine translation, the current sentence-level paradigm faces challenges when dealing with highly-contextual languages like Japanese. In this paper, we explore how context-awareness can improve the performance of the current Neural Machine Translation (NMT) models for English-Japanese business dialogues translation, and what kind of context provides meaningful information to improve translation. As business dialogue involves complex discourse phenomena but offers scarce training resources, we adapted a pretrained mBART model, finetuning on multi-sentence dialogue data, which allows us to experiment with different contexts. We investigate the impact of larger context sizes and propose novel context tokens encoding extra-sentential information, such as speaker turn and scene type. We make use of Conditional Cross-Mutual Information (CXMI) to explore how much of the context the model uses and generalise CXMI to study the impact of the extra sentential context. Overall, we find that models leverage both preceding sentences and extra-sentential context (with CXMI increasing with context size) and we provide a more focused analysis on honorifics translation. Regarding translation quality, increased source-side context paired with scene and speaker information improves the model performance compared to previous work and our context-agnostic baselines, measured in BLEU and COMET metrics.",
}
@inproceedings{menezes-etal-2023-context,
    title = "A Context-Aware Annotation Framework for Customer Support Live Chat Machine Translation",
    author = "Menezes, Miguel  and
      Farajian, M. Amin  and
      Moniz, Helena  and
      Gra{\c{c}}a, Jo{\~a}o Varelas",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.24",
    pages = "286--297",
    abstract = "To measure context-aware machine translation (MT) systems quality, existing solutions have recommended human annotators to consider the full context of a document. In our work, we revised a well known Machine Translation quality assessment framework, Multidimensional Quality Metrics (MQM), (Lommel et al., 2014) by introducing a set of nine annotation categories that allows to map MT errors to source document contextual phenomenon, for simplicity sake we named such phenomena as contextual triggers. Our analysis shows that the adapted categories set enhanced MQM{'}s potential for MT error identification, being able to cover up to 61{\%} more errors, when compared to traditional non-context core MQM{'}s application. Subsequently, we analyzed the severity of these MT {``}contextual errors{''}, showing that the majority fall under the critical and major levels, further indicating the impact of such errors. Finally, we measured the ability of existing evaluation metrics in detecting the proposed MT {``}contextual errors{''}. The results have shown that current state-of-the-art metrics fall short in detecting MT errors that are caused by contextual triggers on the source document side. With the work developed, we hope to understand how impactful context is for enhancing quality within a MT workflow and draw attention to future integration of the proposed contextual annotation framework into current MQM{'}s core typology.",
}
@inproceedings{imamura-etal-2023-pivot,
    title = "Pivot Translation for Zero-resource Language Pairs Based on a Multilingual Pretrained Model",
    author = "Imamura, Kenji  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.29",
    pages = "348--359",
    abstract = "A multilingual translation model enables a single model to handle multiple languages. However, the translation qualities of unlearned language pairs (i.e., zero-shot translation qualities) are still poor. By contrast, pivot translation translates source texts into target ones via a pivot language such as English, thus enabling machine translation without parallel texts between the source and target languages. In this paper, we perform pivot translation using a multilingual model and compare it with direct translation. We improve the translation quality without using parallel texts of direct translation by fine-tuning the model with machine-translated pseudo-translations. We also discuss what type of parallel texts are suitable for effectively improving the translation quality in multilingual pivot translation.",
}
@inproceedings{hatami-etal-2023-filtering,
    title = "A Filtering Approach to Object Region Detection in Multimodal Machine Translation",
    author = "Hatami, Ali  and
      Buitelaar, Paul  and
      Arcan, Mihael",
    editor = "Utiyama, Masao  and
      Wang, Rui",
    booktitle = "Proceedings of Machine Translation Summit XIX, Vol. 1: Research Track",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-research.33",
    pages = "393--405",
    abstract = "Recent studies in Multimodal Machine Translation (MMT) have explored the use of visual information in a multimodal setting to analyze its redundancy with textual information. The aim of this work is to develop a more effective approach to incorporating relevant visual information into the translation process and improve the overall performance of MMT models. This paper proposes an object-level filtering approach in Multimodal Machine Translation, where the approach is applied to object regions extracted from an image to filter out irrelevant objects based on the image captions to be translated. Using the filtered image helps the model to consider only relevant objects and their relative locations to each other. Different matching methods, including string matching and word embeddings, are employed to identify relevant objects. Gaussian blurring is used to soften irrelevant objects from the image and to evaluate the effect of object filtering on translation quality. The performance of the filtering approaches was evaluated on the Multi30K dataset in English to German, French, and Czech translations, based on BLEU, ChrF2, and TER metrics.",
}
@inproceedings{steingrimsson-etal-2023-discard,
    title = "Do Not Discard {--} Extracting Useful Fragments from Low-Quality Parallel Data to Improve Machine Translation",
    author = "Steingr{\'\i}msson, Stein{\th}{\'o}r  and
      Lohar, Pintu  and
      Loftsson, Hrafn  and
      Way, Andy",
    booktitle = "Proceedings of the Second Workshop on Corpus Generation and Corpus Augmentation for Machine Translation",
    month = sep,
    year = "2023",
    address = "Macau SAR, China",
    publisher = "Asia-Pacific Association for Machine Translation",
    url = "https://aclanthology.org/2023.mtsummit-coco4mt.1",
    pages = "1--13",
    abstract = "When parallel corpora are preprocessed for machine translation (MT) training, a part of the parallel data is commonly discarded and deemed non-parallel due to odd-length ratio, overlapping text in source and target sentences or failing some other form of a semantic equivalency test. For language pairs with limited parallel resources, this can be costly as in such cases modest amounts of acceptable data may be useful to help build MT systems that generate higher quality translations. In this paper, we refine parallel corpora for two language pairs, English{--}Bengali and English{--}Icelandic, by extracting sub-sentence fragments from sentence pairs that would otherwise have been discarded, in order to increase recall when compiling training data. We find that by including the fragments, translation quality of NMT systems trained on the data improves significantly when translating from English to Bengali and from English to Icelandic.",
}
@inproceedings{zhou-etal-2023-train,
    title = "Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages",
    author = "Zhou, Zhong  and
      Niehues, Jan  and
      Waibel, Alexander",
    editor = "Ojha, Atul Kr.  and
      Liu, Chao-hong  and
      Vylomova, Ekaterina  and
      Pirinen, Flammie  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.loresmt-1.1",
    doi = "10.18653/v1/2023.loresmt-1.1",
    pages = "1--15",
    abstract = "In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only ∼1,000 in the new, unknown language.",
}
@inproceedings{gow-smith-etal-2023-naver,
    title = "{NAVER} {LABS} {E}urope{'}s Multilingual Speech Translation Systems for the {IWSLT} 2023 Low-Resource Track",
    author = "Gow-Smith, Edward  and
      Berard, Alexandre  and
      Zanon Boito, Marcely  and
      Calapodescu, Ioan",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.10",
    doi = "10.18653/v1/2023.iwslt-1.10",
    pages = "144--158",
    abstract = "This paper presents NAVER LABS Europe{'}s systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year{'}s test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.",
}
@inproceedings{wu-etal-2023-improving,
    title = "Improving Neural Machine Translation Formality Control with Domain Adaptation and Reranking-based Transductive Learning",
    author = "Wu, Zhanglin  and
      Li, Zongyao  and
      Wei, Daimeng  and
      Shang, Hengchao  and
      Guo, Jiaxin  and
      Chen, Xiaoyu  and
      Rao, Zhiqiang  and
      Yu, Zhengzhe  and
      Yang, Jinlong  and
      Li, Shaojun  and
      Xie, Yuhao  and
      Wei, Bin  and
      Zheng, Jiawei  and
      Zhu, Ming  and
      Lei, Lizhi  and
      Yang, Hao  and
      Jiang, Yanfei",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.13",
    doi = "10.18653/v1/2023.iwslt-1.13",
    pages = "180--186",
    abstract = "This paper presents Huawei Translation Service Center (HW-TSC){'}s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets constrained and unconstrained conditions. We train the formality control models for these four language pairs under these two conditions respectively, and submit the corresponding translation results. Our efforts are divided into two fronts: enhancing general translation quality and improving formality control capability. According to the different requirements of the formality control task, we use a multi-stage pre-training method to train a bilingual or multilingual neural machine translation (NMT) model as the basic model, which can improve the general translation quality of the base model to a relatively high level. Then, under the premise of affecting the general translation quality of the basic model as little as possible, we adopt domain adaptation and reranking-based transductive learning methods to improve the formality control capability of the model.",
}
@inproceedings{li-etal-2023-hw,
    title = "{HW}-{TSC} at {IWSLT}2023: Break the Quality Ceiling of Offline Track via Pre-Training and Domain Adaptation",
    author = "Li, Zongyao  and
      Wu, Zhanglin  and
      Rao, Zhiqiang  and
      YuHao, Xie  and
      JiaXin, Guo  and
      Wei, Daimeng  and
      Shang, Hengchao  and
      Minghan, Wang  and
      Chen, Xiaoyu  and
      Yu, Zhengzhe  and
      ShaoJun, Li  and
      LiZhi, Lei  and
      Yang, Hao",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.14",
    doi = "10.18653/v1/2023.iwslt-1.14",
    pages = "187--193",
    abstract = "This paper presents HW-TSC{'}s submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training, and unconstrained training) with models of cascaded architectures. We use data enhancement, pre-training models and other means to improve the ASR quality, and R-Drop, deep model, domain data selection, etc. to improve the translation quality. Compared with last year{'}s best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test set.",
}
@inproceedings{bahar-etal-2023-speech,
    title = "Speech Translation with Style: {A}pp{T}ek{'}s Submissions to the {IWSLT} Subtitling and Formality Tracks in 2023",
    author = {Bahar, Parnia  and
      Wilken, Patrick  and
      Iranzo-S{\'a}nchez, Javier  and
      Di Gangi, Mattia  and
      Matusov, Evgeny  and
      T{\"u}ske, Zolt{\'a}n},
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.22",
    doi = "10.18653/v1/2023.iwslt-1.22",
    pages = "251--260",
    abstract = "AppTek participated in the subtitling and formality tracks of the IWSLT 2023 evaluation. This paper describes the details of our subtitling pipeline - speech segmentation, speech recognition, punctuation prediction and inverse text normalization, text machine translation and direct speech-to-text translation, intelligent line segmentation - and how we make use of the provided subtitling-specific data in training and fine-tuning. The evaluation results show that our final submissions are competitive, in particular outperforming the submissions by other participants by 5{\%} absolute as measured by the SubER subtitle quality metric. For the formality track, we participate with our En-Ru and En-Pt production models, which support formality control via prefix tokens. Except for informal Portuguese, we achieve near perfect formality level accuracy while at the same time offering high general translation quality.",
}
@inproceedings{jain-etal-2023-language,
    title = "Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation",
    author = "Jain, Aditi  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.32",
    doi = "10.18653/v1/2023.iwslt-1.32",
    pages = "341--356",
    abstract = "The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).",
}
@inproceedings{polak-etal-2023-towards,
    title = "Towards Efficient Simultaneous Speech Translation: {CUNI}-{KIT} System for Simultaneous Track at {IWSLT} 2023",
    author = "Pol{\'a}k, Peter  and
      Liu, Danni  and
      Pham, Ngoc-Quan  and
      Niehues, Jan  and
      Waibel, Alexander  and
      Bojar, Ond{\v{r}}ej",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.37",
    doi = "10.18653/v1/2023.iwslt-1.37",
    pages = "389--396",
    abstract = "In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45{\%} relative RTF).",
}
@inproceedings{zhan-etal-2023-depa,
    title = "{D}e{PA}: Improving Non-autoregressive Translation with Dependency-Aware Decoder",
    author = "Zhan, Jiaao  and
      Chen, Qian  and
      Chen, Boxing  and
      Wang, Wen  and
      Bai, Yu  and
      Gao, Yang",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Carpuat, Marine",
    booktitle = "Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.iwslt-1.47",
    doi = "10.18653/v1/2023.iwslt-1.47",
    pages = "478--490",
    abstract = "Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks by up to 1.88 BLEU gain, while maintaining the inference latency comparable to other fully NAT models.",
}
@inproceedings{zhao-etal-2023-generating,
    title = "Generating Synthetic Speech from {S}poken{V}ocab for Speech Translation",
    author = "Zhao, Jinming  and
      Haffari, Gholamreza  and
      Shareghi, Ehsan",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.147",
    doi = "10.18653/v1/2023.findings-eacl.147",
    pages = "1975--1981",
    abstract = "Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert text-based machine translation (MT) data to ST data via text-to-speech (TTS) systems. Yet, using TTS systems can be tedious and slow. In this work, we propose SpokenVocab, a simple, scalable and effective data augmentation technique to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets, corresponding to words in an MT sentence, from a spoken vocabulary bank. Our experiments on multiple language pairs show that stitched speech helps to improve translation quality by an average of 1.83 BLEU score, while performing equally well as TTS-generated speech in improving translation quality. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit.",
}
@inproceedings{zhuo-etal-2023-rethinking,
    title = "Rethinking Round-Trip Translation for Machine Translation Evaluation",
    author = "Zhuo, Terry Yue  and
      Xu, Qiongkai  and
      He, Xuanli  and
      Cohn, Trevor",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.22",
    doi = "10.18653/v1/2023.findings-acl.22",
    pages = "319--337",
    abstract = "Automatic evaluation methods for translation often require model training, and thus the availability of parallel corpora limits their applicability to low-resource settings. Round-trip translation is a potential workaround, which can reframe bilingual evaluation into a much simpler monolingual task. Early results from the era of statistical machine translation (SMT) raised fundamental concerns about the utility of this approach, based on poor correlation with human translation quality judgments. In this paper, we revisit this technique with modern neural translation (NMT) and show that round-trip translation does allow for accurate automatic evaluation without the need for reference translations. These opposite findings can be explained through the copy mechanism in SMT that is absent in NMT. We demonstrate that round-trip translation benefits multiple machine translation evaluation tasks: i) predicting forward translation scores; ii) improving the performance of a quality estimation model; and iii) identifying adversarial competitors in shared tasks via cross-system verification.",
}
@inproceedings{zeng-etal-2023-extract,
    title = "Extract and Attend: Improving Entity Translation in Neural Machine Translation",
    author = "Zeng, Zixin  and
      Wang, Rui  and
      Leng, Yichong  and
      Guo, Junliang  and
      Xie, Shufang  and
      Tan, Xu  and
      Qin, Tao  and
      Liu, Tie-Yan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.107",
    doi = "10.18653/v1/2023.findings-acl.107",
    pages = "1697--1710",
    abstract = "While Neural Machine Translation (NMT) has achieved great progress in recent years, it still suffers from inaccurate translation of entities (e.g., person/organization name, location), due to the lack of entity training instances. When we humans encounter an unknown entity during translation, we usually first look up in a dictionary and then organize the entity translation together with the translations of other parts to form a smooth target sentence. Inspired by this translation process, we propose an Extract-and-Attend approach to enhance entity translation in NMT, where the translation candidates of source entities are first extracted from a dictionary and then attended to by the NMT model to generate the target sentence. Specifically, the translation candidates are extracted by first detecting the entities in a source sentence and then translating the entities through looking up in a dictionary. Then, the extracted candidates are added as a prefix of the decoder input to be attended to by the decoder when generating the target sentence through self-attention. Experiments conducted on En-Zh and En-Ru demonstrate that the proposed method is effective on improving both the translation accuracy of entities and the overall translation quality, with up to 35{\%} reduction on entity error rate and 0.85 gain on BLEU and 13.8 gain on COMET.",
}
@inproceedings{yang-etal-2023-rethinking,
    title = "Rethinking the Word-level Quality Estimation for Machine Translation from Human Judgement",
    author = "Yang, Zhen  and
      Meng, Fandong  and
      Yan, Yuanmeng  and
      Zhou, Jie",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.126",
    doi = "10.18653/v1/2023.findings-acl.126",
    pages = "2012--2025",
    abstract = "Word-level Quality Estimation (QE) of Machine Translation (MT) aims to detect potential translation errors in the translated sentence without reference. Typically, conventional works on word-level QE are usually designed to predict the quality of translated words in terms of the post-editing effort, where the word labels in the dataset, i.e., OK or BAD, are automatically generated by comparing words between MT sentences and the post-edited sentences through a Translation Error Rate (TER) toolkit. While the post-editing effort can be used to measure the translation quality to some extent, we find it usually conflicts with human judgment on whether the word is well or poorly translated. To investigate this conflict, we first create a golden benchmark dataset, namely \textit{HJQE} (Human Judgement on Quality Estimation), where the source and MT sentences are identical to the original TER-based dataset and the expert translators directly annotate the poorly translated words on their judgments. Based on our analysis, we further propose two tag-correcting strategies which can make the TER-based artificial QE corpus closer to \textit{HJQE}. We conduct substantial experiments based on the publicly available WMT En-De and En-Zh corpora. The results not only show our proposed dataset is more consistent with human judgment but also confirm the effectiveness of the proposed tag-correcting strategies.For reviewers, the corpora and codes can be found in the attached files.",
}
@inproceedings{obadic-etal-2023-c,
    title = "{C}-{XNLI}: {C}roatian Extension of {XNLI} Dataset",
    author = "Obadi{\'c}, Leo  and
      Jertec, Andrej  and
      Rajnovi{\'c}, Marko  and
      Dropulji{\'c}, Branimir",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.142",
    doi = "10.18653/v1/2023.findings-acl.142",
    pages = "2258--2267",
    abstract = "Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Facebook{'}s 1.2B parameter m2m{\_}100 model. We thoroughly analyze the Croatian train set and compare its quality with the existing machine-translated German set. The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT). Our findings reveal that a less-resourced language like Croatian is still lacking in translation quality of longer sentences compared to German. However, both sets have a substantial amount of poor quality translations, which should be considered in translation-based training or evaluation setups.",
}
@inproceedings{zhu-etal-2023-beyond,
    title = "Beyond Triplet: Leveraging the Most Data for Multimodal Machine Translation",
    author = "Zhu, Yaoming  and
      Sun, Zewei  and
      Cheng, Shanbo  and
      Huang, Luyang  and
      Wu, Liwei  and
      Wang, Mingxuan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.168",
    doi = "10.18653/v1/2023.findings-acl.168",
    pages = "2679--2697",
    abstract = "Multimodal machine translation (MMT) aims to improve translation quality by incorporating information from other modalities, such as vision. Previous MMT systems focus on better access and use of visual information and tend to validate their methods on image-related datasets. However, these studies face two challenges. First, they can only utilize a limited amount of data that is composed of bilingual texts and images (referred to as {``}triple data{''}), which is scarce. Second, current benchmarks for MMT are restricted and do not correspond to realistic scenarios. Therefore, this paper correspondingly establishes new methods and a new dataset for MMT. We propose a novel framework for MMT that addresses these challenges by utilizing large-scale non-triple data, such as monolingual image-text and parallel text-only data. Additionally, we construct a new e-commercial multimodal translation dataset, named EMMT, of which the test set is specifically designed to include ambiguous words that require visual context for accurate translation. Experiments show that our method is well-suited for real-world scenarios and can significantly improve translation performance with more non-triple data. In addition, our model also rivals or surpasses various SOTA models in conventional multimodal translation benchmarks.",
}
@inproceedings{machacek-etal-2023-robustness,
    title = "Robustness of Multi-Source {MT} to Transcription Errors",
    author = "Mach{\'a}{\v{c}}ek, Dominik  and
      Pol{\'a}k, Peter  and
      Bojar, Ond{\v{r}}ej  and
      Dabre, Raj",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.228",
    doi = "10.18653/v1/2023.findings-acl.228",
    pages = "3707--3723",
    abstract = "Automatic speech translation is sensitive to speech recognition errors, but in a multilingual scenario, the same content may be available in various languages via simultaneous interpreting, dubbing or subtitling. In this paper, we hypothesize that leveraging multiple sources will improve translation quality if the sources complement one another in terms of correct information they contain. To this end, we first show that on a 10-hour ESIC corpus, the ASR errors in the original English speech and its simultaneous interpreting into German and Czech are mutually independent. We then use two sources, English and German, in a multi-source setting for translation into Czech to establish its robustness to ASR errors. Furthermore, we observe this robustness when translating both noisy sources together in a simultaneous translation setting. Our results show that multi-source neural machine translation has the potential to be useful in a real-time simultaneous translation setting, thereby motivating further investigation in this area.",
}
@inproceedings{wang-etal-2023-hybrid,
    title = "Hybrid-Regressive Paradigm for Accurate and Speed-Robust Neural Machine Translation",
    author = "Wang, Qiang  and
      Hu, Xinhui  and
      Chen, Ming",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.367",
    doi = "10.18653/v1/2023.findings-acl.367",
    pages = "5931--5945",
    abstract = "This work empirically confirms that non-autoregressive translation (NAT) is less robust in decoding batch size and hardware settings than autoregressive translation (AT). To address this issue, we demonstrate that prompting a small number of AT predictions can significantly reduce the performance gap between AT and NAT through synthetic experiments. Following this line, we propose hybrid-regressive translation (HRT), a two-stage translation prototype that combines the strengths of AT and NAT. Specifically, HRT first generates discontinuous sequences via autoregression (e.g., make a prediction for every $k$ tokens, $k>1$) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Experiments on five translation tasks show that HRT achieves comparable translation quality with AT while having at least 1.5x faster inference regardless of batch size and device. Additionally, HRT successfully inherits the sound characteristics of AT in the deep-encoder-shallow-decoder architecture, allowing for further speedup without BLEU loss.",
}
@inproceedings{herold-etal-2023-improving,
    title = "Improving Language Model Integration for Neural Machine Translation",
    author = "Herold, Christian  and
      Gao, Yingbo  and
      Zeineldeen, Mohammad  and
      Ney, Hermann",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.444",
    doi = "10.18653/v1/2023.findings-acl.444",
    pages = "7114--7123",
    abstract = "The integration of language models for neural machine translation has been extensively studied in the past. It has been shown that an external language model, trained on additional target-side monolingual data, can help improve translation quality. However, there has always been the assumption that the translation model also learns an implicit target-side language model during training, which interferes with the external language model at decoding time. Recently, some works on automatic speech recognition have demonstrated that, if the implicit language model is neutralized in decoding, further improvements can be gained when integrating an external language model. In this work, we transfer this concept to the task of machine translation and compare with the most prominent way of including additional monolingual data - namely back-translation. We find that accounting for the implicit language model significantly boosts the performance of language model fusion, although this approach is still outperformed by back-translation.",
}
@inproceedings{mcnamee-duh-2023-extensive,
    title = "An Extensive Exploration of Back-Translation in 60 Languages",
    author = "McNamee, Paul  and
      Duh, Kevin",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.518",
    doi = "10.18653/v1/2023.findings-acl.518",
    pages = "8166--8183",
    abstract = "Back-translation is a data augmentation technique that has been shown to improve model quality through the creation of synthetic training bitext. Early studies showed the promise of the technique and follow on studies have produced additional refinements. We have undertaken a broad investigation using back-translation to train models from 60 languages into English; the majority of these languages are considered moderate- or low-resource languages. We observed consistent gains, though compared to prior work we saw conspicuous gains in quite a number of lower-resourced languages. We analyzed differences in translations between baseline and back-translation models, and observed many indications of improved translation quality. Translation of both rare and common terms is improved, and these improvements occur despite the less natural synthetic source-language text used in training.",
}
@inproceedings{vincent-etal-2023-mtcue,
    title = "{MTC}ue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation",
    author = "Vincent, Sebastian  and
      Flynn, Robert  and
      Scarton, Carolina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.521",
    doi = "10.18653/v1/2023.findings-acl.521",
    pages = "8210--8226",
    abstract = "Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker{'}s gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCue significantly outperforms a {``}tagging{''} baseline at translating English text. Analysis reveals that the context encoder of MTCue learns a representation space that organises context based on specific attributes, such as formality, enabling effective zero-shot control. Pre-training on context embeddings also improves MTCue{'}s few-shot performance compared to the {``}tagging{''} baseline. Finally, an ablation study conducted on model components and contextual variables further supports the robustness of MTCue for context-based NMT.",
}
@inproceedings{norouzi-etal-2023-dims,
    title = "{D}i{MS}: Distilling Multiple Steps of Iterative Non-Autoregressive Transformers for Machine Translation",
    author = "Norouzi, Sajad  and
      Hosseinzadeh, Rasa  and
      Perez, Felipe  and
      Volkovs, Maksims",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.542",
    doi = "10.18653/v1/2023.findings-acl.542",
    pages = "8538--8553",
    abstract = "The computational benefits of iterative non-autoregressive transformers decrease as the number of decoding steps increases. As a remedy, we introduce Distill Multiple Steps (DiMS), a simple yet effective distillation technique to decrease the number of required steps to reach a certain translation quality. The distilled model enjoys the computational benefits of early iterations while preserving the enhancements from several iterative steps. DiMS relies on two models namely student and teacher. The student is optimized to predict the output of the teacher after multiple decoding steps while the teacher follows the student via a slow-moving average. The moving average keeps the teacher{'}s knowledge updated and enhances the quality of the labels provided by the teacher. During inference, the student is used for translation and no additional computation is added. We verify the effectiveness of DiMS on various models obtaining 7.8 and 12.9 BLEU points improvements in single-step translation accuracy on distilled and raw versions of WMT{'}14 De-En.Full code for this work is available here: \url{https://github.com/layer6ai-labs/DiMS}",
}
@inproceedings{agrawal-etal-2023-context,
    title = "In-context Examples Selection for Machine Translation",
    author = "Agrawal, Sweta  and
      Zhou, Chunting  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Ghazvininejad, Marjan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.564",
    doi = "10.18653/v1/2023.findings-acl.564",
    pages = "8857--8873",
    abstract = "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated examples can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
}
@inproceedings{wang-etal-2023-revisiting,
    title = "Revisiting Non-Autoregressive Translation at Scale",
    author = "Wang, Zhihao  and
      Wang, Longyue  and
      Su, Jinsong  and
      Yao, Junfeng  and
      Tu, Zhaopeng",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.763",
    doi = "10.18653/v1/2023.findings-acl.763",
    pages = "12051--12065",
    abstract = "In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on the scaled dataset, which can be regarded as a strong baseline for future works. We release code and system outputs at \url{https://github.com/DeepLearnXMU/Scaling4NAT}.",
}
@inproceedings{zhang-etal-2023-lexical,
    title = "Lexical Translation Inconsistency-Aware Document-Level Translation Repair",
    author = "Zhang, Zhen  and
      Li, Junhui  and
      Tao, Shimin  and
      Yang, Hao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.791",
    doi = "10.18653/v1/2023.findings-acl.791",
    pages = "12492--12505",
    abstract = "Following the idea of {``}one translation per discourse{''}, in this paper we aim to improve translation consistency via document-level translation repair (DocRepair), i.e., automatic post-editing on translations of documents. To this end, we propose a lexical translation inconsistency-aware DocRepair to explicitly model translation inconsistency. First we locate the inconsistency in automatic translation. Then we provide translation candidates for those inconsistency. Finally, we propose lattice-like input to properly model inconsistent tokens and phrases and their candidates. Experimental results on three document-level translation datasets show that based on G-Transformer, a state-of-the-art document-to-document (Doc2Doc) translation model, our Doc2Doc DocRepair achieves significant improvement on translation quality in BLEU scores, but also greatly improves lexical translation consistency.",
}
@inproceedings{raffel-chen-2023-implicit,
    title = "Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation",
    author = "Raffel, Matthew  and
      Chen, Lizhong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.816",
    doi = "10.18653/v1/2023.findings-acl.816",
    pages = "12900--12907",
    abstract = "Simultaneous speech translation is an essential communication task difficult for humans whereby a translation is generated concurrently with oncoming speech inputs. For such a streaming task, transformers using block processing to break an input sequence into segments have achieved state-of-the-art performance at a reduced cost. Current methods to allow information to propagate across segments, including left context and memory banks, have faltered as they are both insufficient representations and unnecessarily expensive to compute. In this paper, we propose an Implicit Memory Transformer that implicitly retains memory through a new left context method, removing the need to explicitly represent memory with memory banks. We generate the left context from the attention output of the previous segment and include it in the keys and values of the current segment{'}s attention calculation. Experiments on the MuST-C dataset show that the Implicit Memory Transformer provides a substantial speedup on the encoder forward pass with nearly identical translation quality when compared with the state-of-the-art approach that employs both left context and memory banks.",
}
@inproceedings{lo-knowles-2023-data,
    title = "Data Sampling and (In)stability in Machine Translation Evaluation",
    author = "Lo, Chi-kiu  and
      Knowles, Rebecca",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.826",
    doi = "10.18653/v1/2023.findings-acl.826",
    pages = "13064--13074",
    abstract = "We analyze the different data sampling approaches used in selecting data for human evaluation and ranking of machine translation systems at the highly influential Conference on Machine Translation (WMT). By using automatic evaluation metrics, we are able to focus on the impact of the data sampling procedure as separate from questions about human annotator consistency. We provide evidence that the latest data sampling approach used at WMT skews the annotated data toward shorter documents, not necessarily representative of the full test set. Lastly, we examine a new data sampling method that uses the available labour budget to sample data in a more representative manner, with the goals of improving representation of various document lengths in the sample and producing more stable rankings of system translation quality.",
}
@inproceedings{sharami-etal-2023-tailoring,
    title = "Tailoring Domain Adaptation for Machine Translation Quality Estimation",
    author = "Sharami, Javad Pourmostafa Roshan  and
      Shterionov, Dimitar  and
      Blain, Fr{\'e}d{\'e}ric  and
      Vanmassenhove, Eva  and
      Sisto, Mirella De  and
      Emmery, Chris  and
      Spronck, Pieter",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.2",
    pages = "9--20",
    abstract = "While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizabile, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues {---} data scarcity and domain mismatch {---} this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.",
}
@inproceedings{gumma-etal-2023-empirical,
    title = "An Empirical Study of Leveraging Knowledge Distillation for Compressing Multilingual Neural Machine Translation Models",
    author = "Gumma, Varun  and
      Dabre, Raj  and
      Kumar, Pratyush",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.11",
    pages = "103--114",
    abstract = "Knowledge distillation (KD) is a well-known method for compressing neural models. However, works focusing on distilling knowledge from large multilingual neural machine translation (MNMT) models into smaller ones are practically nonexistent, despite the popularity and superiority of MNMT. This paper bridges this gap by presenting an empirical investigation of knowledge distillation for compressing MNMT models. We take Indic to English translation as a case study and demonstrate that commonly used language-agnostic and language-aware KD approaches yield models that are 4-5x smaller but also suffer from performance drops of up to 3.5 BLEU. To mitigate this, we then experiment with design considerations such as shallower versus deeper models, heavy parameter sharing, multistage training, and adapters. We observe that deeper compact models tend to be as good as shallower non-compact ones and that fine-tuning a distilled model on a high-quality subset slightly boosts translation quality. Overall, we conclude that compressing MNMT models via KD is challenging, indicating immense scope for further research.",
}
@inproceedings{martins-etal-2023-empirical,
    title = "Empirical Assessment of k{NN}-{MT} for Real-World Translation Scenarios",
    author = "Martins, Pedro Henrique  and
      Alves, Jo{\~a}o  and
      Vaz, T{\^a}nia  and
      Gon{\c{c}}alves, Madalena  and
      Silva, Beatriz  and
      Buchicchio, Marianna  and
      de Souza, Jos{\'e} G. C.  and
      Martins, Andr{\'e} F. T.",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.12",
    pages = "115--124",
    abstract = "This paper aims to investigate the effectiveness of the k-Nearest Neighbor Machine Translation model (kNN-MT) in real-world scenarios. kNN-MT is a retrieval-augmented framework that combines the advantages of parametric models with non-parametric datastores built using a set of parallel sentences. Previous studies have primarily focused on evaluating the model using only the BLEU metric and have not tested kNN-MT in real world scenarios. Our study aims to fill this gap by conducting a comprehensive analysis on various datasets comprising different language pairs and different domains, using multiple automatic metrics and expert evaluated Multidimensional Quality Metrics (MQM). We compare kNN-MT with two alternate strategies: fine-tuning all the model parameters and adapter-based finetuning. Finally, we analyze the effect of the datastore size on translation quality, and we examine the number of entries necessary to bootstrap and configure the index.",
}
@inproceedings{gete-etal-2023-works,
    title = "What Works When in Context-aware Neural Machine Translation?",
    author = "Gete, Harritxu  and
      Etchegoyhen, Thierry  and
      Labaka, Gorka",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.15",
    pages = "147--156",
    abstract = "Document-level Machine Translation has emerged as a promising means to enhance automated translation quality, but it is currently unclear how effectively context-aware models use the available context during translation. This paper aims to provide insight into the current state of models based on input concatenation, with an in-depth evaluation on English{--}German and English{--}French standard datasets. We notably evaluate the impact of data bias, antecedent part-of-speech, context complexity, and the syntactic function of the elements involved in discursive phenomena. Our experimental results indicate that the selected models do improve the overall translation in context, with varying sensitivity to the different factors we examined. We notably show that the selected context-aware models operate markedly better on regular syntactic configurations involving subject antecedents and pronouns, with degraded performance as the configurations become more dissimilar.",
}
@inproceedings{kocmi-federmann-2023-large,
    title = "Large Language Models Are State-of-the-Art Evaluators of Translation Quality",
    author = "Kocmi, Tom  and
      Federmann, Christian",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.19",
    pages = "193--203",
    abstract = "We describe GEMBA, a GPT-based metric for assessment of translation quality, which works both with a reference translation and without. In our evaluation, we focus on zero-shot prompting, comparing four prompt variants in two modes, based on the availability of the reference. We investigate seven versions of GPT models, including ChatGPT. We show that our method for translation quality assessment only works with GPT 3.5 and larger models. Comparing to results from WMT22{'}s Metrics shared task, our method achieves state-of-the-art accuracy in both modes when compared to MQM-based human labels. Our results are valid on the system level for all three WMT22 Metrics shared task language pairs, namely English into German, English into Russian, and Chinese into English. This provides a first glimpse into the usefulness of pre-trained, generative large language models for quality assessment of translations. We publicly release all our code and prompt templates used for the experiments described in this work, as well as all corresponding scoring results, to allow for external validation and reproducibility.",
}
@inproceedings{moslem-etal-2023-adaptive,
    title = "Adaptive Machine Translation with Large Language Models",
    author = "Moslem, Yasmin  and
      Haque, Rejwanul  and
      Kelleher, John D.  and
      Way, Andy",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.22",
    pages = "227--237",
    abstract = "Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We observe that the translation quality with few-shot in-context learning can surpass that of strong encoder-decoder MT systems, especially for high-resource languages. Moreover, we investigate whether we can combine MT from strong encoder-decoder models with fuzzy matches, which can further improve translation quality, especially for less supported languages. We conduct our experiments across five diverse language pairs, namely English-to-Arabic (EN-AR), English-to-Chinese (EN-ZH), English-to-French (EN-FR), English-to-Kinyarwanda (EN-RW), and English-to-Spanish (EN-ES).",
}
@inproceedings{lardelli-gromann-2023-gender,
    title = "Gender-Fair Post-Editing: A Case Study Beyond the Binary",
    author = "Lardelli, Manuel  and
      Gromann, Dagmar",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.24",
    pages = "251--260",
    abstract = "Machine Translation (MT) models are well-known to suffer from gender bias, especially for gender beyond a binary conception. Due to the multiplicity of language-specific strategies for gender representation beyond the binary, debiasing MT is extremely challenging. As an alternative, we propose a case study on gender-fair post-editing. In this study, six professional translators each post-edited three English to German machine translations. For each translation, participants were instructed to use a different gender-fair language strategy, that is, gender-neutral rewording, gender-inclusive characters, and a neosystem. The focus of this study is not on translation quality but rather on the ease of integrating gender-fair language into the post-editing process. Findings from non-participant observation and interviews show clear differences in temporal and cognitive effort between participants and strategy as well as in the success of using gender-fair language.",
}
@inproceedings{hiebl-gromann-2023-quality,
    title = "Quality in Human and Machine Translation: An Interdisciplinary Survey",
    author = "Hiebl, Bettina  and
      Gromann, Dagmar",
    editor = "Nurminen, Mary  and
      Brenner, Judith  and
      Koponen, Maarit  and
      Latomaa, Sirkku  and
      Mikhailov, Mikhail  and
      Schierl, Frederike  and
      Ranasinghe, Tharindu  and
      Vanmassenhove, Eva  and
      Vidal, Sergi Alvarez  and
      Aranberri, Nora  and
      Nunziatini, Mara  and
      Escart{\'\i}n, Carla Parra  and
      Forcada, Mikel  and
      Popovic, Maja  and
      Scarton, Carolina  and
      Moniz, Helena",
    booktitle = "Proceedings of the 24th Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2023",
    address = "Tampere, Finland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2023.eamt-1.37",
    pages = "375--384",
    abstract = "Quality assurance is a central component of human and machine translation. In translation studies, translation quality focuses on human evaluation and dimensions, such as purpose, comprehensibility, target audience among many more. Within the field of machine translation, more operationalized definitions of quality lead to automated metrics relying on reference translations or quality estimation. A joint approach to defining and assessing translation quality holds the promise to be mutually beneficial. To contribute towards that objective, this systematic survey provides an interdisciplinary analysis of the concept of translation quality from both perspectives. Thereby, it seeks to inspire cross-fertilization between both fields and further development of an interdisciplinary concept of translation quality.",
}
@inproceedings{zouhar-etal-2023-poor,
    title = "Poor Man{'}s Quality Estimation: Predicting Reference-Based {MT} Metrics Without the Reference",
    author = "Zouhar, Vil{\'e}m  and
      Dhuliawala, Shehzaad  and
      Zhou, Wangchunshu  and
      Daheim, Nico  and
      Kocmi, Tom  and
      Jiang, Yuchen Eleanor  and
      Sachan, Mrinmaya",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.95",
    doi = "10.18653/v1/2023.eacl-main.95",
    pages = "1311--1325",
    abstract = "Machine translation quality estimation (QE) predicts human judgements of a translation hypothesis without seeing the reference. State-of-the-art QE systems based on pretrained language models have been achieving remarkable correlations with human judgements yet they are computationally heavy and require human annotations, which are slow and expensive to create. To address these limitations, we define the problem of metric estimation (ME) where one predicts the automated metric scores also without the reference. We show that even without access to the reference, our model can estimate automated metrics (ρ = 60{\%} for BLEU, ρ = 51{\%} for other metrics) at the sentence-level. Because automated metrics correlate with human judgements, we can leverage the ME task for pre-training a QE model. For the QE task, we find that pre-training on TER is better (ρ = 23{\%}) than training for scratch (ρ = 20{\%}).",
}
@inproceedings{yan-etal-2023-ctc,
    title = "{CTC} Alignments Improve Autoregressive Translation",
    author = "Yan, Brian  and
      Dalmia, Siddharth  and
      Higuchi, Yosuke  and
      Neubig, Graham  and
      Metze, Florian  and
      Black, Alan W  and
      Watanabe, Shinji",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.119",
    doi = "10.18653/v1/2023.eacl-main.119",
    pages = "1623--1639",
    abstract = "Connectionist Temporal Classification (CTC) is a widely used approach for automatic speech recognition (ASR) that performs conditionally independent monotonic alignment. However for translation, CTC exhibits clear limitations due to the contextual and non-monotonic nature of the task and thus lags behind attentional decoder approaches in terms of translation quality. In this work, we argue that CTC does in fact make sense for translation if applied in a joint CTC/attention framework wherein CTC{'}s core properties can counteract several key weaknesses of pure-attention models during training and decoding. To validate this conjecture, we modify the Hybrid CTC/Attention model originally proposed for ASR to support text-to-text translation (MT) and speech-to-text translation (ST). Our proposed joint CTC/attention models outperform pure-attention baselines across six benchmark translation tasks.",
}
@inproceedings{cognetta-etal-2023-parameter,
    title = "Parameter-Efficient {K}orean Character-Level Language Modeling",
    author = "Cognetta, Marco  and
      Moon, Sangwhan  and
      Wolf-sonkin, Lawrence  and
      Okazaki, Naoaki",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.172",
    doi = "10.18653/v1/2023.eacl-main.172",
    pages = "2350--2356",
    abstract = "Character-level language modeling has been shown empirically to perform well on highly agglutinative or morphologically rich languages while using only a small fraction of the parameters required by (sub)word models. Korean fits nicely into this framework, except that, like other CJK languages, it has a very large character vocabulary of 11,172 unique syllables. However, unlike Japanese Kanji and Chinese Hanzi, each Korean syllable can be uniquely factored into a small set of subcharacters, called jamo. We explore a {``}three-hot{''} scheme, where we exploit the decomposability of Korean characters to model at the syllable level but using only jamo-level representations. We find that our three-hot embedding and decoding scheme alleviates the two major issues with prior syllable- and jamo-level models. Namely, it requires fewer than 1{\%} of the embedding parameters of a syllable model, and it does not require tripling the sequence length, as with jamo models. In addition, it addresses a theoretical flaw in a prior three-hot modeling scheme. Our experiments show that, even when reducing the number of embedding parameters by 99.6{\%} (from 11.4M to just 36k), our model suffers no loss in translation quality compared to the baseline syllable model.",
}
@inproceedings{sen-etal-2023-self,
    title = "Self-training Reduces Flicker in Retranslation-based Simultaneous Translation",
    author = "Sen, Sukanta  and
      Sennrich, Rico  and
      Zhang, Biao  and
      Haddow, Barry",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.270",
    doi = "10.18653/v1/2023.eacl-main.270",
    pages = "3734--3744",
    abstract = "In simultaneous translation, the retranslation approach has the advantage of requiring no modifications to the inference engine. However, in order to reduce the undesirable flicker in the output, previous work has resorted to increasing the latency through masking, and introducing specialised inference, thus losing the simplicity of the approach. In this work, we show that self-training improves the flicker-latency tradeoff, while maintaining similar translation quality to the original. Our analysis indicates that self-training reduces flicker by controlling monotonicity. Furthermore, self-training can be combined with biased beam search to further improve the flicker-latency tradeoff.",
}
@inproceedings{kolar-kumar-2023-chatgpt,
    title = "{C}hat{GPT}{\_}{P}owered{\_}{T}ourist{\_}{A}id{\_}{A}pplications{\_}{\_}{P}roficient{\_}in{\_}{H}indi{\_}{\_}{Y}et{\_}{T}o{\_}{M}aster{\_}{T}elugu{\_}and{\_}{K}annada",
    author = "Kolar, Sanjana  and
      Kumar, Rohit",
    editor = "Chakravarthi, Bharathi R.  and
      Priyadharshini, Ruba  and
      M, Anand Kumar  and
      Thavareesan, Sajeetha  and
      Sherly, Elizabeth",
    booktitle = "Proceedings of the Third Workshop on Speech and Language Technologies for Dravidian Languages",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.dravidianlangtech-1.13",
    pages = "97--107",
    abstract = "This research investigates the effectiveness of Chat- GPT, an AI language model by OpenAI, in translating English into Hindi, Telugu, and Kannada languages, aimed at assisting tourists in India{'}s linguistically diverse environment. To measure the translation quality, a test set of 50 questions from diverse fields such as general knowledge, food, and travel was used. These were assessed by five volunteers for accuracy and fluency, and the scores were subsequently converted into a BLEU score. The BLEU score evaluates the closeness of a machine-generated translation to a human translation, with a higher score indicating better translation quality. The Hindi translations outperformed others, showcasing superior accuracy and fluency, whereas Telugu translations lagged behind. Human evaluators rated both the accuracy and fluency of translations, offering a comprehensive perspective on the language model{'}s performance.",
}
@inproceedings{dalayli-2023-use,
    title = "Use of {NLP} Techniques in Translation by {C}hat{GPT}: Case Study",
    author = "Dalayli, Feyza",
    editor = "Haddad, Amal Haddad  and
      Terryn, Ayla Rigouts  and
      Mitkov, Ruslan  and
      Rapp, Reinhard  and
      Zweigenbaum, Pierre  and
      Sharoff, Serge",
    booktitle = "Proceedings of the Workshop on Computational Terminology in NLP and Translation Studies (ConTeNTS) Incorporating the 16th Workshop on Building and Using Comparable Corpora (BUCC)",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.contents-1.3",
    pages = "19--25",
    abstract = "Use of NLP Techniques in Translation by ChatGPT: Case Study Natural Language Processing (NLP) refers to a field of study within the domain of artificial intelligence (AI) and computational linguistics that focuses on the interaction between computers and human language. NLP seeks to develop computational models and algorithms capable of understanding, analyzing, and generating natural language text and speech (Brown et al., 1990). At its core, NLP aims to bridge the gap between human language and machine understanding by employing various techniques from linguistics, computer science, and statistics. It involves the application of linguistic and computational theories to process, interpret, and extract meaningful information from unstructured textual data (Bahdanau, Cho and Bengio, 2015). Researchers and practitioners in NLP employ diverse methodologies, including rule-based approaches, statistical models, machine learning techniques (such as neural networks), and more recently, deep learning architectures. These methodologies enable the development of robust algorithms that can learn from large-scale language data to improve the accuracy and effectiveness of language processing systems (Nilsson, 2010). NLP has numerous real-world applications across various domains, including information retrieval, virtual assistants, chatbots, social media analysis, sentiment monitoring, automated translation services, and healthcare, among others (kaynak). As the field continues to advance, NLP strives to overcome challenges such as understanding the nuances of human language, handling ambiguity, context sensitivity, and incorporating knowledge from diverse sources to enable machines to effectively communicate and interact with humans in a more natural and intuitive manner. Natural Language Processing (NLP) and translation are interconnected fields that share a symbiotic relationship, as NLP techniques and methodologies greatly contribute to the advancement and effectiveness of machine translation systems. NLP, a subfield of artificial intelligence (AI), focuses on the interaction between computers and human language. It encompasses a wide range of tasks, including text analysis, syntactic and semantic parsing, sentiment analysis, information extraction, and machine translation (Bahdanau, Cho and Bengio, 2014). NMT models employ deep learning architectures, such as recurrent neural networks (RNNs) and more specifically, long short-term memory (LSTM) networks, to learn the mapping between source and target language sentences. These models are trained on large-scale parallel corpora, consisting of aligned sentence pairs in different languages. The training process involves optimizing model parameters to minimize the discrepancy between predicted translations and human-generated translations (Wu et al., 2016) NLP techniques are crucial at various stages of machine translation. Preprocessing techniques, such as tokenization, sentence segmentation, and morphological analysis, help break down input text into meaningful linguistic units, making it easier for translation models to process and understand the content. Syntactic and semantic parsing techniques aid in capturing the structural and semantic relationships within sentences, improving the overall coherence and accuracy of translations. Furthermore, NLP-based methods are employed for handling specific translation challenges, such as handling idiomatic expressions, resolving lexical ambiguities, and addressing syntactic divergences between languages. For instance, statistical alignment models, based on NLP algorithms, enable the identification of correspondences between words or phrases in source and target languages, facilitating the generation of more accurate translations (kaynak). Several studies have demonstrated the effectiveness of NLP techniques in enhancing machine translation quality. For example, Bahdanau et al. (2015) introduced the attention mechanism, an NLP technique that enables NMT models to focus on relevant parts of the source sentence during translation. This attention mechanism significantly improved the translation quality of neural machine translation models. ChatGPT is a language model developed by OpenAI that utilizes the principles of Natural Language Processing (NLP) for various tasks, including translations. NLP is a field of artificial intelligence that focuses on the interaction between computers and human language. It encompasses a range of techniques and algorithms for processing, analyzing, and understanding natural language. When it comes to translation, NLP techniques can be applied to facilitate the conversion of text from one language to another. ChatGPT employs a sequence-to-sequence model, a type of neural network architecture commonly used in machine translation tasks. This model takes an input sequence in one language and generates a corresponding output sequence in the target language (OpenAI, 2023). The training process for ChatGPT involves exposing the model to large amounts of multilingual data, allowing it to learn patterns, syntax, and semantic relationships across different languages. This exposure enables the model to develop a general understanding of language structures and meanings, making it capable of performing translation tasks. To enhance translation quality, ChatGPT leverages the Transformer architecture, which has been highly successful in NLP tasks. Transformers utilize attention mechanisms, enabling the model to focus on different parts of the input sequence during the translation process. This attention mechanism allows the model to capture long-range dependencies and improve the overall coherence and accuracy of translations. Additionally, techniques such as subword tokenization, which divides words into smaller units, are commonly employed in NLP translation systems like ChatGPT. Subword tokenization helps handle out-of-vocabulary words and improves the model{'}s ability to handle rare or unknown words (GPT-4 Technical Report, 2023). As can be seen, there have been significant developments in artificial intelligence translations thanks to NLP. However, it is not possible to say that it has fully reached the quality of translation made by people. The only goal in artificial intelligence translations is to reach translations made by humans. In general, there are some fundamental differences between human and ChatGPT translations. Human-made translations and translations generated by ChatGPT (or similar language models) have several key differences (Kelly and Zetzsche, 2014; Koehn, 2010; Sutskever, Vinyals and Le, 2014; Costa-juss{\`a} and Fonollosa, 2018) Translation Quality: Human translators are capable of producing high-quality translations with a deep understanding of both the source and target languages. They can accurately capture the nuances, cultural references, idioms, and context of the original text. On the other hand, ChatGPT translations can sometimes be less accurate or may not fully grasp the intended meaning due to the limitations of the training data and the model{'}s inability to comprehend context in the same way a human can. While ChatGPT can provide reasonable translations, they may lack the finesse and precision of a human translator. Natural Language Processing: Human translators are skilled at processing and understanding natural language, taking into account the broader context, cultural implications, and the intended audience. They can adapt their translations to suit the target audience, tone, and purpose of the text. ChatGPT, although trained on a vast amount of text data, lacks the same level of natural language understanding. It often relies on pattern matching and statistical analysis to generate translations, which can result in less nuanced or contextually appropriate outputs. Subject Matter Expertise: Human translators often specialize in specific domains or subject areas, allowing them to have deep knowledge and understanding of technical or specialized terminology. They can accurately translate complex or industry-specific texts, ensuring the meaning is preserved. ChatGPT, while having access to a wide range of general knowledge, may struggle with domain-specific vocabulary or terminology, leading to inaccuracies or incorrect translations in specialized texts. Cultural Sensitivity: Human translators are well-versed in the cultural nuances of both the source and target languages. They can navigate potential pitfalls, adapt the translation to the cultural context, and avoid unintended offensive or inappropriate language choices. ChatGPT lacks this level of cultural sensitivity and may produce translations that are culturally tone-deaf or insensitive, as it lacks the ability to understand the subtleties and implications of language choices. Revision and Editing: Human translators go through an iterative process of revision and editing to refine their translations, ensuring accuracy, clarity, and quality. They can self-correct errors and refine their translations based on feedback or additional research. ChatGPT, while capable of generating translations, does not have the same ability to self-correct or improve based on feedback. It generates translations in a single pass, without the iterative refinement process that humans can employ. In summary, while ChatGPT can be a useful tool for generating translations, human-made translations generally outperform machine-generated translations in terms of quality, accuracy, contextuality, cultural sensitivity, and domain-specific expertise. In conclusion, NLP and machine translation are closely intertwined, with NLP providing essential tools, methodologies, and techniques that contribute to the development and improvement of machine translation systems. The integration of NLP methods has led to significant advancements in translation accuracy, fluency, and the ability to handle various linguistic complexities. As NLP continues to evolve, its impact on the field of machine translation is expected to grow, enabling the creation of more sophisticated and context-aware translation systems. On the basis of all this information, in this research, it is aimed to compare the translations from English to Turkish made by ChatGPT, one of the most advanced artificial intelligences, with the translations made by humans. In this context, an academic 1 page English text was chosen. The text was translated by both ChatGPT and a translator who is an academic in the field of translation and has 10 years of experience. Afterwards, two different translations were examined comparatively by 5 different translators who are experts in their fields. Semi-structured in-depth interviews were conducted with these translators. The aim of this study is to reveal the role of artificial intelligence tools in translation, which are increasing day by day and suggesting that there will be no need for language learning in the future. On the other hand, many translators argue that artificial intelligence and human translations can be understood. Therefore, if artificial intelligence is successful, there will be no profession called translator in the future. This research seems to be very useful in terms of shedding light on the future. The method of this research is semi-structured in-depth interview. References Bahdanau, D., Cho, K. and Bengio Y. (2015). Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations. Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. A. (1990) statistical approach to machine translation. Computational linguistics 16, 2, 79{--}85. Costa-juss{\`a}, M. R., {\&} Fonollosa, J. A. R. (2018). {``}An Overview of Neural Machine Translation.{''} IEEE Transactions on Neural Networks and Learning Systems. GPT-4 Technical Report (2023). https://arxiv.org/abs/2303.08774. Kelly, N. and Zetzsche, J. (2014). Found in Translation: How Language Shapes Our Lives and Transforms the World. USA: Penguin Book. Koehn, P. (2010). {``}Statistical Machine Translation.{''} Cambridge University Press. Nilsson, N. J. (2010). The Quest For AI- A History Of Ideas And Achievements. http://ai.standford.edu/ nilsson/. OpenAI (2023). https://openai.com/blog/chatgpt/. Sutskever, I., Vinyals, O., {\&} Le, Q. V. (2014). {``}Sequence to Sequence Learning with Neural Networks.{''} Advances in Neural Information Processing Systems. Wu,Y. Schuster, M., Chen, Z., Le, Q. V. and Norouzi M. (2016). Google{'}s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. https://arxiv.org/pdf/1609.08144.pdf.",
}
@inproceedings{graichen-etal-2023-enriching,
    title = "Enriching {W}ay{\'u}unaiki-{S}panish Neural Machine Translation with Linguistic Information",
    author = "Graichen, Nora  and
      Van Genabith, Josef  and
      Espa{\~n}a-bonet, Cristina",
    editor = "Mager, Manuel  and
      Ebrahimi, Abteen  and
      Oncevay, Arturo  and
      Rice, Enora  and
      Rijhwani, Shruti  and
      Palmer, Alexis  and
      Kann, Katharina",
    booktitle = "Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.americasnlp-1.9",
    doi = "10.18653/v1/2023.americasnlp-1.9",
    pages = "67--83",
    abstract = "We present the first neural machine translation system for the low-resource language pair Way{\'u}unaiki{--}Spanish and explore strategies to inject linguistic knowledge into the model to improve translation quality. We explore a wide range of methods and combine complementary approaches. Results indicate that incorporating linguistic information through linguistically motivated subword segmentation, factored models, and pretrained embeddings helps the system to generate improved translations, with the segmentation contributing most. In order to evaluate translation quality in a general domain and go beyond the available religious domain data, we gather and make publicly available a new test set and supplementary material. Although translation quality as measured with automatic metrics is low, we hope these resources will facilitate and support further research on Way{\'u}unaiki.",
}
@inproceedings{jin-etal-2023-morphological,
    title = "Morphological and Semantic Evaluation of {A}ncient {C}hinese Machine Translation",
    author = "Jin, Kai  and
      Zhao, Dan  and
      Liu, Wuying",
    editor = "Anderson, Adam  and
      Gordin, Shai  and
      Li, Bin  and
      Liu, Yudong  and
      Passarotti, Marco C.",
    booktitle = "Proceedings of the Ancient Language Processing Workshop",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.alp-1.11",
    pages = "96--102",
    abstract = "Machine translation (MT) of ancient Chinese texts presents unique challenges due to the complex grammatical structures, cultural nuances, and polysemy of the language. This paper focuses on evaluating the translation quality of different platforms for ancient Chinese texts using The Analects as a case study. The evaluation is conducted using the BLEU, LMS, and ESS metrics, and the platforms compared include three machine translation platforms (Baidu Translate, Bing Microsoft Translator, and DeepL), and one language generation model ChatGPT that can engage in translation endeavors. Results show that Baidu performs the best, surpassing the other platforms in all three metrics, while ChatGPT ranks second and demonstrates unique advantages. The translations generated by ChatGPT are deemed highly valuable as references. The study contributes to understanding the challenges of MT for ancient Chinese texts and provides insights for users and researchers in this field. It also highlights the importance of considering specific domain requirements when evaluating MT systems.",
}
@inproceedings{sekizawa-etal-2023-constructing,
    title = "Constructing Multilingual Code Search Dataset Using Neural Machine Translation",
    author = "Sekizawa, Ryo  and
      Duan, Nan  and
      Lu, Shuai  and
      Yanaka, Hitomi",
    editor = "Padmakumar, Vishakh  and
      Vallejo, Gisela  and
      Fu, Yao",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-srw.10",
    doi = "10.18653/v1/2023.acl-srw.10",
    pages = "69--75",
    abstract = "Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate them on multiple code search test sets. Our results show that the model pre-trained with all natural and programming language data has performed best in most cases. By applying back-translation data filtering to our dataset, we demonstrate that the translation quality affects the model{'}s performance to a certain extent, but the data size matters more.",
}
@inproceedings{fernandes-etal-2023-translation,
    title = "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
    author = "Fernandes, Patrick  and
      Yin, Kayo  and
      Liu, Emmy  and
      Martins, Andr{\'e}  and
      Neubig, Graham",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.36",
    doi = "10.18653/v1/2023.acl-long.36",
    pages = "606--626",
    abstract = "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at \url{https://github.com/neulab/contextual-mt}",
}
@inproceedings{huang-etal-2023-improving,
    title = "Improving Translation Quality Estimation with Bias Mitigation",
    author = "Huang, Hui  and
      Wu, Shuangzhi  and
      Chen, Kehai  and
      Di, Hui  and
      Yang, Muyun  and
      Zhao, Tiejun",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.121",
    doi = "10.18653/v1/2023.acl-long.121",
    pages = "2175--2190",
    abstract = "State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation performance. Our method is based on the contrastive learning between clean and noisy sentence pairs. We first introduce noise to the target side of the parallel sentence pair, forming the negative samples. With the original parallel pairs as the positive sample, the QE model is contrastively trained to distinguish the positive samples from the negative ones. This objective is jointly trained with the regression-style quality estimation, so as to prevent the QE model from overfitting to monolingual features. Experiments on WMT QE evaluation datasets demonstrate that our method improves the estimation performance by a large margin while mitigating the bias.",
}
@inproceedings{jon-bojar-2023-breeding,
    title = "Breeding Machine Translations: Evolutionary approach to survive and thrive in the world of automated evaluation",
    author = "Jon, Josef  and
      Bojar, Ond{\v{r}}ej",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.122",
    doi = "10.18653/v1/2023.acl-long.122",
    pages = "2191--2212",
    abstract = "We propose a genetic algorithm (GA) based method for modifying $n$-best lists produced by a machine translation (MT) system. Our method offers an innovative approach to improving MT quality and identifying weaknesses in evaluation metrics. Using common GA operations (mutation and crossover) on a list of hypotheses in combination with a fitness function (an arbitrary MT metric), we obtain novel and diverse outputs with high metric scores. With a combination of multiple MT metrics as the fitness function, the proposed method leads to an increase in translation quality as measured by other held-out automatic metrics.With a single metric (including popular ones such as COMET) as the fitness function, we find blind spots and flaws in the metric. This allows for an automated search for adversarial examples in an arbitrary metric, without prior assumptions on the form of such example. As a demonstration of the method, we create datasets of adversarial examples and use them to show that reference-free COMET is substantially less robust than the reference-based version.",
}
@inproceedings{koishekenov-etal-2023-memory,
    title = "Memory-efficient {NLLB}-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model",
    author = "Koishekenov, Yeskendir  and
      Berard, Alexandre  and
      Nikoulina, Vassilina",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.198",
    doi = "10.18653/v1/2023.acl-long.198",
    pages = "3567--3585",
    abstract = "The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that enables the removal of up to 80{\%} of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts.",
}
@inproceedings{cao-etal-2023-bridging,
    title = "Bridging the Domain Gaps in Context Representations for $k$-Nearest Neighbor Neural Machine Translation",
    author = "Cao, Zhiwei  and
      Yang, Baosong  and
      Lin, Huan  and
      Wu, Suhang  and
      Wei, Xiangpeng  and
      Liu, Dayiheng  and
      Xie, Jun  and
      Zhang, Min  and
      Su, Jinsong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.321",
    doi = "10.18653/v1/2023.acl-long.321",
    pages = "5841--5853",
    abstract = "$k$-Nearest neighbor machine translation ($k$NN-MT) has attracted increasing attention due to its ability to non-parametrically adapt to new translation domains. By using an upstream NMT model to traverse the downstream training corpus, it is equipped with a datastore containing vectorized key-value pairs, which are retrieved during inference to benefit translation.However, there often exists a significant gap between upstream and downstream domains, which hurts the datastore retrieval and the final translation quality.To deal with this issue, we propose a novel approach to boost the datastore retrieval of $k$NN-MT by reconstructing the original datastore.Concretely, we design a reviser to revise the key representations, making them better fit for the downstream domain. The reviser is trained using the collected semantically-related key-queries pairs, and optimized by two proposed losses: one is the key-queries semantic distance ensuring each revised key representation is semantically related to its corresponding queries, and the other is an L2-norm loss encouraging revised key representations to effectively retain the knowledge learned by the upstream NMT model. Extensive experiments on domain adaptation tasks demonstrate that our method can effectively boost the datastore retrieval and translation quality of $k$NN-MT.Our code is available at \url{https://github.com/DeepLearnXMU/Revised-knn-mt}.",
}
@inproceedings{zhang-etal-2023-understanding,
    title = "Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation",
    author = "Zhang, Huaao  and
      Wang, Qiang  and
      Qin, Bo  and
      Shi, Zelin  and
      Wang, Haibo  and
      Chen, Ming",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.332",
    doi = "10.18653/v1/2023.acl-long.332",
    pages = "6029--6042",
    abstract = "In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. We identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO, are blind to this issue due to oversimplified constraint settings. To solve it, we create a new challenging test set of English-German, increasing the average constraint count per sentence from 1.1{\textasciitilde}1.7 to 6.1 and the length per target constraint from 1.1{\textasciitilde}1.2 words to 3.4 words. Then we find that PH and CS methods degrade as the number of constraints increases, but they have complementary strengths. Specifically, PH is better at retaining high constraint accuracy but lower translation quality as measured by BLEU and COMET scores. In contrast, CS has the opposite results. Based on these observations, we propose a simple but effective method combining the advantages of PH and CS. This approach involves training a model like PH to predict the term labels, and then during inference replacing those labels with target terminology text like CS, so that the subsequent generation is aware of the target term content. Extensive experimental results show that this approach can achieve high constraint accuracy and translation quality simultaneously, regardless of the number or length of constraints.",
}
@inproceedings{wang-etal-2023-easy,
    title = "Easy Guided Decoding in Providing Suggestions for Interactive Machine Translation",
    author = "Wang, Ke  and
      Ge, Xin  and
      Wang, Jiayi  and
      Zhang, Yuqi  and
      Zhao, Yu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.434",
    doi = "10.18653/v1/2023.acl-long.434",
    pages = "7840--7852",
    abstract = "Machine translation technology has made great progress in recent years, but it cannot guarantee error-free results. Human translators perform post-editing on machine translations to correct errors in the scene of computer aided translation. In favor of expediting the post-editing process, many works have investigated machine translation in interactive modes, in which machines can automatically refine the rest of translations constrained by human{'}s edits. Translation Suggestion (TS), as an interactive mode to assist human translators, requires machines to generate alternatives for specific incorrect words or phrases selected by human translators. In this paper, we utilize the parameterized objective function of neural machine translation (NMT) and propose a novel constrained decoding algorithm, namely Prefix-Suffix Guided Decoding (PSGD), to deal with the TS problem without additional training. Compared to state-of-the-art lexical-constrained decoding method, PSGD improves translation quality by an average of 10.6 BLEU and reduces time overhead by an average of 63.4{\%} on benchmark datasets. Furthermore, on both the WeTS and the WMT 2022 Translation Suggestion datasets, it is superior over other supervised learning systems trained with TS annotated data.",
}
@inproceedings{wei-etal-2023-text,
    title = "Text Style Transfer Back-Translation",
    author = "Wei, Daimeng  and
      Wu, Zhanglin  and
      Shang, Hengchao  and
      Li, Zongyao  and
      Wang, Minghan  and
      Guo, Jiaxin  and
      Chen, Xiaoyu  and
      Yu, Zhengzhe  and
      Yang, Hao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.441",
    doi = "10.18653/v1/2023.acl-long.441",
    pages = "7944--7959",
    abstract = "Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a generalized data augmentation method. Our training code and text style transfer model are open-sourced.",
}
@inproceedings{chen-etal-2023-blaser,
    title = "{BLASER}: A Text-Free Speech-to-Speech Translation Evaluation Metric",
    author = "Chen, Mingda  and
      Duquenne, Paul-Ambroise  and
      Andrews, Pierre  and
      Kao, Justine  and
      Mourachko, Alexandre  and
      Schwenk, Holger  and
      Costa-juss{\`a}, Marta R.",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.504",
    doi = "10.18653/v1/2023.acl-long.504",
    pages = "9064--9079",
    abstract = "End-to-End speech-to-speech translation (S2ST) is generally evaluated with text-based metrics. This means that generated speech has to be automatically transcribed, making the evaluation dependent on the availability and quality of automatic speech recognition (ASR) systems. In this paper, we propose a text-free evaluation metric for end-to-end S2ST, named BLASER, to avoid the dependency on ASR systems. BLASER leverages a multilingual multimodal encoder to directly encode the speech segments for source input, translation output and reference into a shared embedding space and computes a score of the translation quality that can be used as a proxy to human evaluation. To evaluate our approach, we construct training and evaluation sets from more than 40k human annotations covering seven language directions. The best results of BLASER are achieved by training with supervision from human rating scores. We show that when evaluated at the sentence level, BLASER correlates significantly better with human judgment compared to ASR dependent metrics including ASR-SENTBLEU in all translation directions and ASR-COMET in five of them. Our analysis shows combining speech and text as inputs to BLASER does not increase the correlation with human scores, but best correlations are achieved when using speech, which motivates the goal of our research. Moreover, we show that using ASR for references is detrimental for text-based metrics.",
}
@inproceedings{briakou-etal-2023-searching,
    title = "Searching for Needles in a Haystack: On the Role of Incidental Bilingualism in {P}a{LM}{'}s Translation Capability",
    author = "Briakou, Eleftheria  and
      Cherry, Colin  and
      Foster, George",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.524",
    doi = "10.18653/v1/2023.acl-long.524",
    pages = "9432--9452",
    abstract = "Large, multilingual language models exhibit surprisingly good zero- or few-shot machine translation capabilities, despite having never seen the intentionally-included translation examples provided to typical neural translation systems. We investigate the role of incidental bilingualism{---}the unintentional consumption of bilingual signals, including translation examples{---}in explaining the translation capabilities of large language models, taking the Pathways Language Model (PaLM) as a case study. We introduce a mixed-method approach to measure and understand incidental bilingualism at scale. We show that PaLM is exposed to over 30 million translation pairs across at least 44 languages. Furthermore, the amount of incidental bilingual content is highly correlated with the amount of monolingual in-language content for non-English languages. We relate incidental bilingual content to zero-shot prompts and show that it can be used to mine new prompts to improve PaLM{'}s out-of-English zero-shot translation quality. Finally, in a series of small-scale ablations, we show that its presence has a substantial impact on translation capabilities, although this impact diminishes with model scale.",
}
@inproceedings{santilli-etal-2023-accelerating,
    title = "Accelerating Transformer Inference for Translation via Parallel Decoding",
    author = "Santilli, Andrea  and
      Severino, Silvio  and
      Postolache, Emilian  and
      Maiorca, Valentino  and
      Mancusi, Michele  and
      Marin, Riccardo  and
      Rodola, Emanuele",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.689",
    doi = "10.18653/v1/2023.acl-long.689",
    pages = "12336--12355",
    abstract = "Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38{\%} w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure.",
}
@inproceedings{pires-etal-2023-learning,
    title = "Learning Language-Specific Layers for Multilingual Machine Translation",
    author = "Pires, Telmo  and
      Schmidt, Robin  and
      Liao, Yi-Hsiu  and
      Peitz, Stephan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.825",
    doi = "10.18653/v1/2023.acl-long.825",
    pages = "14767--14783",
    abstract = "Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English).On the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower. In this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and 1.9 chrF (2.2 spBLEU) on a shared decoder one.",
}
@inproceedings{heafield-etal-2022-findings,
    title = "Findings of the {WMT} 2022 Shared Task on Efficient Translation",
    author = "Heafield, Kenneth  and
      Zhang, Biao  and
      Nail, Graeme  and
      Van Der Linde, Jelmer  and
      Bogoychev, Nikolay",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.4",
    pages = "100--108",
    abstract = "The machine translation efficiency task challenges participants to make their systems faster and smaller with minimal impact on translation quality. How much quality to sacrifice for efficiency depends upon the application, so participants were encouraged to make multiple submissions covering the space of trade-offs. In total, there were 76 submissions from 5 teams. The task covers GPU, single-core CPU, and multi-core CPU hardware tracks as well as batched throughput or single-sentence latency conditions. Submissions showed hundreds of millions of words can be translated for a dollar, average latency is 3.5{--}25 ms, and models fit in 7.5{--}900 MB.",
}
@inproceedings{corral-saralegi-2022-gender,
    title = "Gender Bias Mitigation for {NMT} Involving Genderless Languages",
    author = "Corral, Ander  and
      Saralegi, Xabier",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.10",
    pages = "165--176",
    abstract = "It has been found that NMT systems have a strong preference towards social defaults and biases when translating certain occupations, which due to their widespread use, can unintentionally contribute to amplifying and perpetuating these patterns. In that sense, this work focuses on sentence-level gender agreement between gendered entities and occupations when translating from genderless languages to languages with grammatical gender. Specifically, we address the Basque to Spanish translation direction for which bias mitigation has not been addressed. Gender information in Basque is explicit in neither the grammar nor the morphology. It is only present in a limited number of gender specific common nouns and person proper names. We propose a template-based fine-tuning strategy with explicit gender tags to provide a stronger gender signal for the proper inflection of occupations. This strategy is compared against systems fine-tuned on real data extracted from Wikipedia biographies. We provide a detailed gender bias assessment analysis and perform a template ablation study to determine the optimal set of templates. We report a substantial gender bias mitigation (up to 50{\%} on gender bias scores) while keeping the original translation quality.",
}
@inproceedings{amrhein-haddow-2022-dont,
    title = "Don{'}t Discard Fixed-Window Audio Segmentation in Speech-to-Text Translation",
    author = "Amrhein, Chantal  and
      Haddow, Barry",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.13",
    pages = "203--219",
    abstract = "For real-life applications, it is crucial that end-to-end spoken language translation models perform well on continuous audio, without relying on human-supplied segmentation. For online spoken language translation, where models need to start translating before the full utterance is spoken,most previous work has ignored the segmentation problem. In this paper, we compare various methods for improving models{'} robustness towards segmentation errors and different segmentation strategies in both offline and online settings and report results on translation quality, flicker and delay. Our findings on five different language pairs show that a simple fixed-window audio segmentation can perform surprisingly well given the right conditions.",
}
@inproceedings{jon-etal-2022-cuni,
    title = "{CUNI}-Bergamot Submission at {WMT}22 General Translation Task",
    author = "Jon, Josef  and
      Popel, Martin  and
      Bojar, Ond{\v{r}}ej",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.21",
    pages = "280--289",
    abstract = "We present the CUNI-Bergamot submission for the WMT22 General translation task. We compete in English-Czech direction. Our submission further explores block backtranslation techniques. Compared to the previous work, we measure performance in terms of COMET score and named entities translation accuracy. We evaluate performance of MBR decoding compared to traditional mixed backtranslation training and we show a possible synergy when using both of the techniques simultaneously. The results show that both approaches are effective means of improving translation quality and they yield even better results when combined.",
}
@inproceedings{kalkar-etal-2022-kyb,
    title = "{KYB} General Machine Translation Systems for {WMT}22",
    author = "Kalkar, Shivam  and
      Matsuzaki, Yoko  and
      Li, Ben",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.22",
    pages = "290--294",
    abstract = "We here describe our neural machine translation system for general machine translation shared task in WMT 2022. Our systems are based on the Transformer (Vaswani et al., 2017) with base settings. We explore the high-efficiency model training strategies, aimed to train a model with high-accuracy by using small model and a reasonable amount of data. We performed fine-tuning and ensembling with N-best ranking in English to/from Japanese directions. We found that fine-tuning by filtered JParaCrawl data set leads to better translations for both of direction in English to/from Japanese models. In English to Japanese direction model, ensembling and N-best ranking of 10 different checkpoints improved translations. By comparing with other online translation service, we found that our model achieved a great translation quality.",
}
@inproceedings{popel-etal-2022-cuni,
    title = "{CUNI} Systems for the {WMT} 22 {C}zech-{U}krainian Translation Task",
    author = "Popel, Martin  and
      Libovick{\'y}, Jind{\v{r}}ich  and
      Helcl, Jind{\v{r}}ich",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.30",
    pages = "352--357",
    abstract = "We present Charles University submissions to the WMT 22 GeneralTranslation Shared Task on Czech-Ukrainian and Ukrainian-Czech machine translation. We present two constrained submissions based on block back-translation and tagged back-translation and experiment with rule-basedromanization of Ukrainian. Our results show that the romanization onlyhas a minor effect on the translation quality. Further, we describe Charles Translator,a system that was developed in March 2022 as a response to the migrationfrom Ukraine to the Czech Republic. Compared to our constrained systems,it did not use the romanization and used some proprietary data sources.",
}
@inproceedings{mukherjee-shrivastava-2022-reuse,
    title = "{REUSE}: {RE}ference-free {U}n{S}upervised Quality Estimation Metric",
    author = "Mukherjee, Ananya  and
      Shrivastava, Manish",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.50",
    pages = "564--568",
    abstract = "This paper describes our submission to the WMT2022 shared metrics task. Our unsupervised metric estimates the translation quality at chunk-level and sentence-level. Source and target sentence chunks are retrieved by using a multi-lingual chunker. The chunk-level similarity is computed by leveraging BERT contextual word embeddings and sentence similarity scores are calculated by leveraging sentence embeddings of Language-Agnostic BERT models. The final quality estimation score is obtained by mean pooling the chunk-level and sentence-level similarity scores. This paper outlines our experiments and also reports the correlation with human judgements for en-de, en-ru and zh-en language pairs of WMT17, WMT18 and WMT19 test sets.",
}
@inproceedings{qin-etal-2022-royalflush,
    title = "The {R}oyal{F}lush System for the {WMT} 2022 Efficiency Task",
    author = "Qin, Bo  and
      Jia, Aixin  and
      Wang, Qiang  and
      Lu, Jianning  and
      Pan, Shuqin  and
      Wang, Haibo  and
      Chen, Ming",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.65",
    pages = "671--676",
    abstract = "This paper describes the submission of the RoyalFlush neural machine translation system for the WMT 2022 translation efficiency task. Unlike the commonly used autoregressive translation system, we adopted a two-stage translation paradigm called Hybrid Regression Translation (HRT) to combine the advantages of autoregressive and non-autoregressive translation. Specifically, HRT first autoregressively generates a discontinuous sequence (e.g., make a prediction every k tokens, k1) and then fills in all previously skipped tokens at once in a non-autoregressive manner. Thus, we can easily trade off the translation quality and speed by adjusting k. In addition, by integrating other modeling techniques (e.g., sequence-level knowledge distillation and deep-encoder-shallow-decoder layer allocation strategy) and a mass of engineering efforts, HRT improves 80{\%} inference speed and achieves equivalent translation performance with the same-capacity AT counterpart. Our fastest system reaches 6k+ words/second on the GPU latency setting, estimated to be about 3.1x faster than the last year{'}s winner.",
}
@inproceedings{lupo-etal-2022-focused,
    title = "Focused Concatenation for Context-Aware Neural Machine Translation",
    author = "Lupo, Lorenzo  and
      Dinarelli, Marco  and
      Besacier, Laurent",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.77",
    pages = "830--842",
    abstract = "A straightforward approach to context-aware neural machine translation consists in feeding the standard encoder-decoder architecture with a window of consecutive sentences, formed by the current sentence and a number of sentences from its context concatenated to it. In this work, we propose an improved concatenation approach that encourages the model to focus on the translation of the current sentence, discounting the loss generated by target context. We also propose an additional improvement that strengthen the notion of sentence boundaries and of relative sentence distance, facilitating model compliance to the context-discounted objective. We evaluate our approach with both average-translation quality metrics and contrastive test sets for the translation of inter-sentential discourse phenomena, proving its superiority to the vanilla concatenation approach and other sophisticated context-aware systems.",
}
@inproceedings{hoang-etal-2022-revisiting,
    title = "Revisiting Locality Sensitive Hashing for Vocabulary Selection in Fast Neural Machine Translation",
    author = "Hoang, Hieu  and
      Junczys-dowmunt, Marcin  and
      Grundkiewicz, Roman  and
      Khayrallah, Huda",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.79",
    pages = "855--869",
    abstract = "Neural machine translation models often contain large target vocabularies. The calculation of logits, softmax and beam search is computationally costly over so many classes. We investigate the use of locality sensitive hashing (LSH) to reduce the number of vocabulary items that must be evaluated and explore the relationship between the hashing algorithm, translation speed and quality. Compared to prior work, our LSH-based solution does not require additional augmentation via word-frequency lists or alignments. We propose a training procedure that produces models, which, when combined with our LSH inference algorithm increase translation speed by up to 87{\%} over the baseline, while maintaining translation quality as measured by BLEU. Apart from just using BLEU, we focus on minimizing search errors compared to the full softmax, a much harsher quality criterion.",
}
@inproceedings{abdulmumin-etal-2022-separating,
    title = "Separating Grains from the Chaff: Using Data Filtering to Improve Multilingual Translation for Low-Resourced {A}frican Languages",
    author = "Abdulmumin, Idris  and
      Beukman, Michael  and
      Alabi, Jesujoba  and
      Emezue, Chris Chinenye  and
      Chimoto, Everlyn  and
      Adewumi, Tosin  and
      Muhammad, Shamsuddeen  and
      Adeyemi, Mofetoluwa  and
      Yousuf, Oreen  and
      Singh, Sahib  and
      Gwadabe, Tajuddeen",
    editor = {Koehn, Philipp  and
      Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Jimeno Yepes, Antonio  and
      Kocmi, Tom  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Popel, Martin  and
      Turchi, Marco  and
      Zampieri, Marcos},
    booktitle = "Proceedings of the Seventh Conference on Machine Translation (WMT)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.wmt-1.98",
    pages = "1001--1014",
    abstract = "We participated in the WMT 2022 Large-Scale Machine Translation Evaluation for the African Languages Shared Task. This work describes our approach, which is based on filtering the given noisy data using a sentence-pair classifier that was built by fine-tuning a pre-trained language model. To train the classifier, we obtain positive samples (i.e. high-quality parallel sentences) from a gold-standard curated dataset and extract negative samples (i.e. low-quality parallel sentences) from automatically aligned parallel data by choosing sentences with low alignment scores. Our final machine translation model was then trained on filtered data, instead of the entire noisy dataset. We empirically validate our approach by evaluating on two common datasets and show that data filtering generally improves overall translation quality, in some cases even significantly.",
}
@inproceedings{nakatani-etal-2022-comparing,
    title = "Comparing {BERT}-based Reward Functions for Deep Reinforcement Learning in Machine Translation",
    author = "Nakatani, Yuki  and
      Kajiwara, Tomoyuki  and
      Ninomiya, Takashi",
    booktitle = "Proceedings of the 9th Workshop on Asian Translation",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    url = "https://aclanthology.org/2022.wat-1.2",
    pages = "37--43",
    abstract = "In text generation tasks such as machine translation, models are generally trained using cross-entropy loss. However, mismatches between the loss function and the evaluation metric are often problematic. It is known that this problem can be addressed by direct optimization to the evaluation metric with reinforcement learning. In machine translation, previous studies have used BLEU to calculate rewards for reinforcement learning, but BLEU is not well correlated with human evaluation. In this study, we investigate the impact on machine translation quality through reinforcement learning based on evaluation metrics that are more highly correlated with human evaluation. Experimental results show that reinforcement learning with BERT-based rewards can improve various evaluation metrics.",
}
@inproceedings{kondo-komachi-2022-tmu,
    title = "{TMU} {NMT} System with Automatic Post-Editing by Multi-Source {L}evenshtein Transformer for the Restricted Translation Task of {WAT} 2022",
    author = "Kondo, Seiichiro  and
      Komachi, Mamoru",
    booktitle = "Proceedings of the 9th Workshop on Asian Translation",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Conference on Computational Linguistics",
    url = "https://aclanthology.org/2022.wat-1.4",
    pages = "51--58",
    abstract = "In this paper, we describe our TMU English{--}Japanese systems submitted to the restricted translation task at WAT 2022 (Nakazawa et al., 2022). In this task, we translate an input sentence with the constraint that certain words or phrases (called restricted target vocabularies (RTVs)) should be contained in the output sentence. To satisfy this constraint, we address this task using a combination of two techniques. One is lexical-constraint-aware neural machine translation (LeCA) (Chen et al., 2020), which is a method of adding RTVs at the end of input sentences. The other is multi-source Levenshtein transformer (MSLevT) (Wan et al., 2020), which is a non-autoregressive method for automatic post-editing. Our system generates translations in two steps. First, we generate the translation using LeCA. Subsequently, we filter the sentences that do not satisfy the constraints and post-edit them with MSLevT. Our experimental results reveal that 100{\%} of the RTVs can be included in the generated sentences while maintaining the translation quality of the LeCA model on both English to Japanese (En→Ja) and Japanese to English (Ja→En) tasks. Furthermore, the method used in previous studies requires an increase in the beam size to satisfy the constraints, which is computationally expensive. In contrast, the proposed method does not require a similar increase and can generate translations faster.",
}
@article{freitag-etal-2022-high,
    title = "High Quality Rather than High Model Probability: Minimum {B}ayes Risk Decoding with Neural Metrics",
    author = "Freitag, Markus  and
      Grangier, David  and
      Tan, Qijun  and
      Liang, Bowen",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "10",
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.tacl-1.47",
    doi = "10.1162/tacl_a_00491",
    pages = "811--825",
    abstract = "In Neural Machine Translation, it is typically assumed that the sentence with the highest estimated probability should also be the translation with the highest quality as measured by humans. In this work, we question this assumption and show that model estimates and translation quality only vaguely correlate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to optimize diverse automated metrics of translation quality as an alternative inference strategy to beam search. Instead of targeting the hypotheses with the highest model probability, MBR decoding extracts the hypotheses with the highest estimated quality. Our experiments show that the combination of a neural translation model with a neural reference-based metric, Bleurt, results in significant improvement in human evaluations. This improvement is obtained with translations different from classical beam-search output: These translations have much lower model likelihood and are less favored by surface metrics like Bleu.",
}
@inproceedings{martins-etal-2022-efficient,
    title = "Efficient Machine Translation Domain Adaptation",
    author = "Martins, Pedro  and
      Marinho, Zita  and
      Martins, Andre",
    editor = "Das, Rajarshi  and
      Lewis, Patrick  and
      Min, Sewon  and
      Thai, June  and
      Zaheer, Manzil",
    booktitle = "Proceedings of the 1st Workshop on Semiparametric Methods in NLP: Decoupling Logic from Knowledge",
    month = may,
    year = "2022",
    address = "Dublin, Ireland and Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.spanlp-1.3",
    doi = "10.18653/v1/2022.spanlp-1.3",
    pages = "23--29",
    abstract = "Machine translation models struggle when translating out-of-domain text, which makes domain adaptation a topic of critical importance. However, most domain adaptation methods focus on fine-tuning or training the entire or part of the model on every new domain, which can be costly. On the other hand, semi-parametric models have been shown to successfully perform domain adaptation by retrieving examples from an in-domain datastore (Khandelwal et al., 2021). A drawback of these retrieval-augmented models, however, is that they tend to be substantially slower. In this paper, we explore several approaches to speed up nearest neighbors machine translation. We adapt the methods recently proposed by He et al. (2021) for language modeling, and introduce a simple but effective caching strategy that avoids performing retrieval when similar contexts have been seen before. Translation quality and runtimes for several domains show the effectiveness of the proposed solutions.",
}
@inproceedings{huerta-enochian-etal-2022-kosign,
    title = "{K}o{S}ign Sign Language Translation Project: Introducing The {NIASL}2021 Dataset",
    author = "Huerta-Enochian, Mathew  and
      Lee, Du Hui  and
      Myung, Hye Jin  and
      Byun, Kang Suk  and
      Lee, Jun Woo",
    editor = "Efthimiou, Eleni  and
      Fotinea, Stavroula-Evita  and
      Hanke, Thomas  and
      McDonald, John C.  and
      Shterionov, Dimitar  and
      Wolfe, Rosalee",
    booktitle = "Proceedings of the 7th International Workshop on Sign Language Translation and Avatar Technology: The Junction of the Visual and the Textual: Challenges and Perspectives",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.sltat-1.9",
    pages = "59--66",
    abstract = "We introduce a new sign language production (SLP) and sign language translation (SLT) dataset, NIASL2021, consisting of 201,026 Korean-KSL data pairs. KSL translations of Korean source texts are represented in three formats: video recordings, keypoint position data, and time-aligned gloss annotations for each hand (using a 7,989 sign vocabulary) and for eight different non-manual signals (NMS). We evaluated our sign language elicitation methodology and found that text-based prompting had a negative effect on translation quality in terms of naturalness and comprehension. We recommend distilling text into a visual medium before translating into sign language or adding a prompt-blind review step to text-based translation methodologies.",
}
@inproceedings{hatami-etal-2022-analysing,
    title = "Analysing the Correlation between Lexical Ambiguity and Translation Quality in a Multimodal Setting using {W}ord{N}et",
    author = "Hatami, Ali  and
      Buitelaar, Paul  and
      Arcan, Mihael",
    editor = "Ippolito, Daphne  and
      Li, Liunian Harold  and
      Pacheco, Maria Leonor  and
      Chen, Danqi  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop",
    month = jul,
    year = "2022",
    address = "Hybrid: Seattle, Washington + Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-srw.12",
    doi = "10.18653/v1/2022.naacl-srw.12",
    pages = "89--95",
    abstract = "Multimodal Neural Machine Translation is focusing on using visual information to translate sentences in the source language into the target language. The main idea is to utilise information from visual modalities to promote the output quality of the text-based translation model. Although the recent multimodal strategies extract the most relevant visual information in images, the effectiveness of using visual information on translation quality changes based on the text dataset. Due to this, this work studies the impact of leveraging visual information in multimodal translation models of ambiguous sentences. Our experiments analyse the Multi30k evaluation dataset and calculate ambiguity scores of sentences based on the WordNet hierarchical structure. To calculate the ambiguity of a sentence, we extract the ambiguity scores for all nouns based on the number of senses in WordNet. The main goal is to find in which sentences, visual content can improve the text-based translation model. We report the correlation between the ambiguity scores and translation quality extracted for all sentences in the English-German dataset.",
}
@inproceedings{zhu-etal-2022-non,
    title = "Non-Autoregressive Neural Machine Translation with Consistency Regularization Optimized Variational Framework",
    author = "Zhu, Minghao  and
      Wang, Junli  and
      Yan, Chungang",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.45",
    doi = "10.18653/v1/2022.naacl-main.45",
    pages = "607--617",
    abstract = "Variational Autoencoder (VAE) is an effective framework to model the interdependency for non-autoregressive neural machine translation (NAT). One of the prominent VAE-based NAT frameworks, LaNMT, achieves great improvements to vanilla models, but still suffers from two main issues which lower down the translation quality: (1) mismatch between training and inference circumstances and (2) inadequacy of latent representations. In this work, we target on addressing these issues by proposing posterior consistency regularization. Specifically, we first perform stochastic data augmentation on the input samples to better adapt the model for inference circumstance, and then conduct consistency training on posterior latent variables to construct a more robust latent representations without any expansion on latent size. Experiments on En{\textless}-{\textgreater}De and En{\textless}-{\textgreater}Ro benchmarks confirm the effectiveness of our methods with about 1.5/0.7 and 0.8/0.3 BLEU points improvement to the baseline model with about $12.6\times$ faster than autoregressive Transformer.",
}
@inproceedings{oncevay-etal-2022-quantifying,
    title = "Quantifying Synthesis and Fusion and their Impact on Machine Translation",
    author = "Oncevay, Arturo  and
      Ataman, Duygu  and
      Van Berkel, Niels  and
      Haddow, Barry  and
      Birch, Alexandra  and
      Bjerva, Johannes",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.94",
    doi = "10.18653/v1/2022.naacl-main.94",
    pages = "1308--1321",
    abstract = "Theoretical work in morphological typology offers the possibility of measuring morphological diversity on a continuous scale. However, literature in Natural Language Processing (NLP) typically labels a whole language with a strict type of morphology, e.g. fusional or agglutinative. In this work, we propose to reduce the rigidity of such claims, by quantifying morphological typology at the word and segment level. We consider Payne (2017){'}s approach to classify morphology using two indices: synthesis (e.g. analytic to polysynthetic) and fusion (agglutinative to fusional). For computing synthesis, we test unsupervised and supervised morphological segmentation methods for English, German and Turkish, whereas for fusion, we propose a semi-automatic method using Spanish as a case study. Then, we analyse the relationship between machine translation quality and the degree of synthesis and fusion at word (nouns and verbs for English-Turkish, and verbs in English-Spanish) and segment level (previous language pairs plus English-German in both directions). We complement the word-level analysis with human evaluation, and overall, we observe a consistent impact of both indexes on machine translation quality.",
}
@inproceedings{fernandes-etal-2022-quality,
    title = "Quality-Aware Decoding for Neural Machine Translation",
    author = "Fernandes, Patrick  and
      Farinhas, Ant{\'o}nio  and
      Rei, Ricardo  and
      C. de Souza, Jos{\'e} G.  and
      Ogayo, Perez  and
      Neubig, Graham  and
      Martins, Andre",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.100",
    doi = "10.18653/v1/2022.naacl-main.100",
    pages = "1396--1412",
    abstract = "Despite the progress in machine translation quality estimation and evaluation in the last years, decoding in neural machine translation (NMT) is mostly oblivious to this and centers around finding the most probable translation according to the model (MAP decoding), approximated with beam search. In this paper, we bring together these two lines of research and propose \textit{quality-aware decoding} for NMT, by leveraging recent breakthroughs in reference-free and reference-based MT evaluation through various inference methods like $N$-best reranking and minimum Bayes risk decoding. We perform an extensive comparison of various possible candidate generation and ranking methods across four datasets and two model classes and find that quality-aware decoding consistently outperforms MAP-based decoding according both to state-of-the-art automatic metrics (COMET and BLEURT) and to human assessments.",
}
@inproceedings{jiang-etal-2022-blonde,
    title = "{BlonDe}: An Automatic Evaluation Metric for Document-level Machine Translation",
    author = "Jiang, Yuchen  and
      Liu, Tianyu  and
      Ma, Shuming  and
      Zhang, Dongdong  and
      Yang, Jian  and
      Huang, Haoyang  and
      Sennrich, Rico  and
      Cotterell, Ryan  and
      Sachan, Mrinmaya  and
      Zhou, Ming",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.111",
    doi = "10.18653/v1/2022.naacl-main.111",
    pages = "1550--1565",
    abstract = "Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level, and is more sensitive to document-level nuances. In a large-scale human study, BlonDe also achieves significantly higher Pearson{'}s r correlation with human judgments compared to previous metrics.",
}
@inproceedings{domhan-etal-2022-devil,
    title = "The Devil is in the Details: On the Pitfalls of Vocabulary Selection in Neural Machine Translation",
    author = "Domhan, Tobias  and
      Hasler, Eva  and
      Tran, Ke  and
      Trenous, Sony  and
      Byrne, Bill  and
      Hieber, Felix",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.136",
    doi = "10.18653/v1/2022.naacl-main.136",
    pages = "1861--1874",
    abstract = "Vocabulary selection, or lexical shortlisting, is a well-known technique to improve latency of Neural Machine Translation models by constraining the set of allowed output words during inference. The chosen set is typically determined by separately trained alignment model parameters, independent of the source-sentence context at inference time. While vocabulary selection appears competitive with respect to automatic quality metrics in prior work, we show that it can fail to select the right set of output words, particularly for semantically non-compositional linguistic phenomena such as idiomatic expressions, leading to reduced translation quality as perceived by humans. Trading off latency for quality by increasing the size of the allowed set is often not an option in real-world scenarios. We propose a model of vocabulary selection, integrated into the neural translation model, that predicts the set of allowed output words from contextualized encoder representations. This restores translation quality of an unconstrained system, as measured by human evaluations on WMT newstest2020 and idiomatic expressions, at an inference latency competitive with alignment-based selection using aggressive thresholds, thereby removing the dependency on separately trained alignment models.",
}
@inproceedings{marchisio-etal-2022-systematic,
    title = "On Systematic Style Differences between Unsupervised and Supervised {MT} and an Application for High-Resource Machine Translation",
    author = "Marchisio, Kelly  and
      Freitag, Markus  and
      Grangier, David",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.161",
    doi = "10.18653/v1/2022.naacl-main.161",
    pages = "2214--2225",
    abstract = "Modern unsupervised machine translation (MT) systems reach reasonable translation quality under clean and controlled data conditions. As the performance gap between supervised and unsupervised MT narrows, it is interesting to ask whether the different training methods result in systematically different output beyond what is visible via quality metrics like adequacy or BLEU. We compare translations from supervised and unsupervised MT systems of similar quality, finding that unsupervised output is more fluent and more structurally different in comparison to human translation than is supervised MT. We then demonstrate a way to combine the benefits of both methods into a single system which results in improved adequacy and fluency as rated by human evaluators. Our results open the door to interesting discussions about how supervised and unsupervised MT might be different yet mutually-beneficial.",
}
@inproceedings{zeng-etal-2022-neighbors,
    title = "Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints",
    author = "Zeng, Chun  and
      Chen, Jiangjie  and
      Zhuang, Tianyi  and
      Xu, Rui  and
      Yang, Hao  and
      Ying, Qin  and
      Tao, Shimin  and
      Xiao, Yanghua",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.424",
    doi = "10.18653/v1/2022.naacl-main.424",
    pages = "5777--5790",
    abstract = "Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains. However, current autoregressive approaches suffer from high latency. In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage. We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well. To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints. Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints.",
}
@inproceedings{gladkoff-han-2022-hope,
    title = "{HOPE}: A Task-Oriented and Human-Centric Evaluation Framework Using Professional Post-Editing Towards More Effective {MT} Evaluation",
    author = "Gladkoff, Serge  and
      Han, Lifeng",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.2",
    pages = "13--21",
    abstract = "Traditional automatic evaluation metrics for machine translation have been widely criticized by linguists due to their low accuracy, lack of transparency, focus on language mechanics rather than semantics, and low agreement with human quality evaluation. Human evaluations in the form of MQM-like scorecards have always been carried out in real industry setting by both clients and translation service providers (TSPs). However, traditional human translation quality evaluations are costly to perform and go into great linguistic detail, raise issues as to inter-rater reliability (IRR) and are not designed to measure quality of worse than premium quality translations. In this work, we introduce \textbf{HOPE}, a task-oriented and \textit{ \textbf{h} }uman-centric evaluation framework for machine translation output based \textit{ \textbf{o} }n professional \textit{ \textbf{p} }ost-\textit{ \textbf{e} }diting annotations. It contains only a limited number of commonly occurring error types, and uses a scoring model with geometric progression of error penalty points (EPPs) reflecting error severity level to each translation unit. The initial experimental work carried out on English-Russian language pair MT outputs on marketing content type of text from highly technical domain reveals that our evaluation framework is quite effective in reflecting the MT output quality regarding both overall system-level performance and segment-level transparency, and it increases the IRR for error type interpretation. The approach has several key advantages, such as ability to measure and compare less than perfect MT output from different systems, ability to indicate human perception of quality, immediate estimation of the labor effort required to bring MT output to premium quality, low-cost and faster application, as well as higher IRR. Our experimental data is available at \url{https://github.com/lHan87/HOPE}.",
}
@inproceedings{zaragoza-bernabeu-etal-2022-bicleaner,
    title = "Bicleaner {AI}: Bicleaner Goes Neural",
    author = "Zaragoza-Bernabeu, Jaume  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Ba{\~n}{\'o}n, Marta  and
      Ortiz Rojas, Sergio",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.87",
    pages = "824--831",
    abstract = "This paper describes the experiments carried out during the development of the latest version of Bicleaner, named Bicleaner AI, a tool that aims at detecting noisy sentences in parallel corpora. The tool, which now implements a new neural classifier, uses state-of-the-art techniques based on pre-trained transformer-based language models fine-tuned on a binary classification task. After that, parallel corpus filtering is performed, discarding the sentences that have lower probability of being mutual translations. Our experiments, based on the training of neural machine translation (NMT) with corpora filtered using Bicleaner AI for two different scenarios, show significant improvements in translation quality compared to the previous version of the tool which implemented a classifier based on Extremely Randomized Trees.",
}
@inproceedings{mcnamee-duh-2022-multilingual,
    title = "The Multilingual Microblog Translation Corpus: Improving and Evaluating Translation of User-Generated Text",
    author = "McNamee, Paul  and
      Duh, Kevin",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.96",
    pages = "910--918",
    abstract = "Translation of the noisy, informal language found in social media has been an understudied problem, with a principal factor being the limited availability of translation corpora in many languages. To address this need we have developed a new corpus containing over 200,000 translations of microblog posts that supports translation of thirteen languages into English. The languages are: Arabic, Chinese, Farsi, French, German, Hindi, Korean, Pashto, Portuguese, Russian, Spanish, Tagalog, and Urdu. We are releasing these data as the Multilingual Microblog Translation Corpus to support futher research in translation of informal language. We establish baselines using this new resource, and we further demonstrate the utility of the corpus by conducting experiments with fine-tuning to improve translation quality from a high performing neural machine translation (NMT) system. Fine-tuning provided substantial gains, ranging from +3.4 to +11.1 BLEU. On average, a relative gain of 21{\%} was observed, demonstrating the utility of the corpus.",
}
@inproceedings{gladkoff-etal-2022-measuring,
    title = "Measuring Uncertainty in Translation Quality Evaluation ({TQE})",
    author = "Gladkoff, Serge  and
      Sorokina, Irina  and
      Han, Lifeng  and
      Alekseeva, Alexandra",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.156",
    pages = "1454--1461",
    abstract = "From both human translators (HT) and machine translation (MT) researchers{'} point of view, translation quality evaluation (TQE) is an essential task. Translation service providers (TSPs) have to deliver large volumes of translations which meet customer specifications with harsh constraints of required quality level in tight time-frames and costs. MT researchers strive to make their models better, which also requires reliable quality evaluation. While automatic machine translation evaluation (MTE) metrics and quality estimation (QE) tools are widely available and easy to access, existing automated tools are not good enough, and human assessment from professional translators (HAP) are often chosen as the golden standard (CITATION). Human evaluations, however, are often accused of having low reliability and agreement. Is this caused by subjectivity or statistics is at play? How to avoid the entire text to be checked and be more efficient with TQE from cost and efficiency perspectives, and what is the optimal sample size of the translated text, so as to reliably estimate the translation quality of the entire material? This work carries out such a motivated research to correctly estimate the confidence intervals (CITATION) depending on the sample size of translated text, e.g. the amount of words or sentences, that needs to be processed on TQE workflow step for confident and reliable evaluation of overall translation quality. The methodology we applied for this work is from Bernoulli Statistical Distribution Modelling (BSDM) and Monte Carlo Sampling Analysis (MCSA).",
}
@inproceedings{fomicheva-etal-2022-mlqe,
    title = "{MLQE}-{PE}: A Multilingual Quality Estimation and Post-Editing Dataset",
    author = "Fomicheva, Marina  and
      Sun, Shuo  and
      Fonseca, Erick  and
      Zerva, Chrysoula  and
      Blain, Fr{\'e}d{\'e}ric  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Lopatina, Nina  and
      Specia, Lucia  and
      Martins, Andr{\'e} F. T.",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.530",
    pages = "4963--4974",
    abstract = "We present MLQE-PE, a new dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains annotations for eleven language pairs, including both high- and low-resource languages. Specifically, it is annotated for translation quality with human labels for up to 10,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level binary good/bad labels. Apart from the quality-related scores, each source-translation sentence pair is accompanied by the corresponding post-edited sentence, as well as titles of the articles where the sentences were extracted from, and information on the neural MT models used to translate the text. We provide a thorough description of the data collection and annotation process as well as an analysis of the annotation distribution for each language pair. We also report the performance of baseline systems trained on the MLQE-PE dataset. The dataset is freely available and has already been used for several WMT shared tasks.",
}
@inproceedings{dione-etal-2022-low,
    title = "Low-resource Neural Machine Translation: Benchmarking State-of-the-art Transformer for {W}olof{\textless}-{\textgreater}{F}rench",
    author = "Dione, Cheikh M. Bamba  and
      Lo, Alla  and
      Nguer, Elhadji Mamadou  and
      Ba, Sileye",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.717",
    pages = "6654--6661",
    abstract = "In this paper, we propose two neural machine translation (NMT) systems (French-to-Wolof and Wolof-to-French) based on sequence-to-sequence with attention and Transformer architectures. We trained our models on the parallel French-Wolof corpus (Nguer et al., 2020) of about 83k sentence pairs. Because of the low-resource setting, we experimented with advanced methods for handling data sparsity, including subword segmentation, backtranslation and the copied corpus method. We evaluate the models using BLEU score and find that the transformer outperforms the classic sequence-to-sequence model in all settings, in addition to being less sensitive to noise. In general, the best scores are achieved when training the models on subword-level based units. For such models, using backtranslation proves to be slightly beneficial in low-resource Wolof to high-resource French language translation for the transformer-based models. A slight improvement can also be observed when injecting copied monolingual text in the target language. Moreover, combining the copied method data with backtranslation leads to a slight improvement of the translation quality.",
}
@inproceedings{dai-etal-2022-bertology,
    title = "{BERT}ology for Machine Translation: What {BERT} Knows about Linguistic Difficulties for Translation",
    author = "Dai, Yuqian  and
      de Kamps, Marc  and
      Sharoff, Serge",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.719",
    pages = "6674--6690",
    abstract = "Pre-trained transformer-based models, such as BERT, have shown excellent performance in most natural language processing benchmark tests, but we still lack a good understanding of the linguistic knowledge of BERT in Neural Machine Translation (NMT). Our work uses syntactic probes and Quality Estimation (QE) models to analyze the performance of BERT{'}s syntactic dependencies and their impact on machine translation quality, exploring what kind of syntactic dependencies are difficult for NMT engines based on BERT. While our probing experiments confirm that pre-trained BERT {``}knows{''} about syntactic dependencies, its ability to recognize them often decreases after fine-tuning for NMT tasks. We also detect a relationship between syntactic dependencies in three languages and the quality of their translations, which shows which specific syntactic dependencies are likely to be a significant cause of low-quality translations.",
}
@inproceedings{knowles-littell-2022-translation,
    title = "Translation Memories as Baselines for Low-Resource Machine Translation",
    author = "Knowles, Rebecca  and
      Littell, Patrick",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.728",
    pages = "6759--6767",
    abstract = "Low-resource machine translation research often requires building baselines to benchmark estimates of progress in translation quality. Neural and statistical phrase-based systems are often used with out-of-the-box settings to build these initial baselines before analyzing more sophisticated approaches, implicitly comparing the first machine translation system to the absence of any translation assistance. We argue that this approach overlooks a basic resource: if you have parallel text, you have a translation memory. In this work, we show that using available text as a translation memory baseline against which to compare machine translation systems is simple, effective, and can shed light on additional translation challenges.",
}
@inproceedings{poncelas-effendi-2022-benefiting,
    title = "Benefiting from Language Similarity in the Multilingual {MT} Training: Case Study of {I}ndonesian and {M}alaysian",
    author = "Poncelas, Alberto  and
      Effendi, Johanes",
    editor = "Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Vylomova, Ekaterina  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Pirinen, Tommi A  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.loresmt-1.11",
    pages = "84--92",
    abstract = "The development of machine translation (MT) has been successful in breaking the language barrier of the world{'}s top 10-20 languages. However, for the rest of it, delivering an acceptable translation quality is still a challenge due to the limited resource. To tackle this problem, most studies focus on augmenting data while overlooking the fact that we can borrow high-quality natural data from the closely-related language. In this work, we propose an MT model training strategy by increasing the language directions as a means of augmentation in a multilingual setting. Our experiment result using Indonesian and Malaysian on the state-of-the-art MT model showcases the effectiveness and robustness of our method.",
}
@inproceedings{bastan-khadivi-2022-preordered,
    title = "A Preordered {RNN} Layer Boosts Neural Machine Translation in Low Resource Settings",
    author = "Bastan, Mohaddeseh  and
      Khadivi, Shahram",
    editor = "Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Vylomova, Ekaterina  and
      Abbott, Jade  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Pirinen, Tommi A  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the Fifth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2022)",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.loresmt-1.12",
    pages = "93--98",
    abstract = "Neural Machine Translation (NMT) models are strong enough to convey semantic and syntactic information from the source language to the target language. However, these models are suffering from the need for a large amount of data to learn the parameters. As a result, for languages with scarce data, these models are at risk of underperforming. We propose to augment attention based neural network with reordering information to alleviate the lack of data. This augmentation improves the translation quality for both English to Persian and Persian to English by up to 6{\%} BLEU absolute over the baseline models.",
}
@inproceedings{petrick-etal-2022-locality,
    title = "Locality-Sensitive Hashing for Long Context Neural Machine Translation",
    author = "Petrick, Frithjof  and
      Rosendahl, Jan  and
      Herold, Christian  and
      Ney, Hermann",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Costa-juss{\`a}, Marta",
    booktitle = "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.iwslt-1.4",
    doi = "10.18653/v1/2022.iwslt-1.4",
    pages = "32--42",
    abstract = "After its introduction the Transformer architecture quickly became the gold standard for the task of neural machine translation. A major advantage of the Transformer compared to previous architectures is the faster training speed achieved by complete parallelization across timesteps due to the use of attention over recurrent layers. However, this also leads to one of the biggest problems of the Transformer, namely the quadratic time and memory complexity with respect to the input length. In this work we adapt the locality-sensitive hashing approach of Kitaev et al. (2020) to self-attention in the Transformer, we extended it to cross-attention and apply this memory efficient framework to sentence- and document-level machine translation. Our experiments show that the LSH attention scheme for sentence-level comes at the cost of slightly reduced translation quality. For document-level NMT we are able to include much bigger context sizes than what is possible with the baseline Transformer. However, more context does neither improve translation quality nor improve scores on targeted test suites.",
}
@inproceedings{chang-etal-2022-anticipation,
    title = "Anticipation-Free Training for Simultaneous Machine Translation",
    author = "Chang, Chih-Chiang  and
      Chuang, Shun-Po  and
      Lee, Hung-yi",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Costa-juss{\`a}, Marta",
    booktitle = "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.iwslt-1.5",
    doi = "10.18653/v1/2022.iwslt-1.5",
    pages = "43--61",
    abstract = "Simultaneous machine translation (SimulMT) speeds up the translation process by starting to translate before the source sentence is completely available. It is difficult due to limited context and word order difference between languages. Existing methods increase latency or introduce adaptive read-write policies for SimulMT models to handle local reordering and improve translation quality. However, the long-distance reordering would make the SimulMT models learn translation mistakenly. Specifically, the model may be forced to predict target tokens when the corresponding source tokens have not been read. This leads to aggressive anticipation during inference, resulting in the hallucination phenomenon. To mitigate this problem, we propose a new framework that decompose the translation process into the monotonic translation step and the reordering step, and we model the latter by the auxiliary sorting network (ASN). The ASN rearranges the hidden states to match the order in the target language, so that the SimulMT model could learn to translate more reasonably. The entire model is optimized end-to-end and does not rely on external aligners or data. During inference, ASN is removed to achieve streaming. Experiments show the proposed framework could outperform previous methods with less latency.",
}
@inproceedings{gaido-etal-2022-efficient,
    title = "Efficient yet Competitive Speech Translation: {FBK}@{IWSLT}2022",
    author = "Gaido, Marco  and
      Papi, Sara  and
      Fucci, Dennis  and
      Fiameni, Giuseppe  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Costa-juss{\`a}, Marta",
    booktitle = "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.iwslt-1.13",
    doi = "10.18653/v1/2022.iwslt-1.13",
    pages = "177--189",
    abstract = "The primary goal of this FBK{'}s systems submission to the IWSLT 2022 offline and simultaneous speech translation tasks is to reduce model training costs without sacrificing translation quality. As such, we first question the need of ASR pre-training, showing that it is not essential to achieve competitive results. Second, we focus on data filtering, showing that a simple method that looks at the ratio between source and target characters yields a quality improvement of 1 BLEU. Third, we compare different methods to reduce the detrimental effect of the audio segmentation mismatch between training data manually segmented at sentence level and inference data that is automatically segmented. Towards the same goal of training cost reduction, we participate in the simultaneous task with the same model trained for offline ST. The effectiveness of our lightweight training strategy is shown by the high score obtained on the MuST-C en-de corpus (26.7 BLEU) and is confirmed in high-resource data conditions by a 1.6 BLEU improvement on the IWSLT2020 test set over last year{'}s winning system.",
}
@inproceedings{tsiamas-etal-2022-pretrained,
    title = "Pretrained Speech Encoders and Efficient Fine-tuning Methods for Speech Translation: {UPC} at {IWSLT} 2022",
    author = "Tsiamas, Ioannis  and
      G{\'a}llego, Gerard I.  and
      Escolano, Carlos  and
      Fonollosa, Jos{\'e}  and
      Costa-juss{\`a}, Marta R.",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Costa-juss{\`a}, Marta",
    booktitle = "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.iwslt-1.23",
    doi = "10.18653/v1/2022.iwslt-1.23",
    pages = "265--276",
    abstract = "This paper describes the submissions of the UPC Machine Translation group to the IWSLT 2022 Offline Speech Translation and Speech-to-Speech Translation tracks. The offline task involves translating English speech to German, Japanese and Chinese text. Our Speech Translation systems are trained end-to-end and are based on large pretrained speech and text models. We use an efficient fine-tuning technique that trains only specific layers of our system, and explore the use of adapter modules for the non-trainable layers. We further investigate the suitability of different speech encoders (wav2vec 2.0, HuBERT) for our models and the impact of knowledge distillation from the Machine Translation model that we use for the decoder (mBART). For segmenting the IWSLT test sets we fine-tune a pretrained audio segmentation model and achieve improvements of 5 BLEU compared to the given segmentation. Our best single model uses HuBERT and parallel adapters and achieves 29.42 BLEU at English-German MuST-C tst-COMMON and 26.77 at IWSLT 2020 test. By ensembling many models, we further increase translation quality to 30.83 BLEU and 27.78 accordingly. Furthermore, our submission for English-Japanese achieves 15.85 and English-Chinese obtains 25.63 BLEU on the MuST-C tst-COMMON sets. Finally, we extend our system to perform English-German Speech-to-Speech Translation with a pretrained Text-to-Speech model.",
}
@inproceedings{rippeth-etal-2022-controlling,
    title = "Controlling Translation Formality Using Pre-trained Multilingual Language Models",
    author = "Rippeth, Elijah  and
      Agrawal, Sweta  and
      Carpuat, Marine",
    editor = "Salesky, Elizabeth  and
      Federico, Marcello  and
      Costa-juss{\`a}, Marta",
    booktitle = "Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland (in-person and online)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.iwslt-1.30",
    doi = "10.18653/v1/2022.iwslt-1.30",
    pages = "327--340",
    abstract = "This paper describes the University of Maryland{'}s submission to the Special Task on Formality Control for Spoken Language Translation at IWSLT, which evaluates translation from English into 6 languages with diverse grammatical formality markers. We investigate to what extent this problem can be addressed with a single multilingual model, simultaneously controlling its output for target language and formality. Results show that this strategy can approach the translation quality and formality control achieved by dedicated translation models. However, the nature of the underlying pre-trained language model and of the finetuning samples greatly impact results.",
}
@inproceedings{mao-etal-2022-contrastive,
    title = "When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?",
    author = "Mao, Zhuoyuan  and
      Chu, Chenhui  and
      Dabre, Raj  and
      Song, Haiyue  and
      Wan, Zhen  and
      Kurohashi, Sadao",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.134",
    doi = "10.18653/v1/2022.findings-naacl.134",
    pages = "1766--1775",
    abstract = "Word alignment has proven to benefit many-to-many neural machine translation (NMT). However, high-quality ground-truth bilingual dictionaries were used for pre-editing in previous methods, which are unavailable for most language pairs. Meanwhile, the contrastive objective can implicitly utilize automatically learned word alignment, which has not been explored in many-to-many NMT. This work proposes a word-level contrastive objective to leverage word alignments for many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains for several language pairs. Analyses reveal that in many-to-many NMT, the encoder{'}s sentence retrieval performance highly correlates with the translation quality, which explains when the proposed method impacts translation. This motivates future exploration for many-to-many NMT to improve the encoder{'}s sentence retrieval performance.",
}
@inproceedings{sharma-etal-2022-sensitive,
    title = "How sensitive are translation systems to extra contexts? Mitigating gender bias in Neural Machine Translation models through relevant contexts.",
    author = "Sharma, Shanya  and
      Dey, Manan  and
      Sinha, Koustuv",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.143",
    doi = "10.18653/v1/2022.findings-emnlp.143",
    pages = "1968--1984",
    abstract = "Neural Machine Translation systems built on top of Transformer-based architectures are routinely improving the state-of-the-art in translation quality according to word-overlap metrics. However, a growing number of studies also highlight the inherent gender bias that these models incorporate during training, which reflects poorly in their translations. In this work, we investigate whether these models can be instructed to fix their bias during inference using targeted, guided instructions as contexts. By translating relevant contextual sentences during inference along with the input, we observe large improvements in reducing the gender bias in translations, across three popular test suites (WinoMT, BUG, SimpleGen). We further propose a novel metric to assess several large pre-trained models (OPUS-MT, M2M-100) on their sensitivity towards using contexts during translation to correct their biases. Our approach requires no fine-tuning, and thus can be used easily in production systems to de-bias translations from stereotypical gender-occupation bias. We hope our method, along with our metric, can be used to build better, bias-free translation systems.",
}
@inproceedings{raunak-etal-2022-salted,
    title = "{SALTED}: A Framework for {SA}lient Long-tail Translation Error Detection",
    author = "Raunak, Vikas  and
      Post, Matt  and
      Menezes, Arul",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.379",
    doi = "10.18653/v1/2022.findings-emnlp.379",
    pages = "5163--5179",
    abstract = "Traditional machine translation (MT) metrics provide an average measure of translation quality that is insensitive to the long tail of behavioral problems. Examples include translation of numbers, physical units, dropped content and hallucinations. These errors, which occur rarely and unpredictably in Neural Machine Translation (NMT), greatly undermine the reliability of state-of-the-art MT systems. Consequently, it is important to have visibility into these problems during model development. Towards this end, we introduce SALTED, a specifications-based framework for behavioral testing of NMT models. At the core of our approach is the use of high-precision detectors that flag errors (or alternatively, verify output correctness) between a source sentence and a system output. These detectors provide fine-grained measurements of long-tail errors, providing a trustworthy view of problems that were previously invisible. We demonstrate that such detectors could be used not just to identify salient long-tail errors in MT systems, but also for higher-recall filtering of the training data, fixing targeted errors with model fine-tuning in NMT and generating novel data for metamorphic testing to elicit further bugs in models.",
}
@inproceedings{lu-etal-2022-exploring,
    title = "Exploring Methods for Building Dialects-{M}andarin Code-Mixing Corpora: A Case Study in {T}aiwanese Hokkien",
    author = "Lu, Sin-En  and
      Lu, Bo-Han  and
      Lu, Chao-Yi  and
      Tsai, Richard Tzong-Han",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.469",
    doi = "10.18653/v1/2022.findings-emnlp.469",
    pages = "6287--6305",
    abstract = "In natural language processing (NLP), code-mixing (CM) is a challenging task, especially when the mixed languages include dialects. In Southeast Asian countries such as Singapore, Indonesia, and Malaysia, Hokkien-Mandarin is the most widespread code-mixed language pair among Chinese immigrants, and it is also common in Taiwan. However, dialects such as Hokkien often have a scarcity of resources and the lack of an official writing system, limiting the development of dialect CM research. In this paper, we propose a method to construct a Hokkien-Mandarin CM dataset to mitigate the limitation, overcome the morphological issue under the Sino-Tibetan language family, and offer an efficient Hokkien word segmentation method through a linguistics-based toolkit. Furthermore, we use our proposed dataset and employ transfer learning to train the XLM (cross-lingual language model) for translation tasks. To fit the code-mixing scenario, we adapt XLM slightly. We found that by using linguistic knowledge, rules, and language tags, the model produces good results on CM data translation while maintaining monolingual translation quality.",
}
@inproceedings{wu-etal-2022-modeling,
    title = "Modeling Context With Linear Attention for Scalable Document-Level Translation",
    author = "Wu, Zhaofeng  and
      Peng, Hao  and
      Pappas, Nikolaos  and
      Smith, Noah A.",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.515",
    doi = "10.18653/v1/2022.findings-emnlp.515",
    pages = "6931--6939",
    abstract = "Document-level machine translation leverages inter-sentence dependencies to produce more coherent and consistent translations. However, these models, predominantly based on transformers, are difficult to scale to long documents as their attention layers have quadratic complexity in the sequence length. Recent efforts on efficient attention improve scalability, but their effect on document translation remains unexplored. In this work, we investigate the efficacy of a recent linear attention model by Peng et al. (2021) on document translation and augment it with a sentential gate to promote a recency inductive bias. We evaluate the model on IWSLT 2015 and OpenSubtitles 2018 against the transformer, demonstrating substantially increased decoding speed on long sequences with similar or better BLEU scores. We show that sentential gating further improves translation quality on IWSLT.",
}
@inproceedings{vavre-etal-2022-adapting,
    title = "Adapting Multilingual Models for Code-Mixed Translation",
    author = "Vavre, Aditya  and
      Gupta, Abhirut  and
      Sarawagi, Sunita",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.528",
    doi = "10.18653/v1/2022.findings-emnlp.528",
    pages = "7133--7141",
    abstract = "The scarcity of gold standard code-mixed to pure language parallel data makes it difficult to train translation models reliably. Prior work has addressed the paucity of parallel data with data augmentation techniques. Such methods rely heavily on external resources making systems difficult to train and scale effectively for multiple languages. We present a simple yet highly effective two-stage back-translation based training scheme for adapting multilingual models to the task of code-mixed translation which eliminates dependence on external resources. We show a substantial improvement in translation quality (measured through BLEU), beating existing prior work by up to +3.8 BLEU on code-mixed Hi$\rightarrow$En, Mr$\rightarrow$En, and Bn$\rightarrow$En tasks. On the LinCE Machine Translation leader board, we achieve the highest score for code-mixed Es$\rightarrow$En, beating existing best baseline by +6.5 BLEU, and our own stronger baseline by +1.1 BLEU.",
}
@inproceedings{libovicky-etal-2022-dont,
    title = "Why don{'}t people use character-level machine translation?",
    author = "Libovick{\'y}, Jind{\v{r}}ich  and
      Schmid, Helmut  and
      Fraser, Alexander",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.194",
    doi = "10.18653/v1/2022.findings-acl.194",
    pages = "2470--2485",
    abstract = "We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time.",
}
@inproceedings{li-etal-2022-prompt,
    title = "Prompt-Driven Neural Machine Translation",
    author = "Li, Yafu  and
      Yin, Yongjing  and
      Li, Jing  and
      Zhang, Yue",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.203",
    doi = "10.18653/v1/2022.findings-acl.203",
    pages = "2579--2590",
    abstract = "Neural machine translation (NMT) has obtained significant performance improvement over the recent years. However, NMT models still face various challenges including fragility and lack of style flexibility. Moreover, current methods for instance-level constraints are limited in that they are either constraint-specific or model-specific. To this end, we propose prompt-driven neural machine translation to incorporate prompts for enhancing translation control and enriching flexibility. Empirical results demonstrate the effectiveness of our method in both prompt responding and translation quality. Through human evaluation, we further show the flexibility of prompt control and the efficiency in human-in-the-loop translation.",
}
@inproceedings{zhang-feng-2022-gaussian,
    title = "{G}aussian Multi-head Attention for Simultaneous Machine Translation",
    author = "Zhang, Shaolei  and
      Feng, Yang",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.238",
    doi = "10.18653/v1/2022.findings-acl.238",
    pages = "3019--3030",
    abstract = "Simultaneous machine translation (SiMT) outputs translation while receiving the streaming source inputs, and hence needs a policy to determine where to start translating. The alignment between target and source words often implies the most informative source word for each target word, and hence provides the unified control over translation quality and latency, but unfortunately the existing SiMT methods do not explicitly model the alignment to perform the control. In this paper, we propose Gaussian Multi-head Attention (GMA) to develop a new SiMT policy by modeling alignment and translation in a unified manner. For SiMT policy, GMA models the aligned source position of each target word, and accordingly waits until its aligned position to start translating. To integrate the learning of alignment into the translation model, a Gaussian distribution centered on predicted aligned position is introduced as an alignment-related prior, which cooperates with translation-related soft attention to determine the final attention. Experiments on En-Vi and De-En tasks show that our method outperforms strong baselines on the trade-off between translation and latency.",
}
@inproceedings{li-etal-2022-structural,
    title = "Structural Supervision for Word Alignment and Machine Translation",
    author = "Li, Lei  and
      Fan, Kai  and
      Li, Hongjia  and
      Yuan, Chun",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.322",
    doi = "10.18653/v1/2022.findings-acl.322",
    pages = "4084--4094",
    abstract = "Syntactic structure has long been argued to be potentially useful for enforcing accurate word alignment and improving generalization performance of machine translation. Unfortunately, existing wisdom demonstrates its significance by considering only the syntactic structure of source tokens, neglecting the rich structural information from target tokens and the structural similarity between the source and target sentences. In this work, we propose to incorporate the syntactic structure of both source and target tokens into the encoder-decoder framework, tightly correlating the internal logic of word alignment and machine translation for multi-task learning. Particularly, we won{'}t leverage any annotated syntactic graph of the target side during training, so we introduce Dynamic Graph Convolution Networks (DGCN) on observed target tokens to sequentially and simultaneously generate the target tokens and the corresponding syntactic graphs, and further guide the word alignment. On this basis, Hierarchical Graph Random Walks (HGRW) are performed on the syntactic graphs of both source and target sides, for incorporating structured constraints on machine translation outputs. Experiments on four publicly available language pairs verify that our method is highly effective in capturing syntactic structure in different languages, consistently outperforming baselines in alignment accuracy and demonstrating promising results in translation quality.",
}
@inproceedings{dabre-sukhoo-2022-kreolmorisienmt,
    title = "{K}reol{M}orisien{MT}: A Dataset for Mauritian Creole Machine Translation",
    author = "Dabre, Raj  and
      Sukhoo, Aneerav",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-aacl.3",
    pages = "22--29",
    abstract = "In this paper, we describe KreolMorisienMT, a dataset for benchmarking machine translation quality of Mauritian Creole. Mauritian Creole (Kreol Morisien) is a French-based creole and a lingua franca of the Republic of Mauritius. KreolMorisienMT consists of a parallel corpus between English and Kreol Morisien, French and Kreol Morisien and a monolingual corpus for Kreol Morisien. We first give an overview of Kreol Morisien and then describe the steps taken to create the corpora. Thereafter, we benchmark Kreol Morisien ↔ English and Kreol Morisien ↔ French models leveraging pre-trained models and multilingual transfer learning. Human evaluation reveals our systems{'} high translation quality.",
}
@inproceedings{zhang-feng-2022-information,
    title = "Information-Transport-based Policy for Simultaneous Translation",
    author = "Zhang, Shaolei  and
      Feng, Yang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.65",
    doi = "10.18653/v1/2022.emnlp-main.65",
    pages = "992--1013",
    abstract = "Simultaneous translation (ST) outputs translation while receiving the source inputs, and hence requires a policy to determine whether to translate a target token or wait for the next source token. The major challenge of ST is that each target token can only be translated based on the current received source tokens, where the received source information will directly affect the translation quality. So naturally, how much source information is received for the translation of the current target token is supposed to be the pivotal evidence for the ST policy to decide between translating and waiting. In this paper, we treat the translation as information transport from source to target and accordingly propose an Information-Transport-based Simultaneous Translation (ITST). ITST quantifies the transported information weight from each source token to the current target token, and then decides whether to translate the target token according to its accumulated received information. Experiments on both text-to-text ST and speech-to-text ST (a.k.a., streaming speech translation) tasks show that ITST outperforms strong baselines and achieves state-of-the-art performance.",
}
@inproceedings{schmidt-etal-2022-non,
    title = "Non-Autoregressive Neural Machine Translation: A Call for Clarity",
    author = {Schmidt, Robin  and
      Pires, Telmo  and
      Peitz, Stephan  and
      L{\"o}{\"o}f, Jonas},
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.179",
    doi = "10.18653/v1/2022.emnlp-main.179",
    pages = "2785--2799",
    abstract = "Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token. Consequently, their translation quality still tends to be inferior to their autoregressive counterparts due to several issues involving output token interdependence. In this work, we take a step back and revisit several techniques that have been proposed for improving non-autoregressive translation models and compare their combined translation quality and speed implications under third-party testing environments. We provide novel insights for establishing strong baselines using length prediction or CTC-based architecture variants and contribute standardized BLEU, chrF++, and TER scores using sacreBLEU on four translation tasks, which crucially have been missing as inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7 BLEU points. Our open-sourced code is integrated into fairseq for reproducibility.",
}
@inproceedings{wang-etal-2022-template,
    title = "A Template-based Method for Constrained Neural Machine Translation",
    author = "Wang, Shuo  and
      Li, Peng  and
      Tan, Zhixing  and
      Tu, Zhaopeng  and
      Sun, Maosong  and
      Liu, Yang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.240",
    doi = "10.18653/v1/2022.emnlp-main.240",
    pages = "3665--3679",
    abstract = "Machine translation systems are expected to cope with various types of constraints in many practical scenarios. While neural machine translation (NMT) has achieved strong performance in unconstrained cases, it is non-trivial to impose pre-specified constraints into the translation process of NMT models. Although many approaches have been proposed to address this issue, most existing methods can not satisfy the following three desiderata at the same time: (1) high translation quality, (2) high match accuracy, and (3) low latency. In this work, we propose a template-based method that can yield results with high translation quality and match accuracy and the inference speed of our method is comparable with unconstrained NMT models. Our basic idea is to rearrange the generation of constrained and unconstrained tokens through a template. Our method does not require any changes in the model architecture and the decoding algorithm. Experimental results show that the proposed template-based approach can outperform several representative baselines in both lexically and structurally constrained translation tasks.",
}
@inproceedings{martins-etal-2022-chunk,
    title = "Chunk-based Nearest Neighbor Machine Translation",
    author = "Martins, Pedro Henrique  and
      Marinho, Zita  and
      Martins, Andr{\'e} F. T.",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.284",
    doi = "10.18653/v1/2022.emnlp-main.284",
    pages = "4228--4245",
    abstract = "Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and {``}on-the-fly{''} domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.",
}
@inproceedings{zheng-etal-2022-candidate,
    title = "Candidate Soups: Fusing Candidate Results Improves Translation Quality for Non-Autoregressive Translation",
    author = "Zheng, Huanran  and
      Zhu, Wei  and
      Wang, Pengfei  and
      Wang, Xiaoling",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.318",
    doi = "10.18653/v1/2022.emnlp-main.318",
    pages = "4811--4823",
    abstract = "Non-autoregressive translation (NAT) model achieves a much faster inference speed than the autoregressive translation (AT) model because it can simultaneously predict all tokens during inference. However, its translation quality suffers from degradation compared to AT. And existing NAT methods only focus on improving the NAT model{'}s performance but do not fully utilize it. In this paper, we propose a simple but effective method called {``}Candidate Soups,{''} which can obtain high-quality translations while maintaining the inference speed of NAT models. Unlike previous approaches that pick the individual result and discard the remainders, Candidate Soups (CDS) can fully use the valuable information in the different candidate translations through model uncertainty. Extensive experiments on two benchmarks (WMT{'}14 EN{--}DE and WMT{'}16 EN{--}RO) demonstrate the effectiveness and generality of our proposed method, which can significantly improve the translation quality of various base models. More notably, our best variant outperforms the AT model on three translation tasks with 7.6{\mbox{$\times$}} speedup.",
}
@inproceedings{zhang-etal-2022-competency,
    title = "Competency-Aware Neural Machine Translation: Can Machine Translation Know its Own Translation Quality?",
    author = "Zhang, Pei  and
      Yang, Baosong  and
      Wei, Hao-Ran  and
      Liu, Dayiheng  and
      Fan, Kai  and
      Si, Luo  and
      Xie, Jun",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.330",
    doi = "10.18653/v1/2022.emnlp-main.330",
    pages = "4959--4970",
    abstract = "Neural machine translation (NMT) is often criticized for failures that happenwithout awareness. The lack of competency awareness makes NMT untrustworthy. This is in sharp contrast to human translators who give feedback or conduct further investigations whenever they are in doubt about predictions. To fill this gap, we propose a novel competency-aware NMT by extending conventional NMT with a self-estimator, offering abilities to translate a source sentence and estimate its competency. The self-estimator encodes the information of the decoding procedure and then examines whether it can reconstruct the original semantics of the source sentence. Experimental results on four translation tasks demonstrate that the proposed method not only carries out translation tasks intact but also delivers outstanding performance on quality estimation. Without depending on any reference or annotated data typically required by state-of-the-art metric and quality estimation methods, our model yields an even higher correlation with human quality judgments than a variety of aforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and qualitative analyses show better robustness of competency awareness in our model.",
}
@inproceedings{shi-etal-2022-open,
    title = "Open-Domain Sign Language Translation Learned from Online Video",
    author = "Shi, Bowen  and
      Brentari, Diane  and
      Shakhnarovich, Gregory  and
      Livescu, Karen",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.427",
    doi = "10.18653/v1/2022.emnlp-main.427",
    pages = "6365--6379",
    abstract = "Existing work on sign language translation {--} that is, translation from sign language videos into sentences in a written language {--} has focused mainly on (1) data collected in a controlled environment or (2) data in a specific domain, which limits the applicability to real-world settings. In this paper, we introduce OpenASL, a large-scale American Sign Language (ASL) - English dataset collected from online video sites (e.g., YouTube).OpenASL contains 288 hours of ASL videos in multiple domains from over 200 signers and is the largest publicly available ASL translation dataset to date. To tackle the challenges of sign language translation in realistic settings and without glosses, we propose a set of techniques including sign search as a pretext task for pre-training and fusion of mouthing and handshape features. The proposed techniques produce consistent and large improvements in translation quality, over baseline models basedon prior work.",
}
@inproceedings{ji-etal-2022-increasing,
    title = "Increasing Visual Awareness in Multimodal Neural Machine Translation from an Information Theoretic Perspective",
    author = "Ji, Baijun  and
      Zhang, Tong  and
      Zou, Yicheng  and
      Hu, Bojie  and
      Shen, Si",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.453",
    doi = "10.18653/v1/2022.emnlp-main.453",
    pages = "6755--6764",
    abstract = "Multimodal machine translation (MMT) aims to improve translation quality by equipping the source sentence with its corresponding image. Despite the promising performance, MMT models still suffer the problem of input degradation: models focus more on textual information while visual information is generally overlooked. In this paper, we endeavor to improve MMT performance by increasing visual awareness from an information theoretic perspective. In detail, we decompose the informative visual signals into two parts: source-specific information and target-specific information. We use mutual information to quantify them and propose two methods for objective optimization to better leverage visual signals. Experiments on two datasets demonstrate that our approach can effectively enhance the visual awareness of MMT model and achieve superior results against strong baselines.",
}
@inproceedings{huang-etal-2022-entropy,
    title = "Entropy-Based Vocabulary Substitution for Incremental Learning in Multilingual Neural Machine Translation",
    author = "Huang, Kaiyu  and
      Li, Peng  and
      Ma, Jin  and
      Liu, Yang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.720",
    doi = "10.18653/v1/2022.emnlp-main.720",
    pages = "10537--10550",
    abstract = "In a practical real-world scenario, the longstanding goal is that a universal multilingual translation model can be incrementally updated when new language pairs arrive. Specifically, the initial vocabulary only covers some of the words in new languages, which hurts the translation quality for incremental learning. Although existing approaches attempt to address this issue by replacing the original vocabulary with a rebuilt vocabulary or constructing independent language-specific vocabularies, these methods can not meet the following three demands simultaneously: (1) High translation quality for original and incremental languages, (2) low cost for model training, (3) low time overhead for preprocessing. In this work, we propose an entropy-based vocabulary substitution (EVS) method that just needs to walk through new language pairs for incremental learning in a large-scale multilingual data updating while remaining the size of the vocabulary. Our method has access to learn new knowledge from updated training samples incrementally while keeping high translation quality for original language pairs, alleviating the issue of catastrophic forgetting. Results of experiments show that EVS can achieve better performance and save excess overhead for incremental learning in the multilingual machine translation task.",
}
@inproceedings{eikema-aziz-2022-sampling,
    title = "Sampling-Based Approximations to Minimum {B}ayes Risk Decoding for Neural Machine Translation",
    author = "Eikema, Bryan  and
      Aziz, Wilker",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.754",
    doi = "10.18653/v1/2022.emnlp-main.754",
    pages = "10978--10993",
    abstract = "In NMT we search for the mode of the model distribution to form predictions. The mode and other high-probability translations found by beam search have been shown to often be inadequate in a number of ways. This prevents improving translation quality through better search, as these idiosyncratic translations end up selected by the decoding algorithm, a problem known as the beam search curse. Recently, an approximation to minimum Bayes risk (MBR) decoding has been proposed as an alternative decision rule that would likely not suffer from the same problems. We analyse this approximation and establish that it has no equivalent to the beam search curse. We then design approximations that decouple the cost of exploration from the cost of robust estimation of expected utility. This allows for much larger hypothesis spaces, which we show to be beneficial. We also show that mode-seeking strategies can aid in constructing compact sets of promising hypotheses and that MBR is effective in identifying good translations in them. We conduct experiments on three language pairs varying in amounts of resources available: English into and from German, Romanian, and Nepali.",
}
@inproceedings{don-yehiya-etal-2022-prequel,
    title = "{P}re{Q}u{EL}: Quality Estimation of Machine Translation Outputs in Advance",
    author = "Don-Yehiya, Shachar  and
      Choshen, Leshem  and
      Abend, Omri",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.767",
    doi = "10.18653/v1/2022.emnlp-main.767",
    pages = "11170--11183",
    abstract = "We present the task of PreQuEL, Pre-(Quality-Estimation) Learning. A PreQuEL system predicts how well a given sentence will be translated, without recourse to the actual translation, thus eschewing unnecessary resource allocation when translation quality is bound to be low. PreQuEL can be defined relative to a given MT system (e.g., some industry service) or generally relative to the state-of-the-art. From a theoretical perspective, PreQuEL places the focus on the source text, tracing properties, possibly linguistic features, that make a sentence harder to machine translate. We develop a baseline model for the task and analyze its performance. We also develop a data augmentation method (from parallel corpora), that improves results substantially. We show that this augmentation method can improve the performance of the Quality-Estimation task as well. We investigate the properties of the input text that our model is sensitive to, by testing it on challenge sets and different languages. We conclude that it is aware of syntactic and semantic distinctions, and correlates and even over-emphasizes the importance of standard NLP features.",
}
@inproceedings{zhang-misra-2022-machine,
    title = "Machine translation impact in {E}-commerce multilingual search",
    author = "Zhang, Bryan  and
      Misra, Amita",
    editor = "Li, Yunyao  and
      Lazaridou, Angeliki",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-industry.8",
    doi = "10.18653/v1/2022.emnlp-industry.8",
    pages = "99--109",
    abstract = "Previous work suggests that performance of cross-lingual information retrieval correlates highly with the quality of Machine Translation. However, there may be a threshold beyond which improving query translation quality yields little or no benefit to further improve the retrieval performance. This threshold may depend upon multiple factors including the source and target languages, the existing MT system quality and the search pipeline. In order to identify the benefit of improving an MT system for a given search pipeline, we investigate the sensitivity of retrieval quality to the presence of different levels of MT quality using experimental datasets collected from actual traffic. We systematically improve the performance of our MT systems quality on language pairs as measured by MT evaluation metrics including Bleu and Chrf to determine their impact on search precision metrics and extract signals that help to guide the improvement strategies. Using this information we develop techniques to compare query translations for multiple language pairs and identify the most promising language pairs to invest and improve.",
}
@inproceedings{purason-tattar-2022-multilingual,
    title = "Multilingual Neural Machine Translation With the Right Amount of Sharing",
    author = {Purason, Taido  and
      T{\"a}ttar, Andre},
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\"\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.12",
    pages = "91--100",
    abstract = "Large multilingual Transformer-based machine translation models have had a pivotal role in making translation systems available for hundreds of languages with good zero-shot translation performance. One such example is the universal model with shared encoder-decoder architecture. Additionally, jointly trained language-specific encoder-decoder systems have been proposed for multilingual neural machine translation (NMT) models. This work investigates various knowledge-sharing approaches on the encoder side while keeping the decoder language- or language-group-specific. We propose a novel approach, where we use universal, language-group-specific and language-specific modules to solve the shortcomings of both the universal models and models with language-specific encoders-decoders. Experiments on a multilingual dataset set up to model real-world scenarios, including zero-shot and low-resource translation, show that our proposed models achieve higher translation quality compared to purely universal and language-specific approaches.",
}
@inproceedings{mota-etal-2022-fast,
    title = "Fast-Paced Improvements to Named Entity Handling for Neural Machine Translation",
    author = "Mota, Pedro  and
      Cabarrão, Vera  and
      Farah, Eduardo",
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\"\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.17",
    pages = "141--149",
    abstract = "In this work, we propose a Named Entity handling approach to improve translation quality within an existing Natural Language Processing (NLP) pipeline without modifying the Neural Machine Translation (NMT) component. Our approach seeks to enable fast delivery of such improvements and alleviate user experience problems related to NE distortion. We implement separate NE recognition and translation steps. Then, a combination of standard entity masking technique and a novel semantic equivalent placeholder guarantees that both NE translation is respected and the best overall quality is obtained from NMT. The experiments show that translation quality improves in 38.6{\%} of the test cases when compared to a version of the NLP pipeline with less-developed NE handling capability.",
}
@inproceedings{buschbeck-etal-2022-hi,
    title = "{``}Hi, how can {I} help you?{''} Improving Machine Translation of Conversational Content in a Business Context",
    author = "Buschbeck, Bianka  and
      Mell, Jennifer  and
      Exel, Miriam  and
      Huck, Matthias",
    editor = {Moniz, Helena  and
      Macken, Lieve  and
      Rufener, Andrew  and
      Barrault, Lo{\"\i}c  and
      Costa-juss{\`a}, Marta R.  and
      Declercq, Christophe  and
      Koponen, Maarit  and
      Kemp, Ellie  and
      Pilos, Spyridon  and
      Forcada, Mikel L.  and
      Scarton, Carolina  and
      Van den Bogaert, Joachim  and
      Daems, Joke  and
      Tezcan, Arda  and
      Vanroy, Bram  and
      Fonteyne, Margot},
    booktitle = "Proceedings of the 23rd Annual Conference of the European Association for Machine Translation",
    month = jun,
    year = "2022",
    address = "Ghent, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2022.eamt-1.22",
    pages = "191--200",
    abstract = "This paper addresses the automatic translation of conversational content in a business context, for example support chat dialogues. While such use cases share characteristics with other informal machine translation scenarios, translation requirements with respect to technical and business-related expressions are high. To succeed in such scenarios, we experimented with curating dedicated training and test data, injecting noise to improve robustness, and applying sentence weighting schemes to carefully manage the influence of the different corpora. We show that our approach improves the performance of our models on conversational content for all 18 investigated language pairs while preserving translation quality on other domains - an indispensable requirement to integrate these developments into our MT engines at SAP.",
}
@inproceedings{feng-etal-2022-msamsum,
    title = "{MSAMS}um: Towards Benchmarking Multi-lingual Dialogue Summarization",
    author = "Feng, Xiachong  and
      Feng, Xiaocheng  and
      Qin, Bing",
    editor = "Feng, Song  and
      Wan, Hui  and
      Yuan, Caixia  and
      Yu, Han",
    booktitle = "Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dialdoc-1.1",
    doi = "10.18653/v1/2022.dialdoc-1.1",
    pages = "1--12",
    abstract = "Dialogue summarization helps users capture salient information from various types of dialogues has received much attention recently. However, current works mainly focus on English dialogue summarization, leaving other languages less well explored. Therefore, we present a multi-lingual dialogue summarization dataset, namely MSAMSum, which covers dialogue-summary pairs in six languages. Specifically, we derive MSAMSum from the standard SAMSum using sophisticated translation techniques and further employ two methods to ensure the integral translation quality and summary factual consistency. Given the proposed MSAMum, we systematically set up five multi-lingual settings for this task, including a novel mix-lingual dialogue summarization setting. To illustrate the utility of our dataset, we benchmark various experiments with pre-trained models under different settings and report results in both supervised and zero-shot manners. We also discuss some future works towards this task to motivate future researches.",
}
@inproceedings{sun-xiong-2022-language,
    title = "Language Branch Gated Multilingual Neural Machine Translation",
    author = "Sun, Haoran  and
      Xiong, Deyi",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.447",
    pages = "5046--5053",
    abstract = "Knowledge transfer across languages is crucial for multilingual neural machine translation. In this paper, we propose language branch (LB) gated multilingual neural machine translation that encourages knowledge transfer within the same language branch with a LB-gated module that is integrated into both the encoder and decoder. The LB-gated module distinguishes LB-specific parameters from global parameters shared by all languages and routes languages from the same LB to the corresponding LB-specific network. Comprehensive experiments on the OPUS-100 dataset show that the proposed approach substantially improves translation quality on both middle- and low-resource languages over previous methods. Further analysis demonstrates its ability in learning similarities between language branches.",
}
@inproceedings{cheng-etal-2022-semantically,
    title = "Semantically Consistent Data Augmentation for Neural Machine Translation via Conditional Masked Language Model",
    author = "Cheng, Qiao  and
      Huang, Jin  and
      Duan, Yitao",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.457",
    pages = "5148--5157",
    abstract = "This paper introduces a new data augmentation method for neural machine translation that can enforce stronger semantic consistency both within and across languages. Our method is based on Conditional Masked Language Model (CMLM) which is bi-directional and can be conditional on both left and right context, as well as the label. We demonstrate that CMLM is a good technique for generating context-dependent word distributions. In particular, we show that CMLM is capable of enforcing semantic consistency by conditioning on both source and target during substitution. In addition, to enhance diversity, we incorporate the idea of soft word substitution for data augmentation which replaces a word with a probabilistic distribution over the vocabulary. Experiments on four translation datasets of different scales show that the overall solution results in more realistic data augmentation and better translation quality. Our approach consistently achieves the best performance in comparison with strong and recent works and yields improvements of up to 1.90 BLEU points over the baseline.",
}
@inproceedings{wang-geng-2022-improving,
    title = "Improving Non-Autoregressive Neural Machine Translation via Modeling Localness",
    author = "Wang, Yong  and
      Geng, Xinwei",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.463",
    pages = "5217--5226",
    abstract = "Non-autoregressive translation (NAT) models, which eliminate the sequential dependencies within the target sentence, have achieved remarkable inference speed, but suffer from inferior translation quality. Towards exploring the underlying causes, we carry out a thorough preliminary study on the attention mechanism, which demonstrates the serious weakness in capturing localness compared with conventional autoregressive translation (AT). In response to this problem, we propose to improve the localness of NAT models by explicitly introducing the information about surrounding words. Specifically, temporal convolutions are incorporated into both encoder and decoder sides to obtain localness-aware representations. Extensive experiments on several typical translation datasets show that the proposed method can achieve consistent and significant improvements over strong NAT baselines. Further analyses on the WMT14 En-De translation task reveal that compared with baselines, our approach accelerates the convergence in training and can achieve equivalent performance with a reduction of 70{\%} training steps.",
}
@article{esmail-etal-2022-much,
    title = "How Much Does Lookahead Matter for Disambiguation? Partial {A}rabic Diacritization Case Study",
    author = "Esmail, Saeed  and
      Bar, Kfir  and
      Dershowitz, Nachum",
    journal = "Computational Linguistics",
    volume = "48",
    number = "4",
    month = dec,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-4.20",
    doi = "10.1162/coli_a_00456",
    pages = "1103--1123",
    abstract = {We suggest a model for partial diacritization of deep orthographies. We focus on Arabic, where the optional indication of selected vowels by means of diacritics can resolve ambiguity and improve readability. Our partial diacritizer restores short vowels only when they contribute to the ease of understandability during reading a given running text. The idea is to identify those uncertainties of absent vowels that require the reader to look ahead to disambiguate. To achieve this, two independent neural networks are used for predicting diacritics, one that takes the entire sentence as input and another that considers only the text that has been read thus far. Partial diacritization is then determined by retaining precisely those vowels on which the two networks disagree, preferring the reading based on consideration of the whole sentence over the more na{\"\i}ve reading-order diacritization. For evaluation, we prepared a new dataset of Arabic texts with both full and partial vowelization. In addition to facilitating readability, we find that our partial diacritizer improves translation quality compared either to their total absence or to random selection. Lastly, we study the benefit of knowing the text that follows the word in focus toward the restoration of short vowels during reading, and we measure the degree to which lookahead contributes to resolving ambiguities encountered while reading. L{'}Herbelot had asserted, that the most ancient Korans, written in the Cufic character, had no vowel points; and that these were first invented by Jahia{--}ben Jamer, who died in the 127th year of the Hegira. {``}Toderini{'}s History of Turkish Literature,{''} Analytical Review (1789)},
}
@article{wan-etal-2022-challenges,
    title = "Challenges of Neural Machine Translation for Short Texts",
    author = "Wan, Yu  and
      Yang, Baosong  and
      Wong, Derek Fai  and
      Chao, Lidia Sam  and
      Yao, Liang  and
      Zhang, Haibo  and
      Chen, Boxing",
    journal = "Computational Linguistics",
    volume = "48",
    number = "2",
    month = jun,
    year = "2022",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2022.cl-2.3",
    doi = "10.1162/coli_a_00435",
    pages = "321--342",
    abstract = "Short texts (STs) present in a variety of scenarios, including query, dialog, and entity names. Most of the exciting studies in neural machine translation (NMT) are focused on tackling open problems concerning long sentences rather than short ones. The intuition behind is that, with respect to human learning and processing, short sequences are generally regarded as easy examples. In this article, we first dispel this speculation via conducting preliminary experiments, showing that the conventional state-of-the-art NMT approach, namely, Transformer (Vaswani et al. 2017), still suffers from over-translation and mistranslation errors over STs. After empirically investigating the rationale behind this, we summarize two challenges in NMT for STs associated with translation error types above, respectively: (1) the imbalanced length distribution in training set intensifies model inference calibration over STs, leading to more over-translation cases on STs; and (2) the lack of contextual information forces NMT to have higher data uncertainty on short sentences, and thus NMT model is troubled by considerable mistranslation errors. Some existing approaches, like balancing data distribution for training (e.g., data upsampling) and complementing contextual information (e.g., introducing translation memory) can alleviate the translation issues in NMT for STs. We encourage researchers to investigate other challenges in NMT for STs, thus reducing ST translation errors and enhancing translation quality.",
}
@inproceedings{chunyou-etal-2022-towards,
    title = "Towards Making the Most of Pre-trained Translation Model for Quality Estimation",
    author = "Chunyou, Li  and
      Hui, Di  and
      Hui, Huang  and
      Kazushige, Ouchi  and
      Yufeng, Chen  and
      Jian, Liu  and
      Jinan, Xu",
    editor = "Sun, Maosong  and
      Liu, Yang  and
      Che, Wanxiang  and
      Feng, Yang  and
      Qiu, Xipeng  and
      Rao, Gaoqi  and
      Chen, Yubo",
    booktitle = "Proceedings of the 21st Chinese National Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Nanchang, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2022.ccl-1.77",
    pages = "872--883",
    abstract = "{``}Machine translation quality estimation (QE) aims to evaluate the quality of machine translation automatically without relying on any reference. One common practice is applying the translation model as a feature extractor. However, there exist several discrepancies between the translation model and the QE model. The translation model is trained in an autoregressive manner, while the QE model is performed in a non-autoregressive manner. Besides, the translation model only learns to model human-crafted parallel data, while the QE model needs to model machinetranslated noisy data. In order to bridge these discrepancies, we propose two strategies to posttrain the translation model, namely Conditional Masked Language Modeling (CMLM) and Denoising Restoration (DR). Specifically, CMLM learns to predict masked tokens at the target side conditioned on the source sentence. DR firstly introduces noise to the target side of parallel data, and the model is trained to detect and recover the introduced noise. Both strategies can adapt the pre-trained translation model to the QE-style prediction task. Experimental results show that our model achieves impressive results, significantly outperforming the baseline model, verifying the effectiveness of our proposed methods.{''}",
    language = "English",
}
@inproceedings{klubicka-etal-2022-challenges,
    title = "Challenges of Building Domain-Specific Parallel Corpora from Public Administration Documents",
    author = "Klubi{\v{c}}ka, Filip  and
      Kasuni{\'c}, Lorena  and
      Blazsetin, Danijel  and
      Bago, Petra",
    editor = "Rapp, Reinhard  and
      Zweigenbaum, Pierre  and
      Sharoff, Serge",
    booktitle = "Proceedings of the BUCC Workshop within LREC 2022",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.bucc-1.7",
    pages = "50--55",
    abstract = "PRINCIPLE was a Connecting Europe Facility (CEF)-funded project that focused on the identification, collection and processing of language resources (LRs) for four European under-resourced languages (Croatian, Icelandic, Irish and Norwegian) in order to improve translation quality of eTranslation, an online machine translation (MT) tool provided by the European Commission. The collected LRs were used for the development of neural MT engines in order to verify the quality of the resources. For all four languages, a total of 66 LRs were collected and made available on the ELRC-SHARE repository under various licenses. For Croatian, we have collected and published 20 LRs: 19 parallel corpora and 1 glossary. The majority of data is in the general domain (72 {\%} of translation units), while the rest is in the eJustice (23 {\%}), eHealth (3 {\%}) and eProcurement (2 {\%}) Digital Service Infrastructures (DSI) domains. The majority of the resources were for the Croatian-English language pair. The data was donated by six data contributors from the public as well as private sector. In this paper we present a subset of 13 Croatian LRs developed based on public administration documents, which are all made freely available, as well as challenges associated with the data collection, cleaning and processing.",
}
@inproceedings{zou-etal-2022-investigating,
    title = "Investigating the Impact of Different Pivot Languages on Translation Quality",
    author = "Zou, Longhui  and
      Saeedi, Ali  and
      Carl, Michael",
    editor = "Carl, Michael  and
      Yamada, Masaru  and
      Zou, Longui",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Workshop 1: Empirical Translation Process Research)",
    month = sep,
    year = "2022",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-wetpr.3",
    pages = "15--28",
    abstract = "Translating via an intermediate pivot language is a common practice, but the impact of the pivot language on the quality of the final translation has not often been investigated. In order to compare the effect of different pivots, we back-translate 41 English source segments via vari- ous intermediate channels (Arabic, Chinese and monolingual paraphrasing) into English. We compare the 912 English back-translations of the 41 original English segments using manual evaluation, as well as COMET and various incarnations of BLEU. We compare human from- scratch back-translations with MT back-translations and monolingual paraphrasing. A varia- tion of BLEU (Cum-2) seems to better correlate with our manual evaluation than COMET and the conventional BLEU Cum-4, but a fine-grained qualitative analysis reveals that differences between different pivot languages (Arabic and Chinese) are not captured by the automatized TQA measures.",
}
@inproceedings{ogawa-2022-predicting,
    title = "Predicting the number of errors in human translation using source text and translator characteristics",
    author = "Ogawa, Haruka",
    editor = "Carl, Michael  and
      Yamada, Masaru  and
      Zou, Longui",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Workshop 1: Empirical Translation Process Research)",
    month = sep,
    year = "2022",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-wetpr.4",
    pages = "29--40",
    abstract = "Translation quality and efficiency are of great importance in the language services industry, which is why production duration and error counts are frequently investigated in Translation Process Research. However, a clear picture has not yet emerged as to how these two variables can be optimized or how they relate to one another. In the present study, data from multiple English-Japanese translation sessions is used to predict the number of errors per segment using source text and translator characteristics. An analysis utilizing zero-inflated generalized linear mixed effects models revealed that two source text characteristics (syntactic complexity and the proportion of long words) and three translator characteristics (years of experience, the time translators spent reading a source text before translating, and the time translators spent revising a translation) significantly influenced the number of errors. Furthermore, a lower proportion of long words per source text sentence and more training led to a significantly higher probability of error-free translation. Based on these results, combined with findings from a previous study on production duration, it is concluded that years of experience and the duration of the final revision phase are important factors that have a positive impact on translation efficiency and quality",
}
@inproceedings{r-vashee-2022-translation,
    title = "The Translation Impact of Global {CX}",
    author = "Vashee, Kirti R",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.5",
    pages = "50--69",
    abstract = "As global enterprises focus on improving CX (customer experience) across the world we see the following impact: Huge increase in dynamic, unstructured CX related content; Substantial increase in content and translation volumes; Increased use of {``}raw{''} MT; Changes in the view of translation quality; Changes in the kinds of tools and processes used to enable effective massive-scale translation capabilities. This presentation will provide examples of the content changes and it{'}s impact on optimal tools and the translation production process. Several use case and case studies will be provided to illustrate the growing need for better man-machine collaboration and will also highlight emerging best practices that show that MT has only begun it{'}s deep engagement with international business initiatives for any global enterprise.",
}
@inproceedings{che-xiao-2022-data,
    title = "Data Analytics Meet Machine Translation",
    author = "Che, Allen  and
      Xiao, Martin",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.11",
    pages = "144--158",
    abstract = "Machine translation becomes a critical piece of localization industry. With all kinds of different data, how to monitor the machine translation quality in your localized content? How to build the quality analytics framework? This paper describes a process starting from collecting the daily operation data then cleaning the data and building the analytics framework to get the insight into the data. Finally we{'}re going to share how to build the data collecting matrix, and the script to clean up the data, then run the analytics with an automation script. In the last, we would share the different visualized reports, such as Box Polit, Standard Deviation, Mean, MT touchpoint and golden ratio reports.",
}
@inproceedings{bittlingmayer-etal-2022-quality,
    title = "Quality Prediction",
    author = "Bittlingmayer, Adam  and
      Zubarev, Boris  and
      Aleksanyan, Artur",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.12",
    pages = "159--180",
    abstract = "A growing share of machine translations are approved - untouched - by human translators in post-editing workflows. But they still cost time and money. Now companies are getting human post-editing quality faster and cheaper, by automatically approving the good machine translations - at human accuracy. The approach has evolved, from research papers on machine translation quality estimation, to adoption inside companies like Amazon, Facebook, Microsoft and VMWare, to self-serve cloud APIs like ModelFront. We{'}ll walk through the motivations, use cases, prerequisites, adopters, providers, integration and ROI.",
}
@inproceedings{garland-etal-2022-comparison,
    title = "Comparison Between {ATA} Grading Framework Scores and Auto Scores",
    author = "Garland, Evelyn  and
      Berger, Carola  and
      Ritzdorf, Jon",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.13",
    pages = "181--201",
    abstract = "The authors of this study compared two types of translation quality scores assigned to the same sets of translation samples: 1) the ATA Grading Framework scores assigned by human experts, and 2) auto scores, including BLEU, TER, and COMET (with and without reference). They further explored the impact of different reference translations on the auto scores. Key findings from this study include: 1. auto scores that rely on reference translations depend heavily on which reference is used; 2. referenceless COMET seems promising when it is used to evaluate translations of short passages (250-300 English words); and 3. evidence suggests good agreement between the ATA-Framework score and some auto scores within a middle range, but the relationship becomes non-monotonic beyond the middle range. This study is subject to the limitation of a small sample size and is a retrospective exploratory study not specifically designed to test a pre-defined hypothesis.",
}
@inproceedings{cambra-nunziatini-2022-need,
    title = "All You Need is Source! A Study on Source-based Quality Estimation for Neural Machine Translation",
    author = "Cambra, Jon  and
      Nunziatini, Mara",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.15",
    pages = "210--220",
    abstract = "Segment-level Quality Estimation (QE) is an increasingly sought-after task in the Machine Translation (MT) industry. In recent years, it has experienced an impressive evolution not only thanks to the implementation of supervised models using source and hypothesis information, but also through the usage of MT probabilities. This work presents a different approach to QE where only the source segment and the Neural MT (NMT) training data are needed, making possible an approximation to translation quality before inference. Our work is based on the idea that NMT quality at a segment level depends on the similarity degree between the source segment to be translated and the engine{'}s training data. The features proposed measuring this aspect of data achieve competitive correlations with MT metrics and human judgment and prove to be advantageous for post-editing (PE) prioritization task with domain adapted engines.",
}
@inproceedings{stewart-etal-2022-business,
    title = "Business Critical Errors: A Framework for Adaptive Quality Feedback",
    author = "Stewart, Craig A  and
      Gon{\c{c}}alves, Madalena  and
      Buchicchio, Marianna  and
      Lavie, Alon",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.17",
    pages = "231--256",
    abstract = "Frameworks such as Multidimensional Quality Metrics (MQM) provide detailed feedback on translation quality and can pinpoint concrete linguistic errors. The quality of a translation is, however, also closely tied to its utility in a particular use case. Many customers have highly subjective expectations of translation quality. Features such as register, discourse style and brand consistency can be difficult to accommodate given a broadly applied translation solution. In this presentation we will introduce the concept of Business Critical Errors (BCE). Adapted from MQM, the BCE framework provides a perspective on translation quality that allows us to be reactive and adaptive to expectation whilst also maintaining consistency in our translation evaluation. We will demonstrate tooling used at Unbabel that allows us to evaluate the performance of our MT models on BCE using specialized test suites as well as the ability of our AI evaluation models to successfully capture BCE information.",
}
@inproceedings{giulianelli-2022-customization,
    title = "Customization options for language pairs without {E}nglish",
    author = "Giulianelli, Daniele",
    editor = "Campbell, Janice  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of the 15th Biennial Conference of the Association for Machine Translation in the Americas (Volume 2: Users and Providers Track and Government Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-upg.19",
    pages = "270--281",
    abstract = "At Comparis, we are rolling out our MT program for locales with limited support out-of-the-box and language pairs with limited support for customization. As a leading online company in Switzerland, our content goes from Swiss Standard German (de-CH) into fr-CH, it-CH and en-UK. Even the best generic MT engines perform poorly and many don{'}t even offer customization for language pairs without English. This would result in unusable raw MT and very high PE effort. So we needed custom machine translation, but at a reasonable cost and with a sustainable effort. We evaluated the self-serve machine translation, the machine translation quality estimation tools like ModelFront, and integration options in the translation management systems (TMSes). Using new tools and our existing assets (TMs), custom MT and new AI tools we launched a successful in-house MT program with productivity gains and iterative improvement. We also defined and launched service tiers, from light MTPE to transcreation.",
}
@inproceedings{araabi-etal-2022-effective,
    title = "How Effective is Byte Pair Encoding for Out-Of-Vocabulary Words in Neural Machine Translation?",
    author = "Araabi, Ali  and
      Monz, Christof  and
      Niculae, Vlad",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-research.9",
    pages = "117--130",
    abstract = "Neural Machine Translation (NMT) is an open vocabulary problem. As a result, dealing with the words not occurring during training (a.k.a. out-of-vocabulary (OOV) words) have long been a fundamental challenge for NMT systems. The predominant method to tackle this problem is Byte Pair Encoding (BPE) which splits words, including OOV words, into sub-word segments. BPE has achieved impressive results for a wide range of translation tasks in terms of automatic evaluation metrics. While it is often assumed that by using BPE, NMT systems are capable of handling OOV words, the effectiveness of BPE in translating OOV words has not been explicitly measured. In this paper, we study to what extent BPE is successful in translating OOV words at the word-level. We analyze the translation quality of OOV words based on word type, number of segments, cross-attention weights, and the frequency of segment n-grams in the training data. Our experiments show that while careful BPE settings seem to be fairly useful in translating OOV words across datasets, a considerable percentage of OOV words are translated incorrectly. Furthermore, we highlight the slightly higher effectiveness of BPE in translating OOV words for special cases, such as named-entities and when the languages involved are linguistically close to each other.",
}
@inproceedings{bhardwa-etal-2022-refining,
    title = "Refining an Almost Clean Translation Memory Helps Machine Translation",
    author = "Bhardwa, Shivendra  and
      Alfonso-Hermelo, David  and
      Langlais, Philippe  and
      Bernier-Colborne, Gabriel  and
      Goutte, Cyril  and
      Simard, Michel",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-research.16",
    pages = "215--226",
    abstract = "While recent studies have been dedicated to cleaning very noisy parallel corpora to improve Machine Translation training, we focus in this work on filtering a large and mostly clean Translation Memory. This problem of practical interest has not received much consideration from the community, in contrast with, for example, filtering large web-mined parallel corpora. We experiment with an extensive, multi-domain proprietary Translation Memory and compare five approaches involving deep-, feature-, and heuristic-based solutions. We propose two ways of evaluating this task, manual annotation and resulting Machine Translation quality. We report significant gains over a state-of-the-art, off-the-shelf cleaning system, using two MT engines.",
}
@inproceedings{kovacs-denero-2022-measuring,
    title = "Measuring the Effects of Human and Machine Translation on Website Engagement",
    author = "Kovacs, Geza  and
      DeNero, John",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-research.23",
    pages = "298--308",
    abstract = "With the internet growing increasingly multilingual, it is important to consider translating websites. However, professional translators are much more expensive than machines, and machine translation quality is continually increasing, so we must justify the cost of professional translation by measuring the effects of translation on website engagement, and how users interact with translations. This paper presents an in-the-wild study run on 2 websites fully translated into 15 and 11 languages respectively, where visitors with non-English preferred languages were randomized into being shown text translated by a professional translator, machine translated text, or untranslated English text. We find that both human and machine translations improve engagement, users rarely switch the page language manually, and that in-browser machine translation is often used when English is shown, particularly by users from countries with low English proficiency. We also release a dataset of interaction data collected during our studies, including 3,332,669 sessions from 190 countries across 2 websites.",
}
@inproceedings{licht-etal-2022-consistent,
    title = "Consistent Human Evaluation of Machine Translation across Language Pairs",
    author = "Licht, Daniel  and
      Gao, Cynthia  and
      Lam, Janice  and
      Guzman, Francisco  and
      Diab, Mona  and
      Koehn, Philipp",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 15th biennial conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = sep,
    year = "2022",
    address = "Orlando, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2022.amta-research.24",
    pages = "309--321",
    abstract = "Obtaining meaningful quality scores for machine translation systems through human evaluation remains a challenge given the high variability between human evaluators, partly due to subjective expectations for translation quality for different language pairs. We propose a new metric called XSTS that is more focused on semantic equivalence and a cross-lingual calibration method that enables more consistent assessment. We demonstrate the effectiveness of these novel contributions in large scale evaluation studies across up to 14 language pairs, with translation both into and out of English.",
}
@inproceedings{rikters-etal-2022-machine,
    title = "Machine Translation for {L}ivonian: Catering to 20 Speakers",
    author = "Rikters, Mat{\=\i}ss  and
      Tomingas, Marili  and
      Tuisk, Tuuli  and
      Ern{\v{s}}treits, Valts  and
      Fishel, Mark",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.55",
    doi = "10.18653/v1/2022.acl-short.55",
    pages = "508--514",
    abstract = "Livonian is one of the most endangered languages in Europe with just a tiny handful of speakers and virtually no publicly available corpora. In this paper we tackle the task of developing neural machine translation (NMT) between Livonian and English, with a two-fold aim: on one hand, preserving the language and on the other {--} enabling access to Livonian folklore, lifestories and other textual intangible heritage as well as making it easier to create further parallel corpora. We rely on Livonian{'}s linguistic similarity to Estonian and Latvian and collect parallel and monolingual data for the four languages for translation experiments. We combine different low-resource NMT techniques like zero-shot translation, cross-lingual transfer and synthetic data creation to reach the highest possible translation quality as well as to find which base languages are empirically more helpful for transfer to Livonian. The resulting NMT systems and the collected monolingual and parallel data, including a manually translated and verified translation benchmark, are publicly released via OPUS and Huggingface repositories.",
}
@inproceedings{chen-etal-2022-focus,
    title = "Focus on the Target{'}s Vocabulary: Masked Label Smoothing for Machine Translation",
    author = "Chen, Liang  and
      Xu, Runxin  and
      Chang, Baobao",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.74",
    doi = "10.18653/v1/2022.acl-short.74",
    pages = "665--671",
    abstract = "Label smoothing and vocabulary sharing are two widely used techniques in neural machine translation models. However, we argue that simply applying both techniques can be conflicting and even leads to sub-optimal performance. When allocating smoothed probability, original label smoothing treats the source-side words that would never appear in the target language equally to the real target-side words, which could bias the translation model. To address this issue, we propose Masked Label Smoothing (MLS), a new mechanism that masks the soft label probability of source-side words to zero. Simple yet effective, MLS manages to better integrate label smoothing with vocabulary sharing. Our extensive experiments show that MLS consistently yields improvement over original label smoothing on different datasets, including bilingual and multilingual translation from both translation quality and model{'}s calibration. Our code is released at \url{https://github.com/PKUnlp-icler/MLS}",
}
@inproceedings{dong-etal-2022-learning,
    title = "Learning When to Translate for Streaming Speech",
    author = "Dong, Qian  and
      Zhu, Yaoming  and
      Wang, Mingxuan  and
      Li, Lei",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.50",
    doi = "10.18653/v1/2022.acl-long.50",
    pages = "680--694",
    abstract = "How to find proper moments to generate partial sentence translation given a streaming speech input? Existing approaches waiting-and-translating for a fixed duration often break the acoustic units in speech, since the boundaries between acoustic units in speech are not even. In this paper, we propose MoSST, a simple yet effective method for translating streaming speech content. Given a usually long speech sequence, we develop an efficient monotonic segmentation module inside an encoder-decoder model to accumulate acoustic information incrementally and detect proper speech unit boundaries for the input in speech translation task. Experiments on multiple translation directions of the MuST-C dataset show that outperforms existing methods and achieves the best trade-off between translation quality (BLEU) and latency. Our code is available at \url{https://github.com/dqqcasia/mosst}.",
}
@inproceedings{behnke-etal-2022-bias,
    title = "Bias Mitigation in Machine Translation Quality Estimation",
    author = "Behnke, Hanna  and
      Fomicheva, Marina  and
      Specia, Lucia",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.104",
    doi = "10.18653/v1/2022.acl-long.104",
    pages = "1475--1487",
    abstract = "Machine Translation Quality Estimation (QE) aims to build predictive models to assess the quality of machine-generated translations in the absence of reference translations. While state-of-the-art QE models have been shown to achieve good results, they over-rely on features that do not have a causal impact on the quality of a translation. In particular, there appears to be a partial input bias, i.e., a tendency to assign high-quality scores to translations that are fluent and grammatically correct, even though they do not preserve the meaning of the source. We analyse the partial input bias in further detail and evaluate four approaches to use auxiliary tasks for bias mitigation. Two approaches use additional data to inform and support the main task, while the other two are adversarial, actively discouraging the model from learning the bias. We compare the methods with respect to their ability to reduce the partial input bias while maintaining the overall performance. We find that training a multitask architecture with an auxiliary binary classification task that utilises additional augmented data best achieves the desired effects and generalises well to different languages and quality metrics.",
}
@inproceedings{wang-etal-2022-efficient,
    title = "Efficient Cluster-Based $k$-Nearest-Neighbor Machine Translation",
    author = "Wang, Dexin  and
      Fan, Kai  and
      Chen, Boxing  and
      Xiong, Deyi",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.154",
    doi = "10.18653/v1/2022.acl-long.154",
    pages = "2175--2187",
    abstract = "$k$-Nearest-Neighbor Machine Translation ($k$NN-MT) has been recently proposed as a non-parametric solution for domain adaptation in neural machine translation (NMT). It aims to alleviate the performance degradation of advanced MT systems in translating out-of-domain sentences by coordinating with an additional token-level feature-based retrieval module constructed from in-domain data. Previous studies (Khandelwal et al., 2021; Zheng et al., 2021) have already demonstrated that non-parametric NMT is even superior to models fine-tuned on out-of-domain data. In spite of this success, $k$NN retrieval is at the expense of high latency, in particular for large datastores. To make it practical, in this paper, we explore a more efficient $k$NN-MT and propose to use clustering to improve the retrieval efficiency. Concretely, we first propose a cluster-based Compact Network for feature reduction in a contrastive learning manner to compress context features into 90+{\%} lower dimensional vectors. We then suggest a cluster-based pruning solution to filter out 10{\%} 40{\%} redundant nodes in large datastores while retaining translation quality. Our proposed methods achieve better or comparable performance while reducing up to 57{\%} inference latency against the advanced non-parametric MT model on several machine translation benchmarks. Experimental results indicate that the proposed methods maintain the most useful information of the original datastore and the Compact Network shows good generalization on unseen domains. Codes are available at \url{https://github.com/tjunlp-lab/PCKMT}.",
}
@inproceedings{wang-etal-2022-measuring,
    title = "Measuring and Mitigating Name Biases in Neural Machine Translation",
    author = "Wang, Jun  and
      Rubinstein, Benjamin  and
      Cohn, Trevor",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.184",
    doi = "10.18653/v1/2022.acl-long.184",
    pages = "2576--2590",
    abstract = "Neural Machine Translation (NMT) systems exhibit problematic biases, such as stereotypical gender bias in the translation of occupation terms into languages with grammatical gender. In this paper we describe a new source of bias prevalent in NMT systems, relating to translations of sentences containing person names. To correctly translate such sentences, a NMT system needs to determine the gender of the name. We show that leading systems are particularly poor at this task, especially for female given names. This bias is deeper than given name gender: we show that the translation of terms with ambiguous sentiment can also be affected by person names, and the same holds true for proper nouns denoting race. To mitigate these biases we propose a simple but effective data augmentation method based on randomly switching entities during translation, which effectively eliminates the problem without any effect on translation quality.",
}
@inproceedings{wang-etal-2022-understanding,
    title = "Understanding and Improving Sequence-to-Sequence Pretraining for Neural Machine Translation",
    author = "Wang, Wenxuan  and
      Jiao, Wenxiang  and
      Hao, Yongchang  and
      Wang, Xing  and
      Shi, Shuming  and
      Tu, Zhaopeng  and
      Lyu, Michael",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.185",
    doi = "10.18653/v1/2022.acl-long.185",
    pages = "2591--2600",
    abstract = "In this paper, we present a substantial step in better understanding the SOTA sequence-to-sequence (Seq2Seq) pretraining for neural machine translation (NMT). We focus on studying the impact of the jointly pretrained decoder, which is the main difference between Seq2Seq pretraining and previous encoder-based pretraining approaches for NMT. By carefully designing experiments on three language pairs, we find that Seq2Seq pretraining is a double-edged sword: On one hand, it helps NMT models to produce more diverse translations and reduce adequacy-related translation errors. On the other hand, the discrepancies between Seq2Seq pretraining and NMT finetuning limit the translation quality (i.e., domain discrepancy) and induce the over-estimation issue (i.e., objective discrepancy). Based on these observations, we further propose simple and effective strategies, named in-domain pretraining and input adaptation to remedy the domain and objective discrepancies, respectively. Experimental results on several language pairs show that our approach can consistently improve both translation performance and model robustness upon Seq2Seq pretraining.",
}
@inproceedings{lupo-etal-2022-divide,
    title = "Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models",
    author = "Lupo, Lorenzo  and
      Dinarelli, Marco  and
      Besacier, Laurent",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.312",
    doi = "10.18653/v1/2022.acl-long.312",
    pages = "4557--4572",
    abstract = "Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context. We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons. Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently. Secondly, it eases the retrieval of relevant context, since context segments become shorter. We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets. Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.",
}
@inproceedings{iranzo-sanchez-etal-2022-simultaneous,
    title = "From Simultaneous to Streaming Machine Translation by Leveraging Streaming History",
    author = "Iranzo Sanchez, Javier  and
      Civera, Jorge  and
      Juan-C{\'\i}scar, Alfons",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.480",
    doi = "10.18653/v1/2022.acl-long.480",
    pages = "6972--6985",
    abstract = "Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream-level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task",
}
@inproceedings{zhang-etal-2022-learning,
    title = "Learning Adaptive Segmentation Policy for End-to-End Simultaneous Translation",
    author = "Zhang, Ruiqing  and
      He, Zhongjun  and
      Wu, Hua  and
      Wang, Haifeng",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.542",
    doi = "10.18653/v1/2022.acl-long.542",
    pages = "7862--7874",
    abstract = "End-to-end simultaneous speech-to-text translation aims to directly perform translation from streaming source speech to target text with high translation quality and low latency. A typical simultaneous translation (ST) system consists of a speech translation model and a policy module, which determines when to wait and when to translate. Thus the policy is crucial to balance translation quality and latency. Conventional methods usually adopt fixed policies, e.g. segmenting the source speech with a fixed length and generating translation. However, this method ignores contextual information and suffers from low translation quality. This paper proposes an adaptive segmentation policy for end-to-end ST. Inspired by human interpreters, the policy learns to segment the source streaming speech into meaningful units by considering both acoustic features and translation history, maintaining consistency between the segmentation and translation. Experimental results on English-German and Chinese-English show that our method achieves a good accuracy-latency trade-off over recently proposed state-of-the-art methods.",
}
@inproceedings{wan-etal-2022-unite,
    title = "{U}ni{TE}: Unified Translation Evaluation",
    author = "Wan, Yu  and
      Liu, Dayiheng  and
      Yang, Baosong  and
      Zhang, Haibo  and
      Chen, Boxing  and
      Wong, Derek  and
      Chao, Lidia",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.558",
    doi = "10.18653/v1/2022.acl-long.558",
    pages = "8117--8127",
    abstract = "Translation quality evaluation plays a crucial role in machine translation. According to the input format, it is mainly separated into three tasks, \textit{i.e.}, reference-only, source-only and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of them. This limits the convenience of these methods, and overlooks the commonalities among tasks. In this paper, we propose , which is the first unified framework engaged with abilities to handle all three evaluation tasks. Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks. Extensive analyses show that our \textit{single model} can universally surpass various state-of-the-art or winner methods across tasks.Both source code and associated models are available at \url{https://github.com/NLP2CT/UniTE}.",
}
@inproceedings{kulikov-etal-2022-characterizing,
    title = "Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling",
    author = "Kulikov, Ilia  and
      Eremeev, Maksim  and
      Cho, Kyunghyun",
    editor = "He, Yulan  and
      Ji, Heng  and
      Li, Sujian  and
      Liu, Yang  and
      Chang, Chua-Hui",
    booktitle = "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2022",
    address = "Online only",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.aacl-main.82",
    pages = "1115--1124",
    abstract = "Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine translation, we propose to explicitly minimize the oversmoothing rate during training. We conduct a set of experiments to study the effect of the proposed regularization on both model distribution and decoding performance. We use a neural machine translation task as the testbed and consider three different datasets of varying size. Our experiments reveal three major findings. First, we can control the oversmoothing rate of the model by tuning the strength of the regularization. Second, by enhancing the oversmoothing loss contribution, the probability and the rank of eos token decrease heavily at positions where it is not supposed to be. Third, the proposed regularization impacts the outcome of beam search especially when a large beam is used. The degradation of translation quality (measured in BLEU) with a large beam significantly lessens with lower oversmoothing rate, but the degradation compared to smaller beam sizes remains to exist. From these observations, we conclude that the high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model.",
}
@inproceedings{rosales-nunez-etal-2021-understanding,
    title = "Understanding the Impact of {UGC} Specificities on Translation Quality",
    author = "Rosales N{\'u}{\~n}ez, Jos{\'e} Carlos  and
      Seddah, Djam{\'e}  and
      Wisniewski, Guillaume",
    editor = "Xu, Wei  and
      Ritter, Alan  and
      Baldwin, Tim  and
      Rahimi, Afshin",
    booktitle = "Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021)",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wnut-1.22",
    doi = "10.18653/v1/2021.wnut-1.22",
    pages = "189--198",
    abstract = "This work takes a critical look at the evaluation of user-generated content automatic translation, the well-known specificities of which raise many challenges for MT. Our analyses show that measuring the average-case performance using a standard metric on a UGC test set falls far short of giving a reliable image of the UGC translation quality. That is why we introduce a new data set for the evaluation of UGC translation in which UGC specificities have been manually annotated using a fine-grained typology. Using this data set, we conduct several experiments to measure the impact of different kinds of UGC specificities on translation quality, more precisely than previously possible.",
}
@inproceedings{tran-etal-2021-facebook,
    title = "{F}acebook {AI}{'}s {WMT}21 News Translation Task Submission",
    author = "Tran, Chau  and
      Bhosale, Shruti  and
      Cross, James  and
      Koehn, Philipp  and
      Edunov, Sergey  and
      Fan, Angela",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.19",
    pages = "205--215",
    abstract = "We describe Facebook{'}s multilingual model submission to the WMT2021 shared task on news translation. We participate in 14 language directions: English to and from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. To develop systems covering all these directions, we focus on multilingual models. We utilize data from all available sources {---} WMT, large-scale data mining, and in-domain backtranslation {---} to create high quality bilingual and multilingual baselines. Subsequently, we investigate strategies for scaling multilingual model size, such that one system has sufficient capacity for high quality representations of all eight languages. Our final submission is an ensemble of dense and sparse Mixture-of-Expert multilingual translation models, followed by finetuning on in-domain news data and noisy channel reranking. Compared to previous year{'}s winning submissions, our multilingual system improved the translation quality on all language directions, with an average improvement of 2.0 BLEU. In the WMT2021 task, our system ranks first in 10 directions based on automatic evaluation.",
}
@inproceedings{jon-etal-2021-cuni,
    title = "{CUNI} systems for {WMT}21: Multilingual Low-Resource Translation for {I}ndo-{E}uropean Languages Shared Task",
    author = "Jon, Josef  and
      Nov{\'a}k, Michal  and
      Aires, Jo{\~a}o Paulo  and
      Varis, Dusan  and
      Bojar, Ond{\v{r}}ej",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.42",
    pages = "354--361",
    abstract = "This paper describes Charles University sub-mission for Terminology translation shared task at WMT21. The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database.",
}
@inproceedings{yang-etal-2021-tentrans,
    title = "{T}en{T}rans Multilingual Low-Resource Translation System for {WMT}21 {I}ndo-{E}uropean Languages Task",
    author = "Yang, Han  and
      Hu, Bojie  and
      Xie, Wanying  and
      Han, Ambyera  and
      Liu, Pan  and
      Xu, Jinan  and
      Ju, Qi",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.45",
    pages = "376--382",
    abstract = "This paper describes TenTrans{'} submission to WMT21 Multilingual Low-Resource Translation shared task for the Romance language pairs. This task focuses on improving translation quality from Catalan to Occitan, Romanian and Italian, with the assistance of related high-resource languages. We mainly utilize back-translation, pivot-based methods, multilingual models, pre-trained model fine-tuning, and in-domain knowledge transfer to improve the translation quality. On the test set, our best-submitted system achieves an average of 43.45 case-sensitive BLEU scores across all low-resource pairs. Our data, code, and pre-trained models used in this work are available in TenTrans evaluation examples.",
}
@inproceedings{liu-niehues-2021-maastricht-universitys,
    title = "Maastricht University{'}s Large-Scale Multilingual Machine Translation System for {WMT} 2021",
    author = "Liu, Danni  and
      Niehues, Jan",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.51",
    pages = "425--430",
    abstract = "We present our development of the multilingual machine translation system for the large-scale multilingual machine translation task at WMT 2021. Starting form the provided baseline system, we investigated several techniques to improve the translation quality on the target subset of languages. We were able to significantly improve the translation quality by adapting the system towards the target subset of languages and by generating synthetic data using the initial model. Techniques successfully applied in zero-shot multilingual machine translation (e.g. similarity regularizer) only had a minor effect on the final translation performance.",
}
@inproceedings{knowles-2021-stability,
    title = "On the Stability of System Rankings at {WMT}",
    author = "Knowles, Rebecca",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.56",
    pages = "464--477",
    abstract = "The current approach to collecting human judgments of machine translation quality for the news translation task at WMT {--} segment rating with document context {--} is the most recent in a sequence of changes to WMT human annotation protocol. As these annotation protocols have changed over time, they have drifted away from some of the initial statistical assumptions underpinning them, with consequences that call the validity of WMT news task system rankings into question. In simulations based on real data, we show that the rankings can be influenced by the presence of outliers (high- or low-quality systems), resulting in different system rankings and clusterings. We also examine questions of annotation task composition and how ease or difficulty of translating different documents may influence system rankings. We provide discussion of ways to analyze these issues when considering future changes to annotation protocols.",
}
@inproceedings{kocmi-etal-2021-ship,
    title = "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation",
    author = "Kocmi, Tom  and
      Federmann, Christian  and
      Grundkiewicz, Roman  and
      Junczys-Dowmunt, Marcin  and
      Matsushita, Hitokazu  and
      Menezes, Arul",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.57",
    pages = "478--494",
    abstract = "Automatic metrics are commonly used as the exclusive tool for declaring the superiority of one machine translation system{'}s quality over another. The community choice of automatic metric guides research directions and industrial developments by deciding which models are deemed better. Evaluating metrics correlations with sets of human judgements has been limited by the size of these sets. In this paper, we corroborate how reliable metrics are in contrast to human judgements on {--} to the best of our knowledge {--} the largest collection of judgements reported in the literature. Arguably, pairwise rankings of two systems are the most common evaluation tasks in research or deployment scenarios. Taking human judgement as a gold standard, we investigate which metrics have the highest accuracy in predicting translation quality rankings for such system pairs. Furthermore, we evaluate the performance of various metrics across different language pairs and domains. Lastly, we show that the sole use of BLEU impeded the development of improved models leading to bad deployment decisions. We release the collection of 2.3M sentence-level human judgements for 4380 systems for further analysis and replication of our work.",
}
@inproceedings{hanna-bojar-2021-fine,
    title = "A Fine-Grained Analysis of {BERTS}core",
    author = "Hanna, Michael  and
      Bojar, Ond{\v{r}}ej",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.59",
    pages = "507--517",
    abstract = "BERTScore, a recently proposed automatic metric for machine translation quality, uses BERT, a large pre-trained language model to evaluate candidate translations with respect to a gold translation. Taking advantage of BERT{'}s semantic and syntactic abilities, BERTScore seeks to avoid the flaws of earlier approaches like BLEU, instead scoring candidate translations based on their semantic similarity to the gold sentence. However, BERT is not infallible; while its performance on NLP tasks set a new state of the art in general, studies of specific syntactic and semantic phenomena have shown where BERT{'}s performance deviates from that of humans more generally. This naturally raises the questions we address in this paper: what are the strengths and weaknesses of BERTScore? Do they relate to known weaknesses on the part of BERT? We find that while BERTScore can detect when a candidate differs from a reference in important content words, it is less sensitive to smaller errors, especially if the candidate is lexically or stylistically similar to the reference.",
}
@inproceedings{hangya-etal-2021-improving,
    title = "Improving Machine Translation of Rare and Unseen Word Senses",
    author = "Hangya, Viktor  and
      Liu, Qianchu  and
      Stojanovski, Dario  and
      Fraser, Alexander  and
      Korhonen, Anna",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.66",
    pages = "614--624",
    abstract = "The performance of NMT systems has improved drastically in the past few years but the translation of multi-sense words still poses a challenge. Since word senses are not represented uniformly in the parallel corpora used for training, there is an excessive use of the most frequent sense in MT output. In this work, we propose CmBT (Contextually-mined Back-Translation), an approach for improving multi-sense word translation leveraging pre-trained cross-lingual contextual word representations (CCWRs). Because of their contextual sensitivity and their large pre-training data, CCWRs can easily capture word senses that are missing or very rare in parallel corpora used to train MT. Specifically, CmBT applies bilingual lexicon induction on CCWRs to mine sense-specific target sentences from a monolingual dataset, and then back-translates these sentences to generate a pseudo parallel corpus as additional training data for an MT system. We test the translation quality of ambiguous words on the MuCoW test suite, which was built to test the word sense disambiguation effectiveness of MT systems. We show that our system improves on the translation of difficult unseen and low frequency word senses.",
}
@inproceedings{heafield-etal-2021-findings,
    title = "Findings of the {WMT} 2021 Shared Task on Efficient Translation",
    author = "Heafield, Kenneth  and
      Zhu, Qianqian  and
      Grundkiewicz, Roman",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.68",
    pages = "639--651",
    abstract = "The machine translation efficiency task challenges participants to make their systems faster and smaller with minimal impact on translation quality. How much quality to sacrifice for efficiency depends upon the application, so participants were encouraged to make multiple submissions covering the space of trade-offs. In total, there were 53 submissions by 4 teams. There were GPU, single-core CPU, and multi-core CPU hardware tracks as well as batched throughput or single-sentence latency conditions. Submissions showed hundreds of millions of words can be translated for a dollar, average latency is 5{--}17 ms, and models fit in 7.5{--}150 MB.",
}
@inproceedings{shang-etal-2021-hw,
    title = "{HW}-{TSC}{'}s Participation in the {WMT} 2021 Efficiency Shared Task",
    author = "Shang, Hengchao  and
      Hu, Ting  and
      Wei, Daimeng  and
      Li, Zongyao  and
      Feng, Jianfei  and
      Yu, ZhengZhe  and
      Guo, Jiaxin  and
      Li, Shaojun  and
      Lei, Lizhi  and
      Tao, ShiMin  and
      Yang, Hao  and
      Yao, Jun  and
      Qin, Ying",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.75",
    pages = "781--786",
    abstract = "This paper presents the submission of Huawei Translation Services Center (HW-TSC) to WMT 2021 Efficiency Shared Task. We explore the sentence-level teacher-student distillation technique and train several small-size models that find a balance between efficiency and quality. Our models feature deep encoder, shallow decoder and light-weight RNN with SSRU layer. We use Huawei Noah{'}s Bolt, an efficient and light-weight library for on-device inference. Leveraging INT8 quantization, self-defined General Matrix Multiplication (GEMM) operator, shortlist, greedy search and caching, we submit four small-size and efficient translation models with high translation quality for the one CPU core latency track.",
}
@inproceedings{wang-etal-2021-niutrans,
    title = "The {N}iu{T}rans System for the {WMT} 2021 Efficiency Task",
    author = "Wang, Chenglong  and
      Hu, Chi  and
      Mu, Yongyu  and
      Yan, Zhongxiang  and
      Wu, Siming  and
      Hu, Yimin  and
      Cao, Hang  and
      Li, Bei  and
      Lin, Ye  and
      Xiao, Tong  and
      Zhu, Jingbo",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.76",
    pages = "787--794",
    abstract = "This paper describes the NiuTrans system for the WMT21 translation efficiency task. Following last year{'}s work, we explore various techniques to improve the efficiency while maintaining translation quality. We investigate the combinations of lightweight Transformer architectures and knowledge distillation strategies. Also, we improve the translation efficiency with graph optimization, low precision, dynamic batching, and parallel pre/post-processing. Putting these together, our system can translate 247,000 words per second on an NVIDIA A100, being 3$\times$ faster than our last year{'}s system. Our system is the fastest and has the lowest memory consumption on the GPU-throughput track. The code, model, and pipeline will be available at NiuTrans.NMT.",
}
@inproceedings{ailem-etal-2021-lingua,
    title = "Lingua Custodia{'}s Participation at the {WMT} 2021 Machine Translation Using Terminologies Shared Task",
    author = "Ailem, Melissa  and
      Liu, Jingshu  and
      Qader, Raheel",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.78",
    pages = "799--803",
    abstract = "This paper describes Lingua Custodia{'}s submission to the WMT21 shared task on machine translation using terminologies. We consider three directions, namely English to French, Russian, and Chinese. We rely on a Transformer-based architecture as a building block, and we explore a method which introduces two main changes to the standard procedure to handle terminologies. The first one consists in augmenting the training data in such a way as to encourage the model to learn a copy behavior when it encounters terminology constraint terms. The second change is constraint token masking, whose purpose is to ease copy behavior learning and to improve model generalization. Empirical results show that our method satisfies most terminology constraints while maintaining high translation quality.",
}
@inproceedings{bak-etal-2021-kakao,
    title = "Kakao Enterprise{'}s {WMT}21 Machine Translation Using Terminologies Task Submission",
    author = "Bak, Yunju  and
      Sun, Jimin  and
      Kim, Jay  and
      Lyu, Sungwon  and
      Lee, Changmin",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.79",
    pages = "804--812",
    abstract = "This paper describes Kakao Enterprise{'}s submission to the WMT21 shared Machine Translation using Terminologies task. We integrate terminology constraints by pre-training with target lemma annotations and fine-tuning with exact target annotations utilizing the given terminology dataset. This approach yields a model that achieves outstanding results in terms of both translation quality and term consistency, ranking first based on COMET in the En→Fr language direction. Furthermore, we explore various methods such as back-translation, explicitly training terminologies as additional parallel data, and in-domain data selection.",
}
@inproceedings{jon-etal-2021-cuni-systems,
    title = "{CUNI} Systems for {WMT}21: Terminology Translation Shared Task",
    author = "Jon, Josef  and
      Nov{\'a}k, Michal  and
      Aires, Jo{\~a}o Paulo  and
      Varis, Dusan  and
      Bojar, Ond{\v{r}}ej",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.82",
    pages = "828--834",
    abstract = "This paper describes Charles University sub-mission for Terminology translation Shared Task at WMT21. The objective of this task is to design a system which translates certain terms based on a provided terminology database, while preserving high overall translation quality. We competed in English-French language pair. Our approach is based on providing the desired translations alongside the input sentence and training the model to use these provided terms. We lemmatize the terms both during the training and inference, to allow the model to learn how to produce correct surface forms of the words, when they differ from the forms provided in the terminology database. Our submission ranked second in Exact Match metric which evaluates the ability of the model to produce desired terms in the translation.",
}
@inproceedings{wang-etal-2021-tencent-ai,
    title = "Tencent {AI} Lab Machine Translation Systems for the {WMT}21 Biomedical Translation Task",
    author = "Wang, Xing  and
      Tu, Zhaopeng  and
      Shi, Shuming",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.89",
    pages = "874--878",
    abstract = "This paper describes the Tencent AI Lab submission of the WMT2021 shared task on biomedical translation in eight language directions: English-German, English-French, English-Spanish and English-Russian. We utilized different Transformer architectures, pretraining and back-translation strategies to improve translation quality. Concretely, we explore mBART (Liu et al., 2020) to demonstrate the effectiveness of the pretraining strategy. Our submissions (Tencent AI Lab Machine Translation, TMT) in German/French/Spanish⇒English are ranked 1st respectively according to the official evaluation results in terms of BLEU scores.",
}
@inproceedings{chowdhury-etal-2021-ensemble,
    title = "Ensemble Fine-tuned m{BERT} for Translation Quality Estimation",
    author = "Chowdhury, Shaika  and
      Baili, Naouel  and
      Vannah, Brian",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.93",
    pages = "897--903",
    abstract = "Quality Estimation (QE) is an important component of the machine translation workflow as it assesses the quality of the translated output without consulting reference translations. In this paper, we discuss our submission to the WMT 2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that challenge participants to predict the HTER score for sentence-level post-editing effort. Our proposed system is an ensemble of multilingual BERT (mBERT)-based regression models, which are generated by fine-tuning on different input settings. It demonstrates comparable performance with respect to the Pearson{'}s correlation, and beat the baseline system in MAE/ RMSE for several language pairs. In addition, we adapt our system for the zero-shot setting by exploiting target language-relevant language pairs and pseudo-reference translations.",
}
@inproceedings{rubino-etal-2021-nict,
    title = "{NICT} {K}yoto Submission for the {WMT}{'}21 Quality Estimation Task: Multimetric Multilingual Pretraining for Critical Error Detection",
    author = "Rubino, Raphael  and
      Fujita, Atsushi  and
      Marie, Benjamin",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.99",
    pages = "941--947",
    abstract = "This paper presents the NICT Kyoto submission for the WMT{'}21 Quality Estimation (QE) Critical Error Detection shared task (Task 3). Our approach relies mainly on QE model pretraining for which we used 11 language pairs, three sentence-level and three word-level translation quality metrics. Starting from an XLM-R checkpoint, we perform continued training by modifying the learning objective, switching from masked language modeling to QE oriented signals, before finetuning and ensembling the models. Results obtained on the test set in terms of correlation coefficient and F-score show that automatic metrics and synthetic data perform well for pretraining, with our submissions ranked first for two out of four language pairs. A deeper look at the impact of each metric on the downstream task indicates higher performance for token oriented metrics, while an ablation study emphasizes the usefulness of conducting both self-supervised and QE pretraining.",
}
@inproceedings{yankovskaya-fishel-2021-direct,
    title = "Direct Exploitation of Attention Weights for Translation Quality Estimation",
    author = "Yankovskaya, Lisa  and
      Fishel, Mark",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.101",
    pages = "955--960",
    abstract = "The paper presents our submission to the WMT2021 Shared Task on Quality Estimation (QE). We participate in sentence-level predictions of human judgments and post-editing effort. We propose a glass-box approach based on attention weights extracted from machine translation systems. In contrast to the previous works, we directly explore attention weight matrices without replacing them with general metrics (like entropy). We show that some of our models can be trained with a small amount of a high-cost labelled data. In the absence of training data our approach still demonstrates a moderate linear correlation, when trained with synthetic data.",
}
@inproceedings{stefanik-etal-2021-regressive,
    title = "Regressive Ensemble for Machine Translation Quality Evaluation",
    author = "Stefanik, Michal  and
      Novotn{\'y}, V{\'\i}t  and
      Sojka, Petr",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.112",
    pages = "1041--1048",
    abstract = "This work introduces a simple regressive ensemble for evaluating machine translation quality based on a set of novel and established metrics. We evaluate the ensemble using a correlation to expert-based MQM scores of the WMT 2021 Metrics workshop. In both monolingual and zero-shot cross-lingual settings, we show a significant performance improvement over single metrics. In the cross-lingual settings, we also demonstrate that an ensemble approach is well-applicable to unseen languages. Furthermore, we identify a strong reference-free baseline that consistently outperforms the commonly-used BLEU and METEOR measures and significantly improves our ensemble{'}s performance.",
}
@inproceedings{hwang-etal-2021-contrastive,
    title = "Contrastive Learning for Context-aware Neural Machine Translation Using Coreference Information",
    author = "Hwang, Yongkeun  and
      Yun, Hyeongu  and
      Jung, Kyomin",
    editor = "Barrault, Loic  and
      Bojar, Ondrej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-jussa, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Freitag, Markus  and
      Graham, Yvette  and
      Grundkiewicz, Roman  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kocmi, Tom  and
      Martins, Andre  and
      Morishita, Makoto  and
      Monz, Christof",
    booktitle = "Proceedings of the Sixth Conference on Machine Translation",
    month = nov,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wmt-1.121",
    pages = "1135--1144",
    abstract = "Context-aware neural machine translation (NMT) incorporates contextual information of surrounding texts, that can improve the translation quality of document-level machine translation. Many existing works on context-aware NMT have focused on developing new model architectures for incorporating additional contexts and have shown some promising results. However, most of existing works rely on cross-entropy loss, resulting in limited use of contextual information. In this paper, we propose CorefCL, a novel data augmentation and contrastive learning scheme based on coreference between the source and contextual sentences. By corrupting automatically detected coreference mentions in the contextual sentence, CorefCL can train the model to be sensitive to coreference inconsistency. We experimented with our method on common context-aware NMT models and two document-level translation tasks. In the experiments, our method consistently improved BLEU of compared models on English-German and English-Korean tasks. We also show that our method significantly improves coreference resolution in the English-German contrastive test suite.",
}
@inproceedings{yamakoshi-etal-2021-evaluation,
    title = "Evaluation Scheme of Focal Translation for {J}apanese Partially Amended Statutes",
    author = "Yamakoshi, Takahiro  and
      Komamizu, Takahiro  and
      Ogawa, Yasuhiro  and
      Toyama, Katsuhiko",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.12",
    doi = "10.18653/v1/2021.wat-1.12",
    pages = "124--132",
    abstract = "For updating the translations of Japanese statutes based on their amendments, we need to consider the translation {``}focality;{''} that is, we should only modify expressions that are relevant to the amendment and retain the others to avoid misconstruing its contents. In this paper, we introduce an evaluation metric and a corpus to improve focality evaluations. Our metric is called an Inclusive Score for DIfferential Translation: (ISDIT). ISDIT consists of two factors: (1) the n-gram recall of expressions unaffected by the amendment and (2) the n-gram precision of the output compared to the reference. This metric supersedes an existing one for focality by simultaneously calculating the translation quality of the changed expressions in addition to that of the unchanged expressions. We also newly compile a corpus for Japanese partially amendment translation that secures the focality of the post-amendment translations, while an existing evaluation corpus does not. With the metric and the corpus, we examine the performance of existing translation methods for Japanese partially amendment translations.",
}
@inproceedings{dhar-etal-2021-optimal,
    title = "Optimal Word Segmentation for Neural Machine Translation into {D}ravidian Languages",
    author = "Dhar, Prajit  and
      Bisazza, Arianna  and
      van Noord, Gertjan",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.21",
    doi = "10.18653/v1/2021.wat-1.21",
    pages = "181--190",
    abstract = "Dravidian languages, such as Kannada and Tamil, are notoriously difficult to translate by state-of-the-art neural models. This stems from the fact that these languages are morphologically very rich as well as being low-resourced. In this paper, we focus on subword segmentation and evaluate Linguistically Motivated Vocabulary Reduction (LMVR) against the more commonly used SentencePiece (SP) for the task of translating from English into four different Dravidian languages. Additionally we investigate the optimal subword vocabulary size for each language. We find that SP is the overall best choice for segmentation, and that larger dictionary sizes lead to higher translation quality.",
}
@inproceedings{dabre-chakrabarty-2021-nict,
    title = "{NICT}-5{'}s Submission To {WAT} 2021: {MBART} Pre-training And In-Domain Fine Tuning For Indic Languages",
    author = "Dabre, Raj  and
      Chakrabarty, Abhisek",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.23",
    doi = "10.18653/v1/2021.wat-1.23",
    pages = "198--204",
    abstract = "In this paper we describe our submission to the multilingual Indic language translation wtask {``}MultiIndicMT{''} under the team name {``}NICT-5{''}. This task involves translation from 10 Indic languages into English and vice-versa. The objective of the task was to explore the utility of multilingual approaches using a variety of in-domain and out-of-domain parallel and monolingual corpora. Given the recent success of multilingual NMT pre-training we decided to explore pre-training an MBART model on a large monolingual corpus collection covering all languages in this task followed by multilingual fine-tuning on small in-domain corpora. Firstly, we observed that a small amount of pre-training followed by fine-tuning on small bilingual corpora can yield large gains over when pre-training is not used. Furthermore, multilingual fine-tuning leads to further gains in translation quality which significantly outperforms a very strong multilingual baseline that does not rely on any pre-training.",
}
@inproceedings{kumar-etal-2021-iiit,
    title = "{IIIT} Hyderabad Submission To {WAT} 2021: Efficient Multilingual {NMT} systems for {I}ndian languages",
    author = "Kumar, Sourav  and
      Aggarwal, Salil  and
      Sharma, Dipti",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.25",
    doi = "10.18653/v1/2021.wat-1.25",
    pages = "212--216",
    abstract = "This paper describes the work and the systems submitted by the IIIT-Hyderbad team in the WAT 2021 MultiIndicMT shared task. The task covers 10 major languages of the Indian subcontinent. For the scope of this task, we have built multilingual systems for 20 translation directions namely English-Indic (one-to- many) and Indic-English (many-to-one). Individually, Indian languages are resource poor which hampers translation quality but by leveraging multilingualism and abundant monolingual corpora, the translation quality can be substantially boosted. But the multilingual systems are highly complex in terms of time as well as computational resources. Therefore, we are training our systems by efficiently se- lecting data that will actually contribute to most of the learning process. Furthermore, we are also exploiting the language related- ness found in between Indian languages. All the comparisons were made using BLEU score and we found that our final multilingual sys- tem significantly outperforms the baselines by an average of 11.3 and 19.6 BLEU points for English-Indic (en-xx) and Indic-English (xx- en) directions, respectively.",
}
@inproceedings{dobrowolski-etal-2021-samsung,
    title = "{S}amsung {R}{\&}{D} Institute {P}oland submission to {WAT} 2021 Indic Language Multilingual Task",
    author = "Dobrowolski, Adam  and
      Szyma{\'n}ski, Marcin  and
      Chochowski, Marcin  and
      Przybysz, Pawe{\l}",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Goto, Isao  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Higashiyama, Shohei  and
      Manabe, Hiroshi  and
      Pa, Win Pa  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Chu, Chenhui  and
      Eriguchi, Akiko  and
      Abe, Kaori  and
      Oda, Yusuke  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 8th Workshop on Asian Translation (WAT2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.wat-1.27",
    doi = "10.18653/v1/2021.wat-1.27",
    pages = "224--232",
    abstract = "This paper describes the submission to the WAT 2021 Indic Language Multilingual Task by Samsung R{\&}D Institute Poland. The task covered translation between 10 Indic Languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Oriya, Punjabi, Tamil and Telugu) and English. We combined a variety of techniques: transliteration, filtering, backtranslation, domain adaptation, knowledge-distillation and finally ensembling of NMT models. We applied an effective approach to low-resource training that consist of pretraining on backtranslations and tuning on parallel corpora. We experimented with two different domain-adaptation techniques which significantly improved translation quality when applied to monolingual corpora. We researched and applied a novel approach for finding the best hyperparameters for ensembling a number of translation models. All techniques combined gave significant improvement - up to +8 BLEU over baseline results. The quality of the models has been confirmed by the human evaluation where SRPOL models scored best for all 5 manually evaluated languages.",
}
@inproceedings{khusainova-etal-2021-hierarchical,
    title = "Hierarchical Transformer for Multilingual Machine Translation",
    author = "Khusainova, Albina  and
      Khan, Adil  and
      Rivera, Ad{\'\i}n Ram{\'\i}rez  and
      Romanov, Vitaly",
    editor = {Zampieri, Marcos  and
      Nakov, Preslav  and
      Ljube{\v{s}}i{\'c}, Nikola  and
      Tiedemann, J{\"o}rg  and
      Scherrer, Yves  and
      Jauhiainen, Tommi},
    booktitle = "Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects",
    month = apr,
    year = "2021",
    address = "Kiyv, Ukraine",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.vardial-1.2",
    pages = "12--20",
    abstract = "The choice of parameter sharing strategy in multilingual machine translation models determines how optimally parameter space is used and hence, directly influences ultimate translation quality. Inspired by linguistic trees that show the degree of relatedness between different languages, the new general approach to parameter sharing in multilingual machine translation was suggested recently. The main idea is to use these expert language hierarchies as a basis for multilingual architecture: the closer two languages are, the more parameters they share. In this work, we test this idea using the Transformer architecture and show that despite the success in previous work there are problems inherent to training such hierarchical models. We demonstrate that in case of carefully chosen training strategy the hierarchical architecture can outperform bilingual models and multilingual models with full parameter sharing.",
}
@inproceedings{rivera-trigueros-olvera-lobo-2021-building,
    title = "Building a Corpus for Corporate Websites Machine Translation Evaluation. A Step by Step Methodological Approach",
    author = "Rivera-Trigueros, Irene  and
      Olvera-Lobo, Mar{\'\i}a-Dolores",
    editor = "Mitkov, Ruslan  and
      Sosoni, Vilelmini  and
      Gigu{\`e}re, Julie Christine  and
      Murgolo, Elena  and
      Deysel, Elizabeth",
    booktitle = "Proceedings of the Translation and Interpreting Technology Online Conference",
    month = jul,
    year = "2021",
    address = "Held Online",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.triton-1.11",
    pages = "93--101",
    abstract = "The aim of this paper is to describe the process carried out to develop a paral-lel corpus comprised of texts extracted from the corporate websites of south-ern Spanish SMEs from the sanitary sector which will serve as the basis for MT quality assessment. The stages for compiling the parallel corpora were: (i) selection of websites with content translated in English and Spanish, (ii) downloading of the HTML files of the selected websites, (iii) files filtering and pairing of English files with their Spanish equivalents, (iv) compilation of individual corpora (EN and ES) for each of the selected websites, (v) merging of the individual corpora into a two general corpus one in English and the other in Spanish, (vi) selection a representative sample of segments to be used as original (ES) and reference translations (EN), (vii) building of the parallel corpus intended for MT evaluation. The parallel corpus generated will serve to future Machine Translation quality assessment. In addition, the monolingual corpora generated during the process could as a base to carry out research focused on linguistic {--} bilingual or monolingual − analysis.",
}
@article{xu-carpuat-2021-editor,
    title = "{EDITOR}: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints",
    author = "Xu, Weijia  and
      Carpuat, Marine",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.19",
    doi = "10.1162/tacl_a_00368",
    pages = "311--328",
    abstract = "We introduce an Edit-Based TransfOrmer with Repositioning (EDITOR), which makes sequence generation flexible by seamlessly allowing users to specify preferences in output lexical choice. Building on recent models for non-autoregressive sequence generation (Gu et al., 2019), EDITOR generates new sequences by iteratively editing hypotheses. It relies on a novel reposition operation designed to disentangle lexical choice from word positioning decisions, while enabling efficient oracles for imitation learning and parallel edits at decoding time. Empirically, EDITOR uses soft lexical constraints more effectively than the Levenshtein Transformer (Gu et al., 2019) while speeding up decoding dramatically compared to constrained beam search (Post and Vilar, 2018). EDITOR also achieves comparable or better translation quality with faster decoding speed than the Levenshtein Transformer on standard Romanian-English, English-German, and English-Japanese machine translation tasks.",
}
@inproceedings{obamuyide-etal-2021-bayesian,
    title = "{B}ayesian Model-Agnostic Meta-Learning with Matrix-Valued Kernels for Quality Estimation",
    author = "Obamuyide, Abiola  and
      Fomicheva, Marina  and
      Specia, Lucia",
    editor = "Rogers, Anna  and
      Calixto, Iacer  and
      Vuli{\'c}, Ivan  and
      Saphra, Naomi  and
      Kassner, Nora  and
      Camburu, Oana-Maria  and
      Bansal, Trapit  and
      Shwartz, Vered",
    booktitle = "Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.repl4nlp-1.23",
    doi = "10.18653/v1/2021.repl4nlp-1.23",
    pages = "223--230",
    abstract = "Most current quality estimation (QE) models for machine translation are trained and evaluated in a fully supervised setting requiring significant quantities of labelled training data. However, obtaining labelled data can be both expensive and time-consuming. In addition, the test data that a deployed QE model would be exposed to may differ from its training data in significant ways. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. Thus, it is desirable to be able to adapt QE models efficiently to new user data with limited supervision data. To address these challenges, we propose a Bayesian meta-learning approach for adapting QE models to the needs and preferences of each user with limited supervision. To enhance performance, we further propose an extension to a state-of-the-art Bayesian meta-learning approach which utilizes a matrix-valued kernel for Bayesian meta-learning of quality estimation. Experiments on data with varying number of users and language characteristics demonstrates that the proposed Bayesian meta-learning approach delivers improved predictive performance in both limited and full supervision settings.",
}
@inproceedings{ala-etal-2021-domain,
    title = "Domain Adaptation for {H}indi-{T}elugu Machine Translation Using Domain Specific Back Translation",
    author = "Ala, Hema  and
      Mujadia, Vandan  and
      Sharma, Dipti",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)",
    month = sep,
    year = "2021",
    address = "Held Online",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.ranlp-1.4",
    pages = "26--34",
    abstract = "In this paper, we present a novel approachfor domain adaptation in Neural MachineTranslation which aims to improve thetranslation quality over a new domain. Adapting new domains is a highly challeng-ing task for Neural Machine Translation onlimited data, it becomes even more diffi-cult for technical domains such as Chem-istry and Artificial Intelligence due to spe-cific terminology, etc. We propose DomainSpecific Back Translation method whichuses available monolingual data and gen-erates synthetic data in a different way. This approach uses Out Of Domain words. The approach is very generic and can beapplied to any language pair for any domain. We conduct our experiments onChemistry and Artificial Intelligence do-mains for Hindi and Telugu in both direc-tions. It has been observed that the usageof synthetic data created by the proposedalgorithm improves the BLEU scores significantly.",
}
@inproceedings{etchegoyhen-etal-2021-online,
    title = "Online Learning over Time in Adaptive Neural Machine Translation",
    author = "Etchegoyhen, Thierry  and
      Ponce, David  and
      Gete, Harritxu  and
      Ruiz, Victor",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)",
    month = sep,
    year = "2021",
    address = "Held Online",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.ranlp-1.47",
    pages = "411--420",
    abstract = "Adaptive Machine Translation purports to dynamically include user feedback to improve translation quality. In a post-editing scenario, user corrections of machine translation output are thus continuously incorporated into translation models, reducing or eliminating repetitive error editing and increasing the usefulness of automated translation. In neural machine translation, this goal may be achieved via online learning approaches, where network parameters are updated based on each new sample. This type of adaptation typically requires higher learning rates, which can affect the quality of the models over time. Alternatively, less aggressive online learning setups may preserve model stability, at the cost of reduced adaptation to user-generated corrections. In this work, we evaluate different online learning configurations over time, measuring their impact on user-generated samples, as well as separate in-domain and out-of-domain datasets. Results in two different domains indicate that mixed approaches combining online learning with periodic batch fine-tuning might be needed to balance the benefits of online learning with model stability.",
}
@inproceedings{tars-etal-2021-extremely,
    title = "Extremely low-resource machine translation for closely related languages",
    author = {Tars, Maali  and
      T{\"a}ttar, Andre  and
      Fi{\v{s}}el, Mark},
    editor = "Dobnik, Simon  and
      {\O}vrelid, Lilja",
    booktitle = "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may # " 31--2 " # jun,
    year = "2021",
    address = "Reykjavik, Iceland (Online)",
    publisher = {Link{\"o}ping University Electronic Press, Sweden},
    url = "https://aclanthology.org/2021.nodalida-main.5",
    pages = "41--52",
    abstract = "An effective method to improve extremely low-resource neural machine translation is multilingual training, which can be improved by leveraging monolingual data to create synthetic bilingual corpora using the back-translation method. This work focuses on closely related languages from the Uralic language family: from Estonian and Finnish geographical regions. We find that multilingual learning and synthetic corpora increase the translation quality in every language pair for which we have data. We show that transfer learning and fine-tuning are very effective for doing low-resource machine translation and achieve the best results. We collected new parallel data for V{\~o}ro, North and South Saami and present first results of neural machine translation for these languages.",
}
@inproceedings{wang-etal-2021-exploring,
    title = "Exploring the Importance of Source Text in Automatic Post-Editing for Context-Aware Machine Translation",
    author = "Wang, Chaojun  and
      Hardmeier, Christian  and
      Sennrich, Rico",
    editor = "Dobnik, Simon  and
      {\O}vrelid, Lilja",
    booktitle = "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may # " 31--2 " # jun,
    year = "2021",
    address = "Reykjavik, Iceland (Online)",
    publisher = {Link{\"o}ping University Electronic Press, Sweden},
    url = "https://aclanthology.org/2021.nodalida-main.34",
    pages = "326--335",
    abstract = "Accurate translation requires document-level information, which is ignored by sentence-level machine translation. Recent work has demonstrated that document-level consistency can be improved with automatic post-editing (APE) using only target-language (TL) information. We study an extended APE model that additionally integrates source context. A human evaluation of fluency and adequacy in English{--}Russian translation reveals that the model with access to source context significantly outperforms monolingual APE in terms of adequacy, an effect largely ignored by automatic evaluation metrics. Our results show that TL-only modelling increases fluency without improving adequacy, demonstrating the need for conditioning on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably.",
}
@inproceedings{isbister-etal-2021-stop,
    title = "Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?",
    author = "Isbister, Tim  and
      Carlsson, Fredrik  and
      Sahlgren, Magnus",
    editor = "Dobnik, Simon  and
      {\O}vrelid, Lilja",
    booktitle = "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may # " 31--2 " # jun,
    year = "2021",
    address = "Reykjavik, Iceland (Online)",
    publisher = {Link{\"o}ping University Electronic Press, Sweden},
    url = "https://aclanthology.org/2021.nodalida-main.42",
    pages = "385--390",
    abstract = "Most work in NLP makes the assumption that it is desirable to develop solutions in the native language in question. There is consequently a strong trend towards building native language models even for low-resource languages. This paper questions this development, and explores the idea of simply translating the data into English, thereby enabling the use of pretrained, and large-scale, English language models. We demonstrate empirically that a large English language model coupled with modern machine translation outperforms native language models in most Scandinavian languages. The exception to this is Finnish, which we assume is due to inferior translation quality. Our results suggest that machine translation is a mature technology, which raises a serious counter-argument for training native language models for low-resource languages. This paper therefore strives to make a provocative but important point. As English language models are improving at an unprecedented pace, which in turn improves machine translation, it is from an empirical and environmental stand-point more effective to translate data from low-resource languages into English, than to build language models for such languages.",
}
@inproceedings{kondo-etal-2021-sentence,
    title = "Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation",
    author = "Kondo, Seiichiro  and
      Hotate, Kengo  and
      Hirasawa, Tosho  and
      Kaneko, Masahiro  and
      Komachi, Mamoru",
    editor = "Durmus, Esin  and
      Gupta, Vivek  and
      Liu, Nelson  and
      Peng, Nanyun  and
      Su, Yu",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-srw.18",
    doi = "10.18653/v1/2021.naacl-srw.18",
    pages = "143--149",
    abstract = "Recently, neural machine translation is widely used for its high translation accuracy, but it is also known to show poor performance at long sentence translation. Besides, this tendency appears prominently for low resource languages. We assume that these problems are caused by long sentences being few in the train data. Therefore, we propose a data augmentation method for handling long sentences. Our method is simple; we only use given parallel corpora as train data and generate long sentences by concatenating two sentences. Based on our experiments, we confirm improvements in long sentence translation by proposed data augmentation despite the simplicity. Moreover, the proposed method improves translation quality more when combined with back-translation.",
}
@inproceedings{xu-etal-2021-probing,
    title = "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
    author = "Xu, Hongfei  and
      van Genabith, Josef  and
      Liu, Qiuhui  and
      Xiong, Deyi",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.7",
    doi = "10.18653/v1/2021.naacl-main.7",
    pages = "74--85",
    abstract = "Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4.",
}
@inproceedings{ai-fang-2021-almost,
    title = "Almost Free Semantic Draft for Neural Machine Translation",
    author = "Ai, Xi  and
      Fang, Bin",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.307",
    doi = "10.18653/v1/2021.naacl-main.307",
    pages = "3931--3941",
    abstract = "Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub.",
}
@inproceedings{bao-etal-2021-non,
    title = "Non-Autoregressive Translation by Learning Target Categorical Codes",
    author = "Bao, Yu  and
      Huang, Shujian  and
      Xiao, Tong  and
      Wang, Dongqi  and
      Dai, Xinyu  and
      Chen, Jiajun",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.458",
    doi = "10.18653/v1/2021.naacl-main.458",
    pages = "5749--5759",
    abstract = "Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines.",
}
@inproceedings{wang-etal-2021-cross,
    title = "Cross-lingual Supervision Improves Unsupervised Neural Machine Translation",
    author = "Wang, Mingxuan  and
      Bai, Hongxiao  and
      Zhao, Hai  and
      Li, Lei",
    editor = "Kim, Young-bum  and
      Li, Yunyao  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-industry.12",
    doi = "10.18653/v1/2021.naacl-industry.12",
    pages = "89--96",
    abstract = "We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. {\%} is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT{'}14 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT{'}16 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline.",
}
@inproceedings{wang-etal-2021-autocorrect,
    title = "Autocorrect in the Process of Translation {---} Multi-task Learning Improves Dialogue Machine Translation",
    author = "Wang, Tao  and
      Zhao, Chengqi  and
      Wang, Mingxuan  and
      Li, Lei  and
      Xiong, Deyi",
    editor = "Kim, Young-bum  and
      Li, Yunyao  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-industry.14",
    doi = "10.18653/v1/2021.naacl-industry.14",
    pages = "105--112",
    abstract = "Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09{\%} to 47.16{\%}. We will publish the code and dataset publicly at https://xxx.xx.",
}
@inproceedings{alva-manchego-etal-2021-validating,
    title = "Validating Quality Estimation in a Computer-Aided Translation Workflow: Speed, Cost and Quality Trade-off",
    author = "Alva-Manchego, Fernando  and
      Specia, Lucia  and
      Szoc, Sara  and
      Vanallemeersch, Tom  and
      Depraetere, Heidi",
    editor = "Campbell, Janice  and
      Huyck, Ben  and
      Larocca, Stephen  and
      Marciano, Jay  and
      Savenkov, Konstantin  and
      Yanishevsky, Alex",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Users and Providers Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-up.22",
    pages = "306--315",
    abstract = "In modern computer-aided translation workflows, Machine Translation (MT) systems are used to produce a draft that is then checked and edited where needed by human translators. In this scenario, a Quality Estimation (QE) tool can be used to score MT outputs, and a threshold on the QE scores can be applied to decide whether an MT output can be used as-is or requires human post-edition. While this could reduce cost and turnaround times, it could harm translation quality, as QE models are not 100{\%} accurate. In the framework of the APE-QUEST project (Automated Post-Editing and Quality Estimation), we set up a case-study on the trade-off between speed, cost and quality, investigating the benefits of QE models in a real-world scenario, where we rely on end-user acceptability as quality metric. Using data in the public administration domain for English-Dutch and English-French, we experimented with two use cases: assimilation and dissemination. Results shed some light on how QE scores can be explored to establish thresholds that suit each use case and target language, and demonstrate the potential benefits of adding QE to a translation workflow.",
}
@inproceedings{banerjee-etal-2021-crosslingual,
    title = "Crosslingual Embeddings are Essential in {UNMT} for distant languages: An {E}nglish to {I}ndo{A}ryan Case Study",
    author = "Banerjee, Tamali  and
      V Murthy, Rudra  and
      Bhattacharya, Pushpak",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.3",
    pages = "23--34",
    abstract = "Recent advances in Unsupervised Neural Machine Translation (UNMT) has minimized the gap between supervised and unsupervised machine translation performance for closely related language-pairs. However and the situation is very different for distant language pairs. Lack of overlap in lexicon and low syntactic similarity such as between English and IndoAryan languages leads to poor translation quality in existing UNMT systems. In this paper and we show that initialising the embedding layer of UNMT models with cross-lingual embeddings leads to significant BLEU score improvements over existing UNMT models where the embedding layer weights are randomly initialized. Further and freezing the embedding layer weights leads to better gains compared to updating the embedding layer weights during training. We experimented using Masked Sequence to Sequence (MASS) and Denoising Autoencoder (DAE) UNMT approaches for three distant language pairs. The proposed cross-lingual embedding initialization yields BLEU score improvement of as much as ten times over the baseline for English-Hindi and English-Bengali and English-Gujarati. Our analysis shows that initialising embedding layer with static cross-lingual embedding mapping is essential for training of UNMT models for distant language-pairs.",
}
@inproceedings{larkin-etal-2021-like,
    title = "Like Chalk and Cheese? On the Effects of Translationese in {MT} Training",
    author = "Larkin, Samuel  and
      Simard, Michel  and
      Knowles, Rebecca",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.9",
    pages = "103--113",
    abstract = "We revisit the topic of translation direction in the data used for training neural machine translation systems and focusing on a real-world scenario with known translation direction and imbalances in translation direction: the Canadian Hansard. According to automatic metrics and we observe that using parallel data that was produced in the {``}matching{''} translation direction (Authentic source and translationese target) improves translation quality. In cases of data imbalance in terms of translation direction and we find that tagging of translation direction can close the performance gap. We perform a human evaluation that differs slightly from the automatic metrics and but nevertheless confirms that for this French-English dataset that is known to contain high-quality translations and authentic or tagged mixed source improves over translationese source for training.",
}
@inproceedings{dabre-fujita-2021-investigating,
    title = "Investigating Softmax Tempering for Training Neural Machine Translation Models",
    author = "Dabre, Raj  and
      Fujita, Atsushi",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.10",
    pages = "114--126",
    abstract = "Neural machine translation (NMT) models are typically trained using a softmax cross-entropy loss where the softmax distribution is compared against the gold labels. In low-resource scenarios and NMT models tend to perform poorly because the model training quickly converges to a point where the softmax distribution computed using logits approaches the gold label distribution. Although label smoothing is a well-known solution to address this issue and we further propose to divide the logits by a temperature coefficient greater than one and forcing the softmax distribution to be smoother during training. This makes it harder for the model to quickly over-fit. In our experiments on 11 language pairs in the low-resource Asian Language Treebank dataset and we observed significant improvements in translation quality. Our analysis focuses on finding the right balance of label smoothing and softmax tempering which indicates that they are orthogonal methods. Finally and a study of softmax entropies and gradients reveal the impact of our method on the internal behavior of our NMT models.",
}
@inproceedings{dabre-etal-2021-studying,
    title = "Studying The Impact Of Document-level Context On Simultaneous Neural Machine Translation",
    author = "Dabre, Raj  and
      Imankulova, Aizhan  and
      Kaneko, Masahiro",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.17",
    pages = "202--214",
    abstract = "In a real-time simultaneous translation setting and neural machine translation (NMT) models start generating target language tokens from incomplete source language sentences and making them harder to translate and leading to poor translation quality. Previous research has shown that document-level NMT and comprising of sentence and context encoders and a decoder and leverages context from neighboring sentences and helps improve translation quality. In simultaneous translation settings and the context from previous sentences should be even more critical. To this end and in this paper and we propose wait-k simultaneous document-level NMT where we keep the context encoder as it is and replace the source sentence encoder and target language decoder with their wait-k equivalents. We experiment with low and high resource settings using the ALT and OpenSubtitles2018 corpora and where we observe minor improvements in translation quality. We then perform an analysis of the translations obtained using our models by focusing on sentences that should benefit from the context where we found out that the model does and in fact and benefit from context but is unable to effectively leverage it and especially in a low-resource setting. This shows that there is a need for further innovation in the way useful context is identified and leveraged.",
}
@inproceedings{nowakowski-jassem-2021-neural,
    title = "Neural Machine Translation with Inflected Lexicon",
    author = "Nowakowski, Artur  and
      Jassem, Krzysztof",
    editor = "Duh, Kevin  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of Machine Translation Summit XVIII: Research Track",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-research.23",
    pages = "282--292",
    abstract = "The paper presents experiments in neural machine translation with lexical constraints into a morphologically rich language. In particular and we introduce a method and based on constrained decoding and which handles the inflected forms of lexical entries and does not require any modification to the training data or model architecture. To evaluate its effectiveness and we carry out experiments in two different scenarios: general and domain-specific. We compare our method with baseline translation and i.e. translation without lexical constraints and in terms of translation speed and translation quality. To evaluate how well the method handles the constraints and we propose new evaluation metrics which take into account the presence and placement and duplication and inflectional correctness of lexical terms in the output sentence.",
}
@inproceedings{chen-fazio-2021-morphologically,
    title = "Morphologically-Guided Segmentation For Translation of Agglutinative Low-Resource Languages",
    author = "Chen, William  and
      Fazio, Brett",
    editor = "Ortega, John  and
      Ojha, Atul Kr.  and
      Kann, Katharina  and
      Liu, Chao-Hong",
    booktitle = "Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-loresmt.3",
    pages = "20--31",
    abstract = "Neural Machine Translation (NMT) for Low Resource Languages (LRL) is often limited by the lack of available training data, making it necessary to explore additional techniques to improve translation quality. We propose the use of the Prefix-Root-Postfix-Encoding (PRPE) subword segmentation algorithm to improve translation quality for LRLs, using two agglutinative languages as case studies: Quechua and Indonesian. During the course of our experiments, we reintroduce a parallel corpus for Quechua-Spanish translation that was previously unavailable for NMT. Our experiments show the importance of appropriate subword segmentation, which can go as far as improving translation quality over systems trained on much larger quantities of data. We show this by achieving state-of-the-art results for both languages, obtaining higher BLEU scores than large pre-trained models with much smaller amounts of data.",
}
@inproceedings{zhang-duh-2021-approaching,
    title = "Approaching Sign Language Gloss Translation as a Low-Resource Machine Translation Task",
    author = "Zhang, Xuan  and
      Duh, Kevin",
    editor = "Shterionov, Dimitar",
    booktitle = "Proceedings of the 1st International Workshop on Automatic Translation for Signed and Spoken Languages (AT4SSL)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-at4ssl.7",
    pages = "60--70",
    abstract = "A cascaded Sign Language Translation system first maps sign videos to gloss annotations and then translates glosses into a spoken languages. This work focuses on the second-stage gloss translation component, which is challenging due to the scarcity of publicly available parallel data. We approach gloss translation as a low-resource machine translation task and investigate two popular methods for improving translation quality: hyperparameter search and backtranslation. We discuss the potentials and pitfalls of these methods based on experiments on the RWTH-PHOENIX-Weather 2014T dataset.",
}
@inproceedings{karakanta-etal-2021-simultaneous,
    title = "Simultaneous Speech Translation for Live Subtitling: from Delay to Display",
    author = "Karakanta, Alina  and
      Papi, Sara  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Turchi, Marco  and
      Fantinuoli, Claudio",
    booktitle = "Proceedings of the 1st Workshop on Automatic Spoken Language Translation in Real-World Settings (ASLTRW)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-asltrw.4",
    pages = "35--48",
    abstract = "With the increased audiovisualisation of communication, the need for live subtitles in multilingual events is more relevant than ever. In an attempt to automatise the process, we aim at exploring the feasibility of simultaneous speech translation (SimulST) for live subtitling. However, the word-for-word rate of generation of SimulST systems is not optimal for displaying the subtitles in a comprehensible and readable way. In this work, we adapt SimulST systems to predict subtitle breaks along with the translation. We then propose a display mode that exploits the predicted break structure by presenting the subtitles in scrolling lines. We compare our proposed mode with a display 1) word-for-word and 2) in blocks, in terms of reading speed and delay. Experiments on three language pairs (en→it, de, fr) show that scrolling lines is the only mode achieving an acceptable reading speed while keeping delay close to a 4-second threshold. We argue that simultaneous translation for readable live subtitles still faces challenges, the main one being poor translation quality, and propose directions for steering future research.",
}
@inproceedings{han-etal-2021-translation,
    title = "Translation Quality Assessment: A Brief Survey on Manual and Automatic Methods",
    author = "Han, Lifeng  and
      Smeaton, Alan  and
      Jones, Gareth",
    editor = "Bizzoni, Yuri  and
      Teich, Elke  and
      Espa{\~n}a-Bonet, Cristina  and
      van Genabith, Josef",
    booktitle = "Proceedings for the First Workshop on Modelling Translation: Translatology in the Digital Age",
    month = may,
    year = "2021",
    address = "online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.motra-1.3",
    pages = "15--33",
}
@inproceedings{carpuat-2021-models,
    title = "Models and Tasks for Human-Centered Machine Translation",
    author = "Carpuat, Marine",
    editor = "Doren Singh, Thoudam  and
      Espa{\~n}a i Bonet, Cristina  and
      Bandyopadhyay, Sivaji  and
      van Genabith, Josef",
    booktitle = "Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021)",
    month = sep,
    year = "2021",
    address = "Online (Virtual Mode)",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.mmtlrl-1.1",
    pages = "1",
    abstract = "In this talk, I will describe current research directions in my group that aim to make machine translation (MT) more human-centered. Instead of viewing MT solely as a task that aims to transduce a source sentence into a well-formed target language equivalent, we revisit all steps of the MT research and development lifecycle with the goal of designing MT systems that are able to help people communicate across language barriers. I will present methods to better characterize the parallel training data that powers MT systems, and how the degree of equivalence impacts translation quality. I will introduce models that enable flexible conditional language generation, and will discuss recent work on framing machine translation tasks and evaluation to center human factors.",
}
@inproceedings{singh-etal-2021-multiple,
    title = "Multiple Captions Embellished Multilingual Multi-Modal Neural Machine Translation",
    author = "Singh, Salam Michael  and
      Sanayai Meetei, Loitongbam  and
      Singh, Thoudam Doren  and
      Bandyopadhyay, Sivaji",
    editor = "Doren Singh, Thoudam  and
      Espa{\~n}a i Bonet, Cristina  and
      Bandyopadhyay, Sivaji  and
      van Genabith, Josef",
    booktitle = "Proceedings of the First Workshop on Multimodal Machine Translation for Low Resource Languages (MMTLRL 2021)",
    month = sep,
    year = "2021",
    address = "Online (Virtual Mode)",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.mmtlrl-1.2",
    pages = "2--11",
    abstract = "Neural machine translation based on bilingual text with limited training data suffers from lexical diversity, which lowers the rare word translation accuracy and reduces the generalizability of the translation system. In this work, we utilise the multiple captions from the Multi-30K dataset to increase the lexical diversity aided with the cross-lingual transfer of information among the languages in a multilingual setup. In this multilingual and multimodal setting, the inclusion of the visual features boosts the translation quality by a significant margin. Empirical study affirms that our proposed multimodal approach achieves substantial gain in terms of the automatic score and shows robustness in handling the rare word translation in the pretext of English to/from Hindi and Telugu translation tasks.",
}
@inproceedings{wang-etal-2021-hi,
    title = "{HI}-{CMLM}: Improve {CMLM} with Hybrid Decoder Input",
    author = "Wang, Minghan  and
      Jiaxin, Guo  and
      Wang, Yuxia  and
      Chen, Yimeng  and
      Chang, Su  and
      Wei, Daimeng  and
      Zhang, Min  and
      Tao, Shimin  and
      Yang, Hao",
    editor = "Belz, Anya  and
      Fan, Angela  and
      Reiter, Ehud  and
      Sripada, Yaji",
    booktitle = "Proceedings of the 14th International Conference on Natural Language Generation",
    month = aug,
    year = "2021",
    address = "Aberdeen, Scotland, UK",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.inlg-1.16",
    pages = "167--171",
    abstract = "Mask-predict CMLM (Ghazvininejad et al.,2019) has achieved stunning performance among non-autoregressive NMT models, but we find that the mechanism of predicting all of the target words only depending on the hidden state of [MASK] is not effective and efficient in initial iterations of refinement, resulting in ungrammatical repetitions and slow convergence. In this work, we mitigate this problem by combining copied source with embeddings of [MASK] in decoder. Notably. it{'}s not a straightforward copying that is shown to be useless, but a novel heuristic hybrid strategy {---} fence-mask. Experimental results show that it gains consistent boosts on both WMT14 En{\textless}-{\textgreater}De and WMT16 En{\textless}-{\textgreater}Ro corpus by 0.5 BLEU on average, and 1 BLEU for less-informative short sentences. This reveals that incorporating additional information by proper strategies is beneficial to improve CMLM, particularly translation quality of short texts and speeding up early-stage convergence.",
}
@inproceedings{takai-etal-2021-named,
    title = "Named Entity-Factored Transformer for Proper Noun Translation",
    author = "Takai, Kohichi  and
      Hattori, Gen  and
      Yoneyama, Akio  and
      Yasuda, Keiji  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Bandyopadhyay, Sivaji  and
      Devi, Sobha Lalitha  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 18th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2021",
    address = "National Institute of Technology Silchar, Silchar, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2021.icon-main.2",
    pages = "7--11",
    abstract = "Subword-based neural machine translation decreases the number of out-of-vocabulary (OOV) words and also keeps the translation quality if input sentences include OOV words. The subword-based NMT decomposes a word into shorter units to solve the OOV problem, but it does not work well for non-compositional proper nouns due to the construction of the shorter unit from words. Furthermore, the lack of translation also occurs in proper noun translation. The proposed method applies the Named Entity (NE) fea-ture vector to Factored Transformer for accurate proper noun translation. The proposed method uses two features which are input sentences in subwords unit and the feature obtained from Named Entity Recognition (NER). The pro-posed method improves the problem of non-compositional proper nouns translation included a low-frequency word. According to the experiments, the proposed method using the best NE feature vector outperformed the baseline sub-word-based transformer model by more than 9.6 points in proper noun accuracy and 2.5 points in the BLEU score.",
}
@inproceedings{escolano-etal-2021-multi,
    title = "Multi-Task Learning for Improving Gender Accuracy in Neural Machine Translation",
    author = "Escolano, Carlos  and
      Ojeda, Graciela  and
      Basta, Christine  and
      Costa-jussa, Marta R.",
    editor = "Bandyopadhyay, Sivaji  and
      Devi, Sobha Lalitha  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 18th International Conference on Natural Language Processing (ICON)",
    month = dec,
    year = "2021",
    address = "National Institute of Technology Silchar, Silchar, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2021.icon-main.3",
    pages = "12--17",
    abstract = "Machine Translation is highly impacted by social biases present in data sets, indicating that it reflects and amplifies stereotypes. In this work, we study mitigating gender bias by jointly learning the translation, the part-of-speech, and the gender of the target language with different morphological complexity. This approach has shown improvements up to 6.8 points in gender accuracy without significantly impacting the translation quality.",
}
@inproceedings{iranzo-sanchez-etal-2021-stream-level,
    title = "Stream-level Latency Evaluation for Simultaneous Machine Translation",
    author = "Iranzo-S{\'a}nchez, Javier  and
      Civera Saiz, Jorge  and
      Juan, Alfons",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.58",
    doi = "10.18653/v1/2021.findings-emnlp.58",
    pages = "664--670",
    abstract = "Simultaneous machine translation has recently gained traction thanks to significant quality improvements and the advent of streaming applications. Simultaneous translation systems need to find a trade-off between translation quality and response time, and with this purpose multiple latency measures have been proposed. However, latency evaluations for simultaneous translation are estimated at the sentence level, not taking into account the sequential nature of a streaming scenario. Indeed, these sentence-level latency measures are not well suited for continuous stream translation, resulting in figures that are not coherent with the simultaneous translation policy of the system being assessed. This work proposes a stream level adaptation of the current latency measures based on a re-segmentation approach applied to the output translation, that is successfully evaluated on streaming conditions for a reference IWSLT task.",
}
@inproceedings{xu-etal-2021-learning-hard-retrieval,
    title = "Learning Hard Retrieval Decoder Attention for Transformers",
    author = "Xu, Hongfei  and
      Liu, Qiuhui  and
      van Genabith, Josef  and
      Xiong, Deyi",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.67",
    doi = "10.18653/v1/2021.findings-emnlp.67",
    pages = "779--785",
    abstract = "The Transformer translation model is based on the multi-head attention mechanism, which can be parallelized easily. The multi-head attention network performs the scaled dot-product attention function in parallel, empowering the model by jointly attending to information from different representation subspaces at different positions. In this paper, we present an approach to learning a hard retrieval attention where an attention head only attends to one token in the sentence rather than all tokens. The matrix multiplication between attention probabilities and the value sequence in the standard scaled dot-product attention can thus be replaced by a simple and efficient retrieval operation. We show that our hard retrieval attention mechanism is 1.43 times faster in decoding, while preserving translation quality on a wide range of machine translation tasks when used in the decoder self- and cross-attention networks.",
}
@inproceedings{liu-etal-2021-complementarity-pre,
    title = "On the Complementarity between Pre-Training and Back-Translation for Neural Machine Translation",
    author = "Liu, Xuebo  and
      Wang, Longyue  and
      Wong, Derek F.  and
      Ding, Liang  and
      Chao, Lidia S.  and
      Shi, Shuming  and
      Tu, Zhaopeng",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.247",
    doi = "10.18653/v1/2021.findings-emnlp.247",
    pages = "2900--2907",
    abstract = "Pre-training (PT) and back-translation (BT) are two simple and powerful methods to utilize monolingual data for improving the model performance of neural machine translation (NMT). This paper takes the first step to investigate the complementarity between PT and BT. We introduce two probing tasks for PT and BT respectively and find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder. Experimental results show that PT and BT are nicely complementary to each other, establishing state-of-the-art performances on the WMT16 English-Romanian and English-Russian benchmarks. Through extensive analyses on sentence originality and word frequency, we also demonstrate that combining Tagged BT with PT is more helpful to their complementarity, leading to better translation quality. Source code is freely available at \url{https://github.com/SunbowLiu/PTvsBT}.",
}
@inproceedings{xu-etal-2021-improving-multilingual,
    title = "Improving Multilingual Neural Machine Translation with Auxiliary Source Languages",
    author = "Xu, Weijia  and
      Yin, Yuwei  and
      Ma, Shuming  and
      Zhang, Dongdong  and
      Huang, Haoyang",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.260",
    doi = "10.18653/v1/2021.findings-emnlp.260",
    pages = "3029--3041",
    abstract = "Multilingual neural machine translation models typically handle one source language at a time. However, prior work has shown that translating from multiple source languages improves translation quality. Different from existing approaches on multi-source translation that are limited to the test scenario where parallel source sentences from multiple languages are available at inference time, we propose to improve multilingual translation in a more common scenario by exploiting synthetic source sentences from auxiliary languages. We train our model on synthetic multi-source corpora and apply random masking to enable flexible inference with single-source or bi-source inputs. Extensive experiments on Chinese/English-Japanese and a large-scale multilingual translation benchmark show that our model outperforms the multilingual baseline significantly by up to +4.0 BLEU with the largest improvements on low-resource or distant language pairs.",
}
@inproceedings{li-etal-2021-visual-cues,
    title = "Visual Cues and Error Correction for Translation Robustness",
    author = "Li, Zhenhao  and
      Rei, Marek  and
      Specia, Lucia",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.271",
    doi = "10.18653/v1/2021.findings-emnlp.271",
    pages = "3153--3168",
    abstract = "Neural Machine Translation models are sensitive to noise in the input texts, such as misspelled words and ungrammatical constructions. Existing robustness techniques generally fail when faced with unseen types of noise and their performance degrades on clean texts. In this paper, we focus on three types of realistic noise that are commonly generated by humans and introduce the idea of visual context to improve translation robustness for noisy texts. In addition, we describe a novel error correction training regime that can be used as an auxiliary task to further improve translation robustness. Experiments on English-French and English-German translation show that both multimodal and error correction components improve model robustness to noisy texts, while still retaining translation quality on clean texts.",
}
@inproceedings{parthasarathi-etal-2021-sometimes-want,
    title = "Sometimes We Want Ungrammatical Translations",
    author = "Parthasarathi, Prasanna  and
      Sinha, Koustuv  and
      Pineau, Joelle  and
      Williams, Adina",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.275",
    doi = "10.18653/v1/2021.findings-emnlp.275",
    pages = "3205--3227",
    abstract = "Rapid progress in Neural Machine Translation (NMT) systems over the last few years has focused primarily on improving translation quality, and as a secondary focus, improving robustness to perturbations (e.g. spelling). While performance and robustness are important objectives, by over-focusing on these, we risk overlooking other important properties. In this paper, we draw attention to the fact that for some applications, faithfulness to the original (input) text is important to preserve, even if it means introducing unusual language patterns in the (output) translation. We propose a simple, novel way to quantify whether an NMT system exhibits robustness or faithfulness, by focusing on the case of word-order perturbations. We explore a suite of functions to perturb the word order of source sentences without deleting or injecting tokens, and measure their effects on the target side. Across several experimental conditions, we observe a strong tendency towards robustness rather than faithfulness. These results allow us to better understand the trade-off between faithfulness and robustness in NMT, and opens up the possibility of developing systems where users have more autonomy and control in selecting which property is best suited for their use case.",
}
@inproceedings{glushkova-etal-2021-uncertainty-aware,
    title = "Uncertainty-Aware Machine Translation Evaluation",
    author = "Glushkova, Taisiya  and
      Zerva, Chrysoula  and
      Rei, Ricardo  and
      Martins, Andr{\'e} F. T.",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.330",
    doi = "10.18653/v1/2021.findings-emnlp.330",
    pages = "3920--3938",
    abstract = "Several neural-based metrics have been recently proposed to evaluate machine translation quality. However, all of them resort to point estimates, which provide limited information at segment level. This is made worse as they are trained on noisy, biased and scarce human judgements, often resulting in unreliable quality predictions. In this paper, we introduce uncertainty-aware MT evaluation and analyze the trustworthiness of the predicted quality. We combine the COMET framework with two uncertainty estimation methods, Monte Carlo dropout and deep ensembles, to obtain quality scores along with confidence intervals. We compare the performance of our uncertainty-aware MT evaluation methods across multiple language pairs from the QT21 dataset and the WMT20 metrics task, augmented with MQM annotations. We experiment with varying numbers of references and further discuss the usefulness of uncertainty-aware quality estimation (without references) to flag possibly critical translation mistakes.",
}
@inproceedings{wang-etal-2021-beyond-glass,
    title = "Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation",
    author = "Wang, Ke  and
      Shi, Yangbin  and
      Wang, Jiayi  and
      Zhang, Yuqi  and
      Zhao, Yu  and
      Zheng, Xiaolin",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.401",
    doi = "10.18653/v1/2021.findings-emnlp.401",
    pages = "4687--4698",
    abstract = "Quality Estimation (QE) plays an essential role in applications of Machine Translation (MT). Traditionally, a QE system accepts the original source text and translation from a black-box MT system as input. Recently, a few studies indicate that as a by-product of translation, QE benefits from the model and training data{'}s information of the MT system where the translations come from, and it is called the {``}glass-box QE{''}. In this paper, we extend the definition of {``}glass-box QE{''} generally to uncertainty quantification with both {``}black-box{''} and {``}glass-box{''} approaches and design several features deduced from them to blaze a new trial in improving QE{'}s performance. We propose a framework to fuse the feature engineering of uncertainty quantification into a pre-trained cross-lingual language model to predict the translation quality. Experiment results show that our method achieves state-of-the-art performances on the datasets of WMT 2020 QE shared task.",
}
@inproceedings{fomicheva-etal-2021-eval4nlp,
    title = "The {E}val4{NLP} Shared Task on Explainable Quality Estimation: Overview and Results",
    author = "Fomicheva, Marina  and
      Lertvittayakumjorn, Piyawat  and
      Zhao, Wei  and
      Eger, Steffen  and
      Gao, Yang",
    editor = "Gao, Yang  and
      Eger, Steffen  and
      Zhao, Wei  and
      Lertvittayakumjorn, Piyawat  and
      Fomicheva, Marina",
    booktitle = "Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eval4nlp-1.17",
    doi = "10.18653/v1/2021.eval4nlp-1.17",
    pages = "165--178",
    abstract = "In this paper, we introduce the Eval4NLP-2021 shared task on explainable quality estimation. Given a source-translation pair, this shared task requires not only to provide a sentence-level score indicating the overall quality of the translation, but also to explain this score by identifying the words that negatively impact translation quality. We present the data, annotation guidelines and evaluation setup of the shared task, describe the six participating systems, and analyze the results. To the best of our knowledge, this is the first shared task on explainable NLP evaluation metrics. Datasets and results are available at \url{https://github.com/eval4nlp/SharedTask2021}.",
}
@inproceedings{ramnath-etal-2021-hintedbt,
    title = "{H}inted{BT}: {A}ugmenting {B}ack-{T}ranslation with Quality and Transliteration Hints",
    author = "Ramnath, Sahana  and
      Johnson, Melvin  and
      Gupta, Abhirut  and
      Raghuveer, Aravindan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.129",
    doi = "10.18653/v1/2021.emnlp-main.129",
    pages = "1717--1733",
    abstract = "Back-translation (BT) of target monolingual corpora is a widely used data augmentation strategy for neural machine translation (NMT), especially for low-resource language pairs. To improve effectiveness of the available BT data, we introduce HintedBT{---}a family of techniques which provides hints (through tags) to the encoder and decoder. First, we propose a novel method of using both high and low quality BT data by providing hints (as source tags on the encoder) to the model about the quality of each source-target pair. We don{'}t filter out low quality data but instead show that these hints enable the model to learn effectively from noisy data. Second, we address the problem of predicting whether a source token needs to be translated or transliterated to the target language, which is common in cross-script translation tasks (i.e., where source and target do not share the written script). For such cases, we propose training the model with additional hints (as target tags on the decoder) that provide information about the operation required on the source (translation or both translation and transliteration). We conduct experiments and detailed analyses on standard WMT benchmarks for three cross-script low/medium-resource language pairs: Hindi,Gujarati,Tamil-to-English. Our methods compare favorably with five strong and well established baselines. We show that using these hints, both separately and together, significantly improves translation quality and leads to state-of-the-art performance in all three language pairs in corresponding bilingual settings.",
}
@inproceedings{alinejad-etal-2021-translation,
    title = "Translation-based Supervision for Policy Generation in Simultaneous Neural Machine Translation",
    author = "Alinejad, Ashkan  and
      Shavarani, Hassan S.  and
      Sarkar, Anoop",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.130",
    doi = "10.18653/v1/2021.emnlp-main.130",
    pages = "1734--1744",
    abstract = "In simultaneous machine translation, finding an agent with the optimal action sequence of reads and writes that maintain a high level of translation quality while minimizing the average lag in producing target tokens remains an extremely challenging problem. We propose a novel supervised learning approach for training an agent that can detect the minimum number of reads required for generating each target token by comparing simultaneous translations against full-sentence translations during training to generate oracle action sequences. These oracle sequences can then be used to train a supervised model for action generation at inference time. Our approach provides an alternative to current heuristic methods in simultaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation.",
}
@inproceedings{schioppa-etal-2021-controlling,
    title = "Controlling Machine Translation for Multiple Attributes with Additive Interventions",
    author = "Schioppa, Andrea  and
      Vilar, David  and
      Sokolov, Artem  and
      Filippova, Katja",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.535",
    doi = "10.18653/v1/2021.emnlp-main.535",
    pages = "6676--6696",
    abstract = "Fine-grained control of machine translation (MT) outputs along multiple attributes is critical for many modern MT applications and is a requirement for gaining users{'} trust. A standard approach for exerting control in MT is to prepend the input with a special tag to signal the desired output attribute. Despite its simplicity, attribute tagging has several drawbacks: continuous values must be binned into discrete categories, which is unnatural for certain applications; interference between multiple tags is poorly understood. We address these problems by introducing vector-valued interventions which allow for fine-grained control over multiple attributes simultaneously via a weighted linear combination of the corresponding vectors. For some attributes, our approach even allows for fine-tuning a model trained without annotations to support such interventions. In experiments with three attributes (length, politeness and monotonicity) and two language pairs (English to German and Japanese) our models achieve better control over a wider range of tasks compared to tagging, and translation quality does not degrade when no control is requested. Finally, we demonstrate how to enable control in an already trained model after a relatively cheap fine-tuning stage.",
}
@inproceedings{miao-etal-2021-generative,
    title = "A Generative Framework for Simultaneous Machine Translation",
    author = "Miao, Yishu  and
      Blunsom, Phil  and
      Specia, Lucia",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.536",
    doi = "10.18653/v1/2021.emnlp-main.536",
    pages = "6697--6706",
    abstract = "We propose a generative framework for simultaneous machine translation. Conventional approaches use a fixed number of source words to translate or learn dynamic policies for the number of source words by reinforcement learning. Here we formulate simultaneous translation as a structural sequence-to-sequence learning problem. A latent variable is introduced to model read or translate actions at every time step, which is then integrated out to consider all the possible translation policies. A re-parameterised Poisson prior is used to regularise the policies which allows the model to explicitly balance translation quality and latency. The experiments demonstrate the effectiveness and robustness of the generative framework, which achieves the best BLEU scores given different average translation latencies on benchmark datasets.",
}
@inproceedings{chakrabarty-etal-2021-dont,
    title = "Don{'}t Go Far Off: An Empirical Study on Neural Poetry Translation",
    author = "Chakrabarty, Tuhin  and
      Saakyan, Arkadiy  and
      Muresan, Smaranda",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.577",
    doi = "10.18653/v1/2021.emnlp-main.577",
    pages = "7253--7265",
    abstract = "Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data.",
}
@inproceedings{zhang-feng-2021-universal,
    title = "Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy",
    author = "Zhang, Shaolei  and
      Feng, Yang",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.581",
    doi = "10.18653/v1/2021.emnlp-main.581",
    pages = "7306--7317",
    abstract = "Simultaneous machine translation (SiMT) generates translation before reading the entire source sentence and hence it has to trade off between translation quality and latency. To fulfill the requirements of different translation quality and latency in practical applications, the previous methods usually need to train multiple SiMT models for different latency levels, resulting in large computational costs. In this paper, we propose a universal SiMT model with Mixture-of-Experts Wait-k Policy to achieve the best translation quality under arbitrary latency with only one trained model. Specifically, our method employs multi-head attention to accomplish the mixture of experts where each head is treated as a wait-k expert with its own waiting words number, and given a test latency and source inputs, the weights of the experts are accordingly adjusted to produce the best translation. Experiments on three datasets show that our method outperforms all the strong baselines under different latency, including the state-of-the-art adaptive policy.",
}
@inproceedings{xu-etal-2021-document-graph,
    title = "Document Graph for Neural Machine Translation",
    author = "Xu, Mingzhou  and
      Li, Liangyou  and
      Wong, Derek F.  and
      Liu, Qun  and
      Chao, Lidia S.",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.663",
    doi = "10.18653/v1/2021.emnlp-main.663",
    pages = "8435--8448",
    abstract = "Previous works have shown that contextual information can improve the performance of neural machine translation (NMT). However, most existing document-level NMT methods failed to leverage contexts beyond a few set of previous sentences. How to make use of the whole document as global contexts is still a challenge. To address this issue, we hypothesize that a document can be represented as a graph that connects relevant contexts regardless of their distances. We employ several types of relations, including adjacency, syntactic dependency, lexical consistency, and coreference, to construct the document graph. Then, we incorporate both source and target graphs into the conventional Transformer architecture with graph convolutional networks. Experiments on various NMT benchmarks, including IWSLT English{--}French, Chinese-English, WMT English{--}German and Opensubtitle English{--}Russian, demonstrate that using document graphs can significantly improve the translation quality. Extensive analysis verifies that the document graph is beneficial for capturing discourse phenomena.",
}
@inproceedings{voita-etal-2021-language,
    title = "Language Modeling, Lexical Translation, Reordering: The Training Process of {NMT} through the Lens of Classical {SMT}",
    author = "Voita, Elena  and
      Sennrich, Rico  and
      Titov, Ivan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.667",
    doi = "10.18653/v1/2021.emnlp-main.667",
    pages = "8478--8491",
    abstract = "Differently from the traditional statistical MT that decomposes the translation task into distinct separately learned components, neural machine translation uses a single neural network to model the entire translation process. Despite neural machine translation being de-facto standard, it is still not clear how NMT models acquire different competences over the course of training, and how this mirrors the different models in traditional SMT. In this work, we look at the competences related to three core SMT components and find that during training, NMT first focuses on learning target-side language modeling, then improves translation quality approaching word-by-word translation, and finally learns more complicated reordering patterns. We show that this behavior holds for several models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla non-autoregressive neural machine translation by guiding teacher model selection.",
}
@inproceedings{berard-etal-2021-efficient,
    title = "Efficient Inference for Multilingual Neural Machine Translation",
    author = "Berard, Alexandre  and
      Lee, Dain  and
      Clinchant, Stephane  and
      Jung, Kweonwoo  and
      Nikoulina, Vassilina",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.674",
    doi = "10.18653/v1/2021.emnlp-main.674",
    pages = "8563--8583",
    abstract = "Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several {``}light decoder{''} architectures in two 20-language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to almost 2 times faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation.",
}
@inproceedings{provilkov-malinin-2021-multi,
    title = "Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation",
    author = "Provilkov, Ivan  and
      Malinin, Andrey",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.677",
    doi = "10.18653/v1/2021.emnlp-main.677",
    pages = "8612--8621",
    abstract = "Neural Machine Translation (NMT) is known to suffer from a beam-search problem: after a certain point, increasing beam size causes an overall drop in translation quality. This effect is especially pronounced for long sentences. While much work was done analyzing this phenomenon, primarily for autoregressive NMT models, there is still no consensus on its underlying cause. In this work, we analyze errors that cause major quality degradation with large beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that strongly contributes to the quality degradation with large beams is dataset length-bias - NMT datasets are strongly biased towards short sentences. To mitigate this issue, we propose a new data augmentation technique {--} Multi-Sentence Resampling (MSR). This technique extends the training examples by concatenating several sentences from the original dataset to make a long training example. We demonstrate that MSR significantly reduces degradation with growing beam size and improves final translation quality on the IWSTL15 En-Vi, IWSTL17 En-Fr, and WMT14 En-De datasets.",
}
@inproceedings{zouhar-etal-2021-neural,
    title = "Neural Machine Translation Quality and Post-Editing Performance",
    author = "Zouhar, Vil{\'e}m  and
      Popel, Martin  and
      Bojar, Ond{\v{r}}ej  and
      Tamchyna, Ale{\v{s}}",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.801",
    doi = "10.18653/v1/2021.emnlp-main.801",
    pages = "10204--10214",
    abstract = "We test the natural expectation that using MT in professional translation saves human processing time. The last such study was carried out by Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the translation quality. In contrast, we focus on neural MT (NMT) of high quality, which has become the state-of-the-art approach since then and also got adopted by most translation companies. Through an experimental study involving over 30 professional translators for English -{\textgreater} Czech translation, we examine the relationship between NMT performance and post-editing time and quality. Across all models, we found that better MT systems indeed lead to fewer changes in the sentences in this industry setting. The relation between system quality and post-editing time is however not straightforward and, contrary to the results on phrase-based MT, BLEU is definitely not a stable predictor of the time or final output quality.",
}
@inproceedings{li-etal-2021-miss,
    title = "{M}i{SS}: An Assistant for Multi-Style Simultaneous Translation",
    author = "Li, Zuchao  and
      Parnow, Kevin  and
      Utiyama, Masao  and
      Sumita, Eiichiro  and
      Zhao, Hai",
    editor = "Adel, Heike  and
      Shi, Shuming",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.1",
    doi = "10.18653/v1/2021.emnlp-demo.1",
    pages = "1--10",
    abstract = "In this paper, we present \textbf{MiSS}, an assistant for multi-style simultaneous translation. Our proposed translation system has five key features: highly accurate translation, simultaneous translation, translation for multiple text styles, back-translation for translation quality evaluation, and grammatical error correction. With this system, we aim to provide a complete translation experience for machine translation users. Our design goals are high translation accuracy, real-time translation, flexibility, and measurable translation quality. Compared with the free commercial translation systems commonly used, our translation assistance system regards the machine translation application as a more complete and fully-featured tool for users. By incorporating additional features and giving the user better control over their experience, we improve translation efficiency and performance. Additionally, our assistant system combines machine translation, grammatical error correction, and interactive edits, and uses a crowdsourcing mode to collect more data for further training to improve both the machine translation and grammatical error correction models. A short video demonstrating our system is available at \url{https://www.youtube.com/watch?v=ZGCo7KtRKd8}.",
}
@inproceedings{liu-etal-2021-enriching,
    title = "Enriching Non-Autoregressive Transformer with Syntactic and Semantic Structures for Neural Machine Translation",
    author = "Liu, Ye  and
      Wan, Yao  and
      Zhang, Jianguo  and
      Zhao, Wenting  and
      Yu, Philip",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.105",
    doi = "10.18653/v1/2021.eacl-main.105",
    pages = "1235--1244",
    abstract = "The non-autoregressive models have boosted the efficiency of neural machine translation through parallelized decoding at the cost of effectiveness, when comparing with the autoregressive counterparts. In this paper, we claim that the syntactic and semantic structures among natural language are critical for non-autoregressive machine translation and can further improve the performance. However, these structures are rarely considered in the existing non-autoregressive models. Inspired by this intuition, we propose to incorporate the explicit syntactic and semantic structure of languages into a non-autoregressive Transformer, for the task of neural machine translation. Moreover, we also consider the intermediate latent alignment within target sentences to better learn the long-term token dependencies. Experimental results on two real-world datasets (i.e., WMT14 En-De and WMT16 En- Ro) show that our model achieves a significantly faster speed, as well as keeps the translation quality when compared with several state-of-the-art non-autoregressive models.",
}
@inproceedings{kong-etal-2021-multilingual,
    title = "Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders",
    author = "Kong, Xiang  and
      Renduchintala, Adithya  and
      Cross, James  and
      Tang, Yuqing  and
      Gu, Jiatao  and
      Li, Xian",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.138",
    doi = "10.18653/v1/2021.eacl-main.138",
    pages = "1613--1624",
    abstract = "Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-to-one translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality.",
}
@inproceedings{tambouratzis-2021-alignment,
    title = "Alignment verification to improve {NMT} translation towards highly inflectional languages with limited resources",
    author = "Tambouratzis, George",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.158",
    doi = "10.18653/v1/2021.eacl-main.158",
    pages = "1841--1851",
    abstract = "The present article discusses how to improve translation quality when using limited training data to translate towards morphologically rich languages. The starting point is a neural MT system, used to train translation models, using solely publicly available parallel data. An initial analysis of the translation output has shown that quality is sub-optimal, due mainly to an insufficient amount of training data. To improve translation quality, a hybridized solution is proposed, using an ensemble of relatively simple NMT systems trained with different metrics, combined with an open source module, designed for a low-resource MT system. Experimental results of the proposed hybridized method with multiple independent test sets achieve improvements over (i) both the best individual NMT and (ii) the standard ensemble system provided in the Marian-NMT system. Improvements over Marian-NMT are in many cases statistically significant. Finally, a qualitative analysis of translation results indicates a greater robustness for the hybridized method.",
}
@inproceedings{weller-etal-2021-streaming,
    title = "Streaming Models for Joint Speech Recognition and Translation",
    author = "Weller, Orion  and
      Sperber, Matthias  and
      Gollan, Christian  and
      Kluivers, Joris",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.216",
    doi = "10.18653/v1/2021.eacl-main.216",
    pages = "2533--2539",
    abstract = "Using end-to-end models for speech translation (ST) has increasingly been the focus of the ST community. These models condense the previously cascaded systems by directly converting sound waves into translated text. However, cascaded models have the advantage of including automatic speech recognition output, useful for a variety of practical ST systems that often display transcripts to the user alongside the translations. To bridge this gap, recent work has shown initial progress into the feasibility for end-to-end models to produce both of these outputs. However, all previous work has only looked at this problem from the consecutive perspective, leaving uncertainty on whether these approaches are effective in the more challenging streaming setting. We develop an end-to-end streaming ST model based on a re-translation approach and compare against standard cascading approaches. We also introduce a novel inference method for the joint case, interleaving both transcript and translation in generation and removing the need to use separate decoders. Our evaluation across a range of metrics capturing accuracy, latency, and consistency shows that our end-to-end models are statistically similar to cascading models, while having half the number of parameters. We also find that both systems provide strong translation quality at low latency, keeping 99{\%} of consecutive quality at a lag of just under a second.",
}
@inproceedings{arthur-etal-2021-learning,
    title = "Learning Coupled Policies for Simultaneous Machine Translation using Imitation Learning",
    author = "Arthur, Philip  and
      Cohn, Trevor  and
      Haffari, Gholamreza",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.233",
    doi = "10.18653/v1/2021.eacl-main.233",
    pages = "2709--2719",
    abstract = "We present a novel approach to efficiently learn a simultaneous translation model with coupled programmer-interpreter policies. First, we present an algorithmic oracle to produce oracle READ/WRITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality quality while keeping the delay low.",
}
@inproceedings{moradi-etal-2021-measuring,
    title = "Measuring and Improving Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.243",
    doi = "10.18653/v1/2021.eacl-main.243",
    pages = "2791--2802",
    abstract = "While the attention heatmaps produced by neural machine translation (NMT) models seem insightful, there is little evidence that they reflect a model{'}s true internal reasoning. We provide a measure of faithfulness for NMT based on a variety of stress tests where attention weights which are crucial for prediction are perturbed and the model should alter its predictions if the learned weights are a faithful explanation of the predictions. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and has a useful regularization effect on the NMT model and can even improve translation quality in some cases.",
}
@inproceedings{ive-etal-2021-exploiting,
    title = "Exploiting Multimodal Reinforcement Learning for Simultaneous Machine Translation",
    author = "Ive, Julia  and
      Li, Andy Mingren  and
      Miao, Yishu  and
      Caglayan, Ozan  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.281",
    doi = "10.18653/v1/2021.eacl-main.281",
    pages = "3222--3233",
    abstract = "This paper addresses the problem of simultaneous machine translation (SiMT) by exploring two main concepts: (a) adaptive policies to learn a good trade-off between high translation quality and low latency; and (b) visual information to support this process by providing additional (visual) contextual information which may be available before the textual input is produced. For that, we propose a multimodal approach to simultaneous machine translation using reinforcement learning, with strategies to integrate visual and textual information in both the agent and the environment. We provide an exploration on how different types of visual information and integration strategies affect the quality and latency of simultaneous translation models, and demonstrate that visual cues lead to higher quality while keeping the latency low.",
}
@article{shao-etal-2021-sequence,
    title = "Sequence-Level Training for Non-Autoregressive Neural Machine Translation",
    author = "Shao, Chenze  and
      Feng, Yang  and
      Zhang, Jinchao  and
      Meng, Fandong  and
      Zhou, Jie",
    journal = "Computational Linguistics",
    volume = "47",
    number = "4",
    month = dec,
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.cl-4.29",
    doi = "10.1162/coli_a_00421",
    pages = "891--925",
    abstract = "In recent years, Neural Machine Translation (NMT) has achieved notable results in various translation tasks. However, the word-by-word generation manner determined by the autoregressive mechanism leads to high translation latency of the NMT and restricts its low-latency applications. Non-Autoregressive Neural Machine Translation (NAT) removes the autoregressive mechanism and achieves significant decoding speedup by generating target words independently and simultaneously. Nevertheless, NAT still takes the word-level cross-entropy loss as the training objective, which is not optimal because the output of NAT cannot be properly evaluated due to the multimodality problem. In this article, we propose using sequence-level training objectives to train NAT models, which evaluate the NAT outputs as a whole and correlates well with the real translation quality. First, we propose training NAT models to optimize sequence-level evaluation metrics (e.g., BLEU) based on several novel reinforcement algorithms customized for NAT, which outperform the conventional method by reducing the variance of gradient estimation. Second, we introduce a novel training objective for NAT models, which aims to minimize the Bag-of-N-grams (BoN) difference between the model output and the reference sentence. The BoN training objective is differentiable and can be calculated efficiently without doing any approximations. Finally, we apply a three-stage training strategy to combine these two methods to train the NAT model. We validate our approach on four translation tasks (WMT14 En↔De, WMT16 En↔Ro), which shows that our approach largely outperforms NAT baselines and achieves remarkable performance on all translation tasks. The source code is available at \url{https://github.com/ictnlp/Seq-NAT}.",
}
@inproceedings{xuewen-etal-2021-reducing,
    title = "Reducing Length Bias in Scoring Neural Machine Translation via a Causal Inference Method",
    author = "Xuewen, Shi  and
      Heyan, Huang  and
      Ping, Jian  and
      Yi-Kun, Tang",
    editor = "Li, Sheng  and
      Sun, Maosong  and
      Liu, Yang  and
      Wu, Hua  and
      Liu, Kang  and
      Che, Wanxiang  and
      He, Shizhu  and
      Rao, Gaoqi",
    booktitle = "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2021",
    address = "Huhhot, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2021.ccl-1.78",
    pages = "874--885",
    abstract = "Neural machine translation (NMT) usually employs beam search to expand the searching spaceand obtain more translation candidates. However the increase of the beam size often suffersfrom plenty of short translations resulting in dramatical decrease in translation quality. In this paper we handle the length bias problem through a perspective of causal inference. Specially we regard the model generated translation score S as a degraded true translation quality affectedby some noise and one of the confounders is the translation length. We apply a Half-Sibling Re-gression method to remove the length effect on S and then we can obtain a debiased translation score without length information. The proposed method is model agnostic and unsupervised which is adaptive to any NMT model and test dataset. We conduct the experiments on three translation tasks with different scales of datasets. Experimental results and further analyses showthat our approaches gain comparable performance with the empirical baseline methods.",
    language = "English",
}
@inproceedings{feiyu-etal-2021-incorporating,
    title = "Incorporating translation quality estimation into {C}hinese-{K}orean neural machine translation",
    author = "Feiyu, Li  and
      Yahui, Zhao  and
      Feiyang, Yang  and
      Rongyi, Cui",
    editor = "Li, Sheng  and
      Sun, Maosong  and
      Liu, Yang  and
      Wu, Hua  and
      Liu, Kang  and
      Che, Wanxiang  and
      He, Shizhu  and
      Rao, Gaoqi",
    booktitle = "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2021",
    address = "Huhhot, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2021.ccl-1.81",
    pages = "906--915",
    abstract = "Exposure bias and poor translation diversity are two common problems in neural machine trans-lation (NMT) which are caused by the general of the teacher forcing strategy for training inthe NMT models. Moreover the NMT models usually require the large-scale and high-quality parallel corpus. However Korean is a low resource language and there is no large-scale parallel corpus between Chinese and Korean which is a challenging for the researchers. Therefore wepropose a method which is to incorporate translation quality estimation into the translation processand adopt reinforcement learning. The evaluation mechanism is used to guide the training of the model so that the prediction cannot converge completely to the ground truth word. When the model predicts a sequence different from the ground truth word the evaluation mechanism cangive an appropriate evaluation and reward to the model. In addition we alleviated the lack of Korean corpus resources by adding training data. In our experiment we introduce a monolingual corpus of a certain scale to construct pseudo-parallel data. At the same time we also preprocessed the Korean corpus with different granularities to overcome the data sparsity. Experimental results show that our work is superior to the baselines in Chinese-Korean and Korean-Chinese translation tasks which fully certificates the effectiveness of our method.",
    language = "English",
}
@inproceedings{steingrimsson-etal-2021-effective,
    title = "Effective Bitext Extraction From Comparable Corpora Using a Combination of Three Different Approaches",
    author = "Steingr{\'\i}msson, Stein{\th}{\'o}r  and
      Lohar, Pintu  and
      Loftsson, Hrafn  and
      Way, Andy",
    editor = "Rapp, Reinhard  and
      Sharoff, Serge  and
      Zweigenbaum, Pierre",
    booktitle = "Proceedings of the 14th Workshop on Building and Using Comparable Corpora (BUCC 2021)",
    month = sep,
    year = "2021",
    address = "Online (Virtual Mode)",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/2021.bucc-1.3",
    pages = "8--17",
    abstract = "Parallel sentences extracted from comparable corpora can be useful to supplement parallel corpora when training machine translation (MT) systems. This is even more prominent in low-resource scenarios, where parallel corpora are scarce. In this paper, we present a system which uses three very different measures to identify and score parallel sentences from comparable corpora. We measure the accuracy of our methods in low-resource settings by comparing the results against manually curated test data for English{--}Icelandic, and by evaluating an MT system trained on the concatenation of the parallel data extracted by our approach and an existing data set. We show that the system is capable of extracting useful parallel sentences with high accuracy, and that the extracted pairs substantially increase translation quality of an MT system trained on the data, as measured by automatic evaluation metrics.",
}
@inproceedings{wang-etal-2021-length,
    title = "How Length Prediction Influence the Performance of Non-Autoregressive Translation?",
    author = "Wang, Minghan  and
      Jiaxin, Guo  and
      Wang, Yuxia  and
      Chen, Yimeng  and
      Chang, Su  and
      Shang, Hengchao  and
      Zhang, Min  and
      Tao, Shimin  and
      Yang, Hao",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Dupoux, Emmanuel  and
      Giulianelli, Mario  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.blackboxnlp-1.14",
    doi = "10.18653/v1/2021.blackboxnlp-1.14",
    pages = "205--213",
    abstract = "Length prediction is a special task in a series of NAT models where target length has to be determined before generation. However, the performance of length prediction and its influence on translation quality has seldom been discussed. In this paper, we present comprehensive analyses on length prediction task of NAT, aiming to find the factors that influence performance, as well as how it associates with translation quality. We mainly perform experiments based on Conditional Masked Language Model (CMLM) (Ghazvininejad et al., 2019), a representative NAT model, and evaluate it on two language pairs, En-De and En-Ro. We draw two conclusions: 1) The performance of length prediction is mainly influenced by properties of language pairs such as alignment pattern, word order or intrinsic length ratio, and is also affected by the usage of knowledge distilled data. 2) There is a positive correlation between the performance of the length prediction and the BLEU score.",
}
@inproceedings{liu-etal-2021-bits,
    title = "{BIT}{'}s system for {A}uto{S}imul{T}rans2021",
    author = "Liu, Mengge  and
      Chen, Shuoying  and
      Li, Minqin  and
      Wang, Zhipeng  and
      Guo, Yuhang",
    editor = "Wu, Hua  and
      Cherry, Colin  and
      Huang, Liang  and
      He, Zhongjun  and
      Liu, Qun  and
      Elbayad, Maha  and
      Liberman, Mark  and
      Wang, Haifeng  and
      Ma, Mingbo  and
      Zhang, Ruiqing",
    booktitle = "Proceedings of the Second Workshop on Automatic Simultaneous Translation",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.autosimtrans-1.2",
    doi = "10.18653/v1/2021.autosimtrans-1.2",
    pages = "12--18",
    abstract = "In this paper we introduce our Chinese-English simultaneous translation system participating in AutoSimulTrans2021. In simultaneous translation, translation quality and delay are both important. In order to reduce the translation delay, we cut the streaming-input source sentence into segments and translate the segments before the full sentence is received. In order to obtain high-quality translations, we pre-train a translation model with adequate corpus and fine-tune the model with domain adaptation and sentence length adaptation. The experimental results on the evaluation data show that our system performs better than the baseline system.",
}
@inproceedings{zhang-etal-2021-findings,
    title = "Findings of the Second Workshop on Automatic Simultaneous Translation",
    author = "Zhang, Ruiqing  and
      Zhang, Chuanqiang  and
      He, Zhongjun  and
      Wu, Hua  and
      Wang, Haifeng",
    editor = "Wu, Hua  and
      Cherry, Colin  and
      Huang, Liang  and
      He, Zhongjun  and
      Liu, Qun  and
      Elbayad, Maha  and
      Liberman, Mark  and
      Wang, Haifeng  and
      Ma, Mingbo  and
      Zhang, Ruiqing",
    booktitle = "Proceedings of the Second Workshop on Automatic Simultaneous Translation",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.autosimtrans-1.6",
    doi = "10.18653/v1/2021.autosimtrans-1.6",
    pages = "36--44",
    abstract = "This paper presents the results of the shared task of the 2nd Workshop on Automatic Simultaneous Translation (AutoSimTrans). The task includes two tracks, one for text-to-text translation and one for speech-to-text, requiring participants to build systems to translate from either the source text or speech into the target text. Different from traditional machine translation, the AutoSimTrans shared task evaluates not only translation quality but also latency. We propose a metric {``}Monotonic Optimal Sequence{''} (MOS) considering both quality and latency to rank the submissions. We also discuss some important open issues in simultaneous translation.",
}
@inproceedings{stadler-etal-2021-observing,
    title = "Observing the Learning Curve of {NMT} Systems With Regard to Linguistic Phenomena",
    author = "Stadler, Patrick  and
      Macketanz, Vivien  and
      Avramidis, Eleftherios",
    editor = "Kabbara, Jad  and
      Lin, Haitao  and
      Paullada, Amandalynne  and
      Vamvas, Jannis",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-srw.20",
    doi = "10.18653/v1/2021.acl-srw.20",
    pages = "186--196",
    abstract = "In this paper we present our observations and evaluations by observing the linguistic performance of the system on several steps on the training process of various English-to-German Neural Machine Translation models. The linguistic performance is measured through a semi-automatic process using a test suite. Among several linguistic observations, we find that the translation quality of some linguistic categories decreased within the recorded iterations. Additionally, we notice some drops of the translation quality of certain categories when using a larger corpus.",
}
@inproceedings{obamuyide-etal-2021-continual,
    title = "Continual Quality Estimation with Online {B}ayesian Meta-Learning",
    author = "Obamuyide, Abiola  and
      Fomicheva, Marina  and
      Specia, Lucia",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.25",
    doi = "10.18653/v1/2021.acl-short.25",
    pages = "190--197",
    abstract = "Most current quality estimation (QE) models for machine translation are trained and evaluated in a static setting where training and test data are assumed to be from a fixed distribution. However, in real-life settings, the test data that a deployed QE model would be exposed to may differ from its training data. In particular, training samples are often labelled by one or a small set of annotators, whose perceptions of translation quality and needs may differ substantially from those of end-users, who will employ predictions in practice. To address this challenge, we propose an online Bayesian meta-learning framework for the continuous training of QE models that is able to adapt them to the needs of different users, while being robust to distributional shifts in training and test data. Experiments on data with varying number of users and language characteristics validate the effectiveness of the proposed approach.",
}
@inproceedings{zhang-etal-2021-beyond,
    title = "Beyond Sentence-Level End-to-End Speech Translation: Context Helps",
    author = "Zhang, Biao  and
      Titov, Ivan  and
      Haddow, Barry  and
      Sennrich, Rico",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.200",
    doi = "10.18653/v1/2021.acl-long.200",
    pages = "2566--2578",
    abstract = "Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied. We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency. We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model. Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST. Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.",
}
@inproceedings{jiao-etal-2021-self,
    title = "Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation",
    author = "Jiao, Wenxiang  and
      Wang, Xing  and
      Tu, Zhaopeng  and
      Shi, Shuming  and
      Lyu, Michael  and
      King, Irwin",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.221",
    doi = "10.18653/v1/2021.acl-long.221",
    pages = "2840--2850",
    abstract = "Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data. The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal. In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data. To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data. Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains. Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability. Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach. Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.",
}
@inproceedings{ding-etal-2021-rejuvenating,
    title = "Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation",
    author = "Ding, Liang  and
      Wang, Longyue  and
      Liu, Xuebo  and
      Wong, Derek F.  and
      Tao, Dacheng  and
      Tu, Zhaopeng",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.266",
    doi = "10.18653/v1/2021.acl-long.266",
    pages = "3431--3441",
    abstract = "Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models. However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words. To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining. By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source. Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words. To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance. We conduct experiments on five translation benchmarks over two advanced architectures. Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively. Our code, data, and trained models are available at \url{https://github.com/longyuewangdcu/RLFW-NAT}.",
}
@inproceedings{tang-etal-2021-improving,
    title = "Improving Speech Translation by Understanding and Learning from the Auxiliary Text Translation Task",
    author = "Tang, Yun  and
      Pino, Juan  and
      Li, Xian  and
      Wang, Changhan  and
      Genzel, Dmitriy",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.328",
    doi = "10.18653/v1/2021.acl-long.328",
    pages = "4252--4261",
    abstract = "Pretraining and multitask learning are widely used to improve the speech translation performance. In this study, we are interested in training a speech translation model along with an auxiliary text translation task. We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework. Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules. We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task. The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality. Inspired by these findings, we propose three methods to improve translation quality. First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks. Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer. Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task. Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs.",
}
@inproceedings{li-yao-2021-rewriter,
    title = "Rewriter-Evaluator Architecture for Neural Machine Translation",
    author = "Li, Yangming  and
      Yao, Kaisheng",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.443",
    doi = "10.18653/v1/2021.acl-long.443",
    pages = "5701--5710",
    abstract = "A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding. However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process. To address this issue, we introduce a novel architecture of Rewriter-Evaluator. Translating a source sentence involves multiple rewriting passes. In every pass, a rewriter generates a new translation to improve the past translation. Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator. We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator. Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods. An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy. Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.",
}
@inproceedings{liang-etal-2021-modeling,
    title = "Modeling Bilingual Conversational Characteristics for Neural Chat Translation",
    author = "Liang, Yunlong  and
      Meng, Fandong  and
      Chen, Yufeng  and
      Xu, Jinan  and
      Zhou, Jie",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.444",
    doi = "10.18653/v1/2021.acl-long.444",
    pages = "5711--5724",
    abstract = "Neural chat translation aims to translate bilingual conversational text, which has a broad application in international exchanges and cooperation. Despite the impressive performance of sentence-level and context-aware Neural Machine Translation (NMT), there still remain challenges to translate bilingual conversational text due to its inherent characteristics such as role preference, dialogue coherence, and translation consistency. In this paper, we aim to promote the translation quality of conversational text by modeling the above properties. Specifically, we design three latent variational modules to learn the distributions of bilingual conversational characteristics. Through sampling from these learned distributions, the latent variables, tailored for role preference, dialogue coherence, and translation consistency, are incorporated into the NMT model for better translation. We evaluate our approach on the benchmark dataset BConTrasT (English{\textless}-{\textgreater}German) and a self-collected bilingual dialogue corpus, named BMELD (English{\textless}-{\textgreater}Chinese). Extensive experiments show that our approach notably boosts the performance over strong baselines by a large margin and significantly surpasses some state-of-the-art context-aware NMT models in terms of BLEU and TER. Additionally, we make the BMELD dataset publicly available for the research community.",
}
@inproceedings{fernandes-etal-2021-measuring,
    title = "Measuring and Increasing Context Usage in Context-Aware Machine Translation",
    author = "Fernandes, Patrick  and
      Yin, Kayo  and
      Neubig, Graham  and
      Martins, Andr{\'e} F. T.",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.505",
    doi = "10.18653/v1/2021.acl-long.505",
    pages = "6467--6478",
    abstract = "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",
}
@inproceedings{briakou-carpuat-2021-beyond,
    title = "Beyond Noise: Mitigating the Impact of Fine-grained Semantic Divergences on Neural Machine Translation",
    author = "Briakou, Eleftheria  and
      Carpuat, Marine",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.562",
    doi = "10.18653/v1/2021.acl-long.562",
    pages = "7236--7249",
    abstract = "While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise. As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training. To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models. We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions. Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.",
}
@inproceedings{lee-etal-2021-intellicat,
    title = "{I}ntelli{CAT}: Intelligent Machine Translation Post-Editing with Quality Estimation and Translation Suggestion",
    author = "Lee, Dongjun  and
      Ahn, Junhyeong  and
      Park, Heesoo  and
      Jo, Jaemin",
    editor = "Ji, Heng  and
      Park, Jong C.  and
      Xia, Rui",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.2",
    doi = "10.18653/v1/2021.acl-demo.2",
    pages = "11--19",
    abstract = "We present IntelliCAT, an interactive translation interface with neural models that streamline the post-editing process on machine translation output. We leverage two quality estimation (QE) models at different granularities: sentence-level QE, to predict the quality of each machine-translated sentence, and word-level QE, to locate the parts of the machine-translated sentence that need correction. Additionally, we introduce a novel translation suggestion model conditioned on both the left and right contexts, providing alternatives for specific words or phrases for correction. Finally, with word alignments, IntelliCAT automatically preserves the original document{'}s styles in the translated document. The experimental results show that post-editing based on the proposed QE and translation suggestions can significantly improve translation quality. Furthermore, a user study reveals that three features provided in IntelliCAT significantly accelerate the post-editing task, achieving a 52.9{\%} speedup in translation time compared to translating from scratch. The interface is publicly available at \url{https://intellicat.beringlab.com/}.",
}
@inproceedings{rei-etal-2021-mt,
    title = "{MT}-{T}elescope: {A}n interactive platform for contrastive evaluation of {MT} systems",
    author = "Rei, Ricardo  and
      Farinha, Ana C  and
      Stewart, Craig  and
      Coheur, Luisa  and
      Lavie, Alon",
    editor = "Ji, Heng  and
      Park, Jong C.  and
      Xia, Rui",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.9",
    doi = "10.18653/v1/2021.acl-demo.9",
    pages = "73--80",
    abstract = "We present MT-Telescope, a visualization platform designed to facilitate comparative analysis of the output quality of two Machine Translation (MT) systems. While automated MT evaluation metrics are commonly used to evaluate MT systems at a corpus-level, our platform supports fine-grained segment-level analysis and interactive visualisations that expose the fundamental differences in the performance of the compared systems. MT-Telescope also supports dynamic corpus filtering to enable focused analysis on specific phenomena such as; translation of named entities, handling of terminology, and the impact of input segment length on translation quality. Furthermore, the platform provides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-Telescope is open source, written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality.",
}
@inproceedings{bawden-etal-2020-university,
    title = "The {U}niversity of {E}dinburgh{'}s {E}nglish-{T}amil and {E}nglish-{I}nuktitut Submissions to the {WMT}20 News Translation Task",
    author = "Bawden, Rachel  and
      Birch, Alexandra  and
      Dobreva, Radina  and
      Oncevay, Arturo  and
      Miceli Barone, Antonio Valerio  and
      Williams, Philip",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.5",
    pages = "92--99",
    abstract = "We describe the University of Edinburgh{'}s submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.",
}
@inproceedings{germann-etal-2020-speed,
    title = "Speed-optimized, Compact Student Models that Distill Knowledge from a Larger Teacher Model: the {UEDIN}-{CUNI} Submission to the {WMT} 2020 News Translation Task",
    author = "Germann, Ulrich  and
      Grundkiewicz, Roman  and
      Popel, Martin  and
      Dobreva, Radina  and
      Bogoychev, Nikolay  and
      Heafield, Kenneth",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.17",
    pages = "191--196",
    abstract = "We describe the joint submission of the University of Edinburgh and Charles University, Prague, to the Czech/English track in the WMT 2020 Shared Task on News Translation. Our fast and compact student models distill knowledge from a larger, slower teacher. They are designed to offer a good trade-off between translation quality and inference efficiency. On the WMT 2020 Czech ↔ English test sets, they achieve translation speeds of over 700 whitespace-delimited source words per second on a single CPU thread, thus making neural translation feasible on consumer hardware without a GPU.",
}
@inproceedings{goyal-etal-2020-contact,
    title = "Contact Relatedness can help improve multilingual {NMT}: {M}icrosoft {STCI}-{MT} @ {WMT}20",
    author = "Goyal, Vikrant  and
      Kunchukuttan, Anoop  and
      Kejriwal, Rahul  and
      Jain, Siddharth  and
      Bhagwat, Amit",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.19",
    pages = "202--206",
    abstract = "We describe our submission for the English→Tamil and Tamil→English news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation.",
}
@inproceedings{gwinnup-anderson-2020-afrl,
    title = "The {AFRL} {WMT}20 News Translation Systems",
    author = "Gwinnup, Jeremy  and
      Anderson, Tim",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.20",
    pages = "207--212",
    abstract = "This report summarizes the Air Force Research Laboratory (AFRL) machine translation (MT) systems submitted to the news-translation task as part of the 2020 Conference on Machine Translation (WMT20) evaluation campaign. This year we largely repurpose strategies from previous years{'} efforts with larger datasets and also train models with precomputed word alignments under various settings in an effort to improve translation quality.",
}
@inproceedings{marie-etal-2020-combination,
    title = "Combination of Neural Machine Translation Systems at {WMT}20",
    author = "Marie, Benjamin  and
      Rubino, Raphael  and
      Fujita, Atsushi",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.23",
    pages = "230--238",
    abstract = "This paper presents neural machine translation systems and their combination built for the WMT20 English-Polish and Japanese-{\textgreater}English translation tasks. We show that using a Transformer Big architecture, additional training data synthesized from monolingual data, and combining many NMT systems through n-best list reranking improve translation quality. However, while we observed such improvements on the validation data, we did not observed similar improvements on the test data. Our analysis reveals that the presence of translationese texts in the validation data led us to take decisions in building NMT systems that were not optimal to obtain the best results on the test data.",
}
@inproceedings{ul-haq-etal-2020-document,
    title = "Document Level {NMT} of Low-Resource Languages with Backtranslation",
    author = "Ul Haq, Sami  and
      Abdul Rauf, Sadaf  and
      Shaukat, Arsalan  and
      Saeed, Abdullah",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.53",
    pages = "442--446",
    abstract = "This paper describes our system submission to WMT20 shared task on similar language translation. We examined the use of documentlevel neural machine translation (NMT) systems for low-resource, similar language pair Marathi−Hindi. Our system is an extension of state-of-the-art Transformer architecture with hierarchical attention networks to incorporate contextual information. Since, NMT requires large amount of parallel data which is not available for this task, our approach is focused on utilizing monolingual data with back translation to train our models. Our experiments reveal that document-level NMT can be a reasonable alternative to sentence-level NMT for improving translation quality of low resourced languages even when used with synthetic data.",
}
@inproceedings{dabre-fujita-2020-combining,
    title = "Combining Sequence Distillation and Transfer Learning for Efficient Low-Resource Neural Machine Translation Models",
    author = "Dabre, Raj  and
      Fujita, Atsushi",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.61",
    pages = "492--502",
    abstract = "In neural machine translation (NMT), sequence distillation (SD) through creation of distilled corpora leads to efficient (compact and fast) models. However, its effectiveness in extremely low-resource (ELR) settings has not been well-studied. On the other hand, transfer learning (TL) by leveraging larger helping corpora greatly improves translation quality in general. This paper investigates a combination of SD and TL for training efficient NMT models for ELR settings, where we utilize TL with helping corpora twice: once for distilling the ELR corpora and then during compact model training. We experimented with two ELR settings: Vietnamese{--}English and Hindi{--}English from the Asian Language Treebank dataset with 18k training sentence pairs. Using the compact models with 40{\%} smaller parameters trained on the distilled ELR corpora, greedy search achieved 3.6 BLEU points improvement in average while reducing 40{\%} of decoding time. We also confirmed that using both the distilled ELR and helping corpora in the second round of TL further improves translation quality. Our work highlights the importance of stage-wise application of SD and TL for efficient NMT modeling for ELR settings.",
}
@inproceedings{freitag-firat-2020-complete,
    title = "Complete Multilingual Neural Machine Translation",
    author = "Freitag, Markus  and
      Firat, Orhan",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.66",
    pages = "550--560",
    abstract = "Multilingual Neural Machine Translation (MNMT) models are commonly trained on a joint set of bilingual corpora which is acutely English-centric (i.e. English either as source or target language). While direct data between two languages that are non-English is explicitly available at times, its use is not common. In this paper, we first take a step back and look at the commonly used bilingual corpora (WMT), and resurface the existence and importance of implicit structure that existed in it: multi-way alignment across examples (the same sentence in more than two languages). We set out to study the use of multi-way aligned examples in order to enrich the original English-centric parallel corpora. We reintroduce this direct parallel data from multi-way aligned corpora between all source and target languages. By doing so, the English-centric graph expands into a complete graph, every language pair being connected. We call MNMT with such connectivity pattern complete Multilingual Neural Machine Translation (cMNMT) and demonstrate its utility and efficacy with a series of experiments and analysis. In combination with a novel training data sampling strategy that is conditioned on the target language only, cMNMT yields competitive translation quality for all language pairs. We further study the size effect of multi-way aligned data, its transfer learning capabilities and how it eases adding a new language in MNMT. Finally, we stress test cMNMT at scale and demonstrate that we can train a cMNMT model with up to 12,432 language pairs that provides competitive translation quality for all language pairs.",
}
@inproceedings{huo-etal-2020-diving,
    title = "Diving Deep into Context-Aware Neural Machine Translation",
    author = "Huo, Jingjing  and
      Herold, Christian  and
      Gao, Yingbo  and
      Dahlmann, Leonard  and
      Khadivi, Shahram  and
      Ney, Hermann",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.71",
    pages = "604--616",
    abstract = "Context-aware neural machine translation (NMT) is a promising direction to improve the translation quality by making use of the additional context, e.g., document-level translation, or having meta-information. Although there exist various architectures and analyses, the effectiveness of different context-aware NMT models is not well explored yet. This paper analyzes the performance of document-level NMT models on four diverse domains with a varied amount of parallel document-level bilingual data. We conduct a comprehensive set of experiments to investigate the impact of document-level NMT. We find that there is no single best approach to document-level NMT, but rather that different architectures come out on top on different tasks. Looking at task-specific problems, such as pronoun resolution or headline translation, we find improvements in the context-aware systems, even in cases where the corpus-level metrics like BLEU show no significant improvement. We also show that document-level back-translation significantly helps to compensate for the lack of document-level bi-texts.",
}
@inproceedings{lo-2020-extended,
    title = "Extended Study on Using Pretrained Language Models and {Y}i{S}i-1 for Machine Translation Evaluation",
    author = "Lo, Chi-kiu",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.99",
    pages = "895--902",
    abstract = "We present an extended study on using pretrained language models and YiSi-1 for machine translation evaluation. Although the recently proposed contextual embedding based metrics, YiSi-1, significantly outperform BLEU and other metrics in correlating with human judgment on translation quality, we have yet to understand the full strength of using pretrained language models for machine translation evaluation. In this paper, we study YiSi-1{'}s correlation with human translation quality judgment by varying three major attributes (which architecture; which inter- mediate layer; whether it is monolingual or multilingual) of the pretrained language mod- els. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output.",
}
@inproceedings{lo-larkin-2020-machine,
    title = "Machine Translation Reference-less Evaluation using {Y}i{S}i-2 with Bilingual Mappings of Massive Multilingual Language Model",
    author = "Lo, Chi-kiu  and
      Larkin, Samuel",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.100",
    pages = "903--910",
    abstract = "We present a study on using YiSi-2 with massive multilingual pretrained language models for machine translation (MT) reference-less evaluation. Aiming at finding better semantic representation for semantic MT evaluation, we first test YiSi-2 with contextual embed- dings extracted from different layers of two different pretrained models, multilingual BERT and XLM-RoBERTa. We also experiment with learning bilingual mappings that trans- form the vector subspace of the source language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2{'}s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the tar- get embedding space using a cross-lingual lin- ear projection (CLP) matrix learnt from a small development set.",
}
@inproceedings{lo-joanis-2020-improving,
    title = "Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models",
    author = "Lo, Chi-kiu  and
      Joanis, Eric",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.110",
    pages = "972--978",
    abstract = "The National Research Council of Canada{'}s team submissions to the parallel corpus filtering task at the Fifth Conference on Machine Translation are based on two key components: (1) iteratively refined statistical sentence alignments for extracting sentence pairs from document pairs and (2) a crosslingual semantic textual similarity metric based on a pretrained multilingual language model, XLM-RoBERTa, with bilingual mappings learnt from a minimal amount of clean parallel data for scoring the parallelism of the extracted sentence pairs. The translation quality of the neural machine translation systems trained and fine-tuned on the parallel data extracted by our submissions improved significantly when compared to the organizers{'} LASER-based baseline, a sentence-embedding method that worked well last year. For re-aligning the sentences in the document pairs (component 1), our statistical approach has outperformed the current state-of-the-art neural approach in this low-resource context.",
}
@inproceedings{baek-etal-2020-patquest,
    title = "{PATQUEST}: Papago Translation Quality Estimation",
    author = "Baek, Yujin  and
      Kim, Zae Myung  and
      Moon, Jihyung  and
      Kim, Hyunjoong  and
      Park, Eunjeong",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.113",
    pages = "991--998",
    abstract = "This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score).",
}
@inproceedings{hu-etal-2020-niutrans-system,
    title = "The {N}iu{T}rans System for the {WMT}20 Quality Estimation Shared Task",
    author = "Hu, Chi  and
      Liu, Hui  and
      Feng, Kai  and
      Xu, Chen  and
      Xu, Nuo  and
      Zhou, Zefan  and
      Yan, Shiqin  and
      Luo, Yingfeng  and
      Wang, Chenglong  and
      Meng, Xia  and
      Xiao, Tong  and
      Zhu, Jingbo",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.117",
    pages = "1018--1023",
    abstract = "This paper describes the submissions of the NiuTrans Team to the WMT 2020 Quality Estimation Shared Task. We participated in all tasks and all language pairs. We explored the combination of transfer learning, multi-task learning and model ensemble. Results on multiple tasks show that deep transformer machine translation models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task.",
}
@inproceedings{lee-2020-two,
    title = "Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation",
    author = "Lee, Dongjun",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.118",
    pages = "1024--1028",
    abstract = "In this paper, we describe the Bering Lab{'}s submission to the WMT 2020 Shared Task on Quality Estimation (QE). For word-level and sentence-level translation quality estimation, we fine-tune XLM-RoBERTa, the state-of-the-art cross-lingual language model, with a few additional parameters. Model training consists of two phases. We first pre-train our model on a huge artificially generated QE dataset, and then we fine-tune the model with a human-labeled dataset. When evaluated on the WMT 2020 English-German QE test set, our systems achieve the best result on the target-side of word-level QE and the second best results on the source-side of word-level QE and sentence-level QE among all submissions.",
}
@inproceedings{zhou-etal-2020-zero,
    title = "Zero-Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns",
    author = "Zhou, Lei  and
      Ding, Liang  and
      Takeda, Koichi",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.125",
    pages = "1068--1074",
    abstract = "This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment, Quality Estimation (QE). In this study, we empirically reveal the mismatching issue when directly adopting BERTScore (Zhang et al., 2020) to QE. Specifically, there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity. In response to this issue, we propose to expose explicit cross lingual patterns, e.g. word alignments and generation score, to our proposed zero-shot models. Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue, thereby improving the performance. Encouragingly, our zero-shot QE method could achieve comparable performance with supervised QE method, and even outperforms the supervised counterpart on 2 out of 6 directions. We expect our work could shed light on the zero-shot QE model improvement.",
}
@inproceedings{wan-etal-2020-incorporating,
    title = "Incorporating Terminology Constraints in Automatic Post-Editing",
    author = "Wan, David  and
      Kedzie, Chris  and
      Ladhak, Faisal  and
      Carpuat, Marine  and
      McKeown, Kathleen",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.141",
    pages = "1193--1204",
    abstract = "Users of machine translation (MT) may want to ensure the use of specific lexical terminologies. While there exist techniques for incorporating terminology constraints during inference for MT, current APE approaches cannot ensure that they will appear in the final translation. In this paper, we present both autoregressive and non-autoregressive models for lexically constrained APE, demonstrating that our approach enables preservation of 95{\%} of the terminologies and also improves translation quality on English-German benchmarks. Even when applied to lexically constrained MT output, our approach is able to improve preservation of the terminologies. However, we show that our models do not learn to copy constraints systematically and suggest a simple data augmentation technique that leads to improved performance and robustness.",
}
@inproceedings{basta-etal-2020-towards,
    title = "Towards Mitigating Gender Bias in a decoder-based Neural Machine Translation model by Adding Contextual Information",
    author = "Basta, Christine  and
      Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    editor = "Cunha, Rossana  and
      Shaikh, Samira  and
      Varis, Erika  and
      Georgi, Ryan  and
      Tsai, Alicia  and
      Anastasopoulos, Antonios  and
      Chandu, Khyathi Raghavi",
    booktitle = "Proceedings of the The Fourth Widening Natural Language Processing Workshop",
    month = jul,
    year = "2020",
    address = "Seattle, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.winlp-1.25",
    doi = "10.18653/v1/2020.winlp-1.25",
    pages = "99--102",
    abstract = "Gender bias negatively impacts many natural language processing applications, including machine translation (MT). The motivation behind this work is to study whether recent proposed MT techniques are significantly contributing to attenuate biases in document-level and gender-balanced data. For the study, we consider approaches of adding the previous sentence and the speaker information, implemented in a decoder-based neural MT system. We show improvements both in translation quality (+1 BLEU point) as well as in gender bias mitigation on WinoMT (+5{\%} accuracy).",
}
@inproceedings{ahmadnia-aranovich-2020-effective,
    title = "An Effective Optimization Method for Neural Machine Translation: The Case of {E}nglish-{P}ersian Bilingually Low-Resource Scenario",
    author = "Ahmadnia, Benyamin  and
      Aranovich, Raul",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wat-1.2",
    pages = "45--49",
    abstract = "In this paper, we propose a useful optimization method for low-resource Neural Machine Translation (NMT) by investigating the effectiveness of multiple neural network optimization algorithms. Our results confirm that applying the proposed optimization method on English-Persian translation can exceed translation quality compared to the English-Persian Statistical Machine Translation (SMT) paradigm.",
}
@inproceedings{imamura-sumita-2020-transformer,
    title = "Transformer-based Double-token Bidirectional Autoregressive Decoding in Neural Machine Translation",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wat-1.3",
    pages = "50--57",
    abstract = "This paper presents a simple method that extends a standard Transformer-based autoregressive decoder, to speed up decoding. The proposed method generates a token from the head and tail of a sentence (two tokens in total) in each step. By simultaneously generating multiple tokens that rarely depend on each other, the decoding speed is increased while the degradation in translation quality is minimized. In our experiments, the proposed method increased the translation speed by around 113{\%}-155{\%} in comparison with a standard autoregressive decoder, while degrading the BLEU scores by no more than 1.03. It was faster than an iterative non-autoregressive decoder in many conditions.",
}
@inproceedings{dabre-chakrabarty-2020-nicts,
    title = "{NICT}{`}s Submission To {WAT} 2020: How Effective Are Simple Many-To-Many Neural Machine Translation Models?",
    author = "Dabre, Raj  and
      Chakrabarty, Abhisek",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wat-1.9",
    pages = "98--102",
    abstract = "In this paper we describe our team{`}s (NICT-5) Neural Machine Translation (NMT) models whose translations were submitted to shared tasks of the 7th Workshop on Asian Translation. We participated in the Indic language multilingual sub-task as well as the NICT-SAP multilingual multi-domain sub-task. We focused on naive many-to-many NMT models which gave reasonable translation quality despite their simplicity. Our observations are twofold: (a.) Many-to-many models suffer from a lack of consistency where the translation quality for some language pairs is very good but for some others it is terrible when compared against one-to-many and many-to-one baselines. (b.) Oversampling smaller corpora does not necessarily give the best translation quality for the language pair associated with that pair.",
}
@inproceedings{laskar-etal-2020-multimodal,
    title = "Multimodal Neural Machine Translation for {E}nglish to {H}indi",
    author = "Laskar, Sahinur Rahman  and
      Khilji, Abdullah Faiz Ur Rahman  and
      Pakray, Partha  and
      Bandyopadhyay, Sivaji",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wat-1.11",
    pages = "109--113",
    abstract = "Machine translation (MT) focuses on the automatic translation of text from one natural language to another natural language. Neural machine translation (NMT) achieves state-of-the-art results in the task of machine translation because of utilizing advanced deep learning techniques and handles issues like long-term dependency, and context-analysis. Nevertheless, NMT still suffers low translation quality for low resource languages. To encounter this challenge, the multi-modal concept comes in. The multi-modal concept combines textual and visual features to improve the translation quality of low resource languages. Moreover, the utilization of monolingual data in the pre-training step can improve the performance of the system for low resource language translations. Workshop on Asian Translation 2020 (WAT2020) organized a translation task for multimodal translation in English to Hindi. We have participated in the same in two-track submission, namely text-only and multi-modal translation with team name CNLP-NITS. The evaluated results are declared at the WAT2020 translation task, which reports that our multi-modal NMT system attained higher scores than our text-only NMT on both challenge and evaluation test set. For the challenge test data, our multi-modal neural machine translation system achieves Bilingual Evaluation Understudy (BLEU) score of 33.57, Rank-based Intuitive Bilingual Evaluation Score (RIBES) 0.754141, Adequacy-Fluency Metrics (AMFM) score 0.787320 and for evaluation test data, BLEU, RIBES, and, AMFM score of 40.51, 0.803208, and 0.820980 for English to Hindi translation respectively.",
}
@inproceedings{jooste-etal-2020-adapt,
    title = "The {ADAPT} Centre{'}s Neural {MT} Systems for the {WAT} 2020 Document-Level Translation Task",
    author = "Jooste, Wandri  and
      Haque, Rejwanul  and
      Way, Andy",
    editor = "Nakazawa, Toshiaki  and
      Nakayama, Hideki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Pa, Win Pa  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya  and
      Manabe, Hiroshi  and
      Sudoh, Katsuhito  and
      Kurohashi, Sadao  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 7th Workshop on Asian Translation",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wat-1.17",
    pages = "142--146",
    abstract = "In this paper we describe the ADAPT Centre{'}s submissions to the WAT 2020 document-level Business Scene Dialogue (BSD) Translation task. We only consider translating from Japanese to English for this task and we use the MarianNMT toolkit to train Transformer models. In order to improve the translation quality, we made use of both in-domain and out-of-domain data for training our Machine Translation (MT) systems, as well as various data augmentation techniques for fine-tuning the model parameters. This paper outlines the experiments we ran to train our systems and report the accuracy achieved through these various experiments.",
}
@article{marie-fujita-2020-synthesizing,
    title = "Synthesizing Parallel Data of User-Generated Texts with Zero-Shot Neural Machine Translation",
    author = "Marie, Benjamin  and
      Fujita, Atsushi",
    editor = "Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.46",
    doi = "10.1162/tacl_a_00341",
    pages = "710--725",
    abstract = "Neural machine translation (NMT) systems are usually trained on clean parallel data. They can perform very well for translating clean in-domain texts. However, as demonstrated by previous work, the translation quality significantly worsens when translating noisy texts, such as user-generated texts (UGT) from online social media. Given the lack of parallel data of UGT that can be used to train or adapt NMT systems, we synthesize parallel data of UGT, exploiting monolingual data of UGT through crosslingual language model pre-training and zero-shot NMT systems. This paper presents two different but complementary approaches: One alters given clean parallel data into UGT-like parallel data whereas the other generates translations from monolingual data of UGT. On the MTNT translation tasks, we show that our synthesized parallel data can lead to better NMT systems for UGT while making them more robust in translating texts from various domains and styles.",
}
@inproceedings{hsu-etal-2020-efficient,
    title = "Efficient Inference For Neural Machine Translation",
    author = "Hsu, Yi-Te  and
      Garg, Sarthak  and
      Liao, Yi-Hsiu  and
      Chatsviorkin, Ilya",
    editor = "Moosavi, Nafise Sadat  and
      Fan, Angela  and
      Shwartz, Vered  and
      Glava{\v{s}}, Goran  and
      Joty, Shafiq  and
      Wang, Alex  and
      Wolf, Thomas",
    booktitle = "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sustainlp-1.7",
    doi = "10.18653/v1/2020.sustainlp-1.7",
    pages = "48--53",
    abstract = "Large Transformer models have achieved state-of-the-art results in neural machine translation and have become standard in the field. In this work, we look for the optimal combination of known techniques to optimize inference speed without sacrificing translation quality. We conduct an empirical study that stacks various approaches and demonstrates that combination of replacing decoder self-attention with simplified recurrent units, adopting a deep encoder and a shallow decoder architecture and multi-head attention pruning can achieve up to 109{\%} and 84{\%} speedup on CPU and GPU respectively and reduce the number of parameters by 25{\%} while maintaining the same translation quality in terms of BLEU.",
}
@inproceedings{miyazaki-etal-2020-machine,
    title = "Machine Translation from Spoken Language to Sign Language using Pre-trained Language Model as Encoder",
    author = "Miyazaki, Taro  and
      Morita, Yusuke  and
      Sano, Masanori",
    editor = "Efthimiou, Eleni  and
      Fotinea, Stavroula-Evita  and
      Hanke, Thomas  and
      Hochgesang, Julie A.  and
      Kristoffersen, Jette  and
      Mesch, Johanna",
    booktitle = "Proceedings of the LREC2020 9th Workshop on the Representation and Processing of Sign Languages: Sign Language Resources in the Service of the Language Community, Technological Challenges and Application Perspectives",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/2020.signlang-1.23",
    pages = "139--144",
    abstract = "Sign language is the first language for those who were born deaf or lost their hearing in early childhood, so such individuals require services provided with sign language. To achieve flexible open-domain services with sign language, machine translations into sign language are needed. Machine translations generally require large-scale training corpora, but there are only small corpora for sign language. To overcome this data-shortage scenario, we developed a method that involves using a pre-trained language model of spoken language as the initial model of the encoder of the machine translation model. We evaluated our method by comparing it to baseline methods, including phrase-based machine translation, using only 130,000 phrase pairs of training data. Our method outperformed the baseline method, and we found that one of the reasons of translation error is from pointing, which is a special feature used in sign language. We also conducted trials to improve the translation quality for pointing. The results are somewhat disappointing, so we believe that there is still room for improving translation quality, especially for pointing.",
    language = "English",
    ISBN = "979-10-95546-54-2",
}
@inproceedings{dabre-etal-2020-balancing,
    title = "Balancing Cost and Benefit with Tied-Multi Transformers",
    author = "Dabre, Raj  and
      Rubino, Raphael  and
      Fujita, Atsushi",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Heafield, Kenneth  and
      Junczys-Dowmunt, Marcin  and
      Konstas, Ioannis  and
      Li, Xian  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.3",
    doi = "10.18653/v1/2020.ngt-1.3",
    pages = "24--34",
    abstract = "We propose a novel procedure for training multiple Transformers with tied parameters which compresses multiple models into one enabling the dynamic choice of the number of encoder and decoder layers during decoding. In training an encoder-decoder model, typically, the output of the last layer of the N-layer encoder is fed to the M-layer decoder, and the output of the last decoder layer is used to compute loss. Instead, our method computes a single loss consisting of NxM losses, where each loss is computed from the output of one of the M decoder layers connected to one of the N encoder layers. Such a model subsumes NxM models with different number of encoder and decoder layers, and can be used for decoding with fewer than the maximum number of encoder and decoder layers. Given our flexible tied model, we also address to a-priori selection of the number of encoder and decoder layers for faster decoding, and explore recurrent stacking of layers and knowledge distillation for model compression. We present a cost-benefit analysis of applying the proposed approaches for neural machine translation and show that they reduce decoding costs while preserving translation quality.",
}
@inproceedings{hu-etal-2020-niutrans,
    title = "The {N}iu{T}rans System for {WNGT} 2020 Efficiency Task",
    author = "Hu, Chi  and
      Li, Bei  and
      Li, Yinqiao  and
      Lin, Ye  and
      Li, Yanyang  and
      Wang, Chenglong  and
      Xiao, Tong  and
      Zhu, Jingbo",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Heafield, Kenneth  and
      Junczys-Dowmunt, Marcin  and
      Konstas, Ioannis  and
      Li, Xian  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.24",
    doi = "10.18653/v1/2020.ngt-1.24",
    pages = "204--210",
    abstract = "This paper describes the submissions of the NiuTrans Team to the WNGT 2020 Efficiency Shared Task. We focus on the efficient implementation of deep Transformer models (Wang et al., 2019; Li et al., 2019) using NiuTensor, a flexible toolkit for NLP tasks. We explored the combination of deep encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018.",
}
@inproceedings{ul-haq-etal-2020-improving,
    title = "Improving Document-Level Neural Machine Translation with Domain Adaptation",
    author = "Ul Haq, Sami  and
      Abdul Rauf, Sadaf  and
      Shoukat, Arslan  and
      Hira, Noor-e-",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Heafield, Kenneth  and
      Junczys-Dowmunt, Marcin  and
      Konstas, Ioannis  and
      Li, Xian  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.ngt-1.27",
    doi = "10.18653/v1/2020.ngt-1.27",
    pages = "225--231",
    abstract = "Recent studies have shown that translation quality of NMT systems can be improved by providing document-level contextual information. In general sentence-based NMT models are extended to capture contextual information from large-scale document-level corpora which are difficult to acquire. Domain adaptation on the other hand promises adapting components of already developed systems by exploiting limited in-domain data. This paper presents FJWU{'}s system submission at WNGT, we specifically participated in Document level MT task for German-English translation. Our system is based on context-aware Transformer model developed on top of original NMT architecture by integrating contextual information using attention networks. Our experimental results show providing previous sentences as context significantly improves the BLEU score as compared to a strong NMT baseline. We also studied the impact of domain adaptation on document level translationand were able to improve results by adaptingthe systems according to the testing domain.",
}
@inproceedings{yuan-sharoff-2020-sentence,
    title = "Sentence Level Human Translation Quality Estimation with Attention-based Neural Networks",
    author = "Yuan, Yu  and
      Sharoff, Serge",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.229",
    pages = "1858--1865",
    abstract = "This paper explores the use of Deep Learning methods for automatic estimation of quality of human translations. Automatic estimation can provide useful feedback for translation teaching, examination and quality control. Conventional methods for solving this task rely on manually engineered features and external knowledge. This paper presents an end-to-end neural model without feature engineering, incorporating a cross attention mechanism to detect which parts in sentence pairs are most relevant for assessing quality. Another contribution concerns oprediction of fine-grained scores for measuring different aspects of translation quality, such as terminological accuracy or idiomatic writing. Empirical results on a large human annotated dataset show that the neural model outperforms feature-based methods significantly. The dataset and the tools are available.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{amjad-etal-2020-data,
    title = "Data Augmentation using Machine Translation for Fake News Detection in the {U}rdu Language",
    author = "Amjad, Maaz  and
      Sidorov, Grigori  and
      Zhila, Alisa",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.309",
    pages = "2537--2542",
    abstract = "The task of fake news detection is to distinguish legitimate news articles that describe real facts from those which convey deceiving and fictitious information. As the fake news phenomenon is omnipresent across all languages, it is crucial to be able to efficiently solve this problem for languages other than English. A common approach to this task is supervised classification using features of various complexity. Yet supervised machine learning requires substantial amount of annotated data. For English and a small number of other languages, annotated data availability is much higher, whereas for the vast majority of languages, it is almost scarce. We investigate whether machine translation at its present state could be successfully used as an automated technique for annotated corpora creation and augmentation for fake news detection focusing on the English-Urdu language pair. We train a fake news classifier for Urdu on (1) the manually annotated dataset originally in Urdu and (2) the machine-translated version of an existing annotated fake news dataset originally in English. We show that at the present state of machine translation quality for the English-Urdu language pair, the fully automated data augmentation through machine translation did not provide improvement for fake news detection in Urdu.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{choe-etal-2020-word2word,
    title = "word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs",
    author = "Choe, Yo Joong  and
      Park, Kyubyong  and
      Kim, Dongwoo",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.371",
    pages = "3036--3045",
    abstract = "We present word2word, a publicly available dataset and an open-source Python package for cross-lingual word translations extracted from sentence-level parallel corpora. Our dataset provides top-k word translations in 3,564 (directed) language pairs across 62 languages in OpenSubtitles2018 (Lison et al., 2018). To obtain this dataset, we use a count-based bilingual lexicon extraction model based on the observation that not only source and target words but also source words themselves can be highly correlated. We illustrate that the resulting bilingual lexicons have high coverage and attain competitive translation quality for several language pairs. We wrap our dataset and model in an easy-to-use Python library, which supports downloading and retrieving top-k word translations in any of the supported language pairs as well as computing top-k word translations for custom parallel corpora.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{mino-etal-2020-content,
    title = "Content-Equivalent Translated Parallel News Corpus and Extension of Domain Adaptation for {NMT}",
    author = "Mino, Hideya  and
      Tanaka, Hideki  and
      Ito, Hitoshi  and
      Goto, Isao  and
      Yamada, Ichiro  and
      Tokunaga, Takenobu",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.445",
    pages = "3616--3622",
    abstract = "In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of parallel corpora. Neural machine translation (NMT) systems suffer degraded performance when trained with noisy data. Because there is no clean Japanese-English parallel data for news articles, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different features, such as noisy data and clean data. Though the existing methods try to overcome this problem by using tags for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our corpus increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{raganato-etal-2020-evaluation,
    title = "An Evaluation Benchmark for Testing the Word Sense Disambiguation Capabilities of Machine Translation Systems",
    author = {Raganato, Alessandro  and
      Scherrer, Yves  and
      Tiedemann, J{\"o}rg},
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.452",
    pages = "3668--3675",
    abstract = "Lexical ambiguity is one of the many challenging linguistic phenomena involved in translation, i.e., translating an ambiguous word with its correct sense. In this respect, previous work has shown that the translation quality of neural machine translation systems can be improved by explicitly modeling the senses of ambiguous words. Recently, several evaluation test sets have been proposed to measure the word sense disambiguation (WSD) capability of machine translation systems. However, to date, these evaluation test sets do not include any training data that would provide a fair setup measuring the sense distributions present within the training data itself. In this paper, we present an evaluation benchmark on WSD for machine translation for 10 language pairs, comprising training data with known sense distributions. Our approach for the construction of the benchmark builds upon the wide-coverage multilingual sense inventory of BabelNet, the multilingual neural parsing pipeline TurkuNLP, and the OPUS collection of translated texts from the web. The test suite is available at \url{http://github.com/Helsinki-NLP/MuCoW}.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{mao-etal-2020-jass,
    title = "{JASS}: {J}apanese-specific Sequence to Sequence Pre-training for Neural Machine Translation",
    author = "Mao, Zhuoyuan  and
      Cromieres, Fabien  and
      Dabre, Raj  and
      Song, Haiyue  and
      Kurohashi, Sadao",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.454",
    pages = "3683--3691",
    abstract = "Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese{--}English and News Commentary Japanese{--}Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{ive-etal-2020-post,
    title = "A Post-Editing Dataset in the Legal Domain: Do we Underestimate Neural Machine Translation Quality?",
    author = "Ive, Julia  and
      Specia, Lucia  and
      Szoc, Sara  and
      Vanallemeersch, Tom  and
      Van den Bogaert, Joachim  and
      Farah, Eduardo  and
      Maroti, Christine  and
      Ventura, Artur  and
      Khalilov, Maxim",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.455",
    pages = "3692--3697",
    abstract = "We introduce a machine translation dataset for three pairs of languages in the legal domain with post-edited high-quality neural machine translation and independent human references. The data was collected as part of the EU APE-QUEST project and comprises crawled content from EU websites with translation from English into three European languages: Dutch, French and Portuguese. Altogether, the data consists of around 31K tuples including a source sentence, the respective machine translation by a neural machine translation system, a post-edited version of such translation by a professional translator, and - where available - the original reference translation crawled from parallel language websites. We describe the data collection process, provide an analysis of the resulting post-edits and benchmark the data using state-of-the-art quality estimation and automatic post-editing models. One interesting by-product of our post-editing analysis suggests that neural systems built with publicly available general domain data can provide high-quality translations, even though comparison to human references suggests that this quality is quite low. This makes our dataset a suitable candidate to test evaluation metrics. The data is freely available as an ELRC-SHARE resource.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{fonteyne-etal-2020-literary,
    title = "Literary Machine Translation under the Magnifying Glass: Assessing the Quality of an {NMT}-Translated Detective Novel on Document Level",
    author = "Fonteyne, Margot  and
      Tezcan, Arda  and
      Macken, Lieve",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.468",
    pages = "3790--3798",
    abstract = "Several studies (covering many language pairs and translation tasks) have demonstrated that translation quality has improved enormously since the emergence of neural machine translation systems. This raises the question whether such systems are able to produce high-quality translations for more creative text types such as literature and whether they are able to generate coherent translations on document level. Our study aimed to investigate these two questions by carrying out a document-level evaluation of the raw NMT output of an entire novel. We translated Agatha Christie{'}s novel The Mysterious Affair at Styles with Google{'}s NMT system from English into Dutch and annotated it in two steps: first all fluency errors, then all accuracy errors. We report on the overall quality, determine the remaining issues, compare the most frequent error types to those in general-domain MT, and investigate whether any accuracy and fluency errors co-occur regularly. Additionally, we assess the inter-annotator agreement on the first chapter of the novel.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{etchegoyhen-gete-2020-handle,
    title = "Handle with Care: A Case Study in Comparable Corpora Exploitation for Neural Machine Translation",
    author = "Etchegoyhen, Thierry  and
      Gete, Harritxu",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.469",
    pages = "3799--3807",
    abstract = "We present the results of a case study in the exploitation of comparable corpora for Neural Machine Translation. A large comparable corpus for Basque-Spanish was prepared, on the basis of independently-produced news by the Basque public broadcaster EiTB, and we discuss the impact of various techniques to exploit the original data in order to determine optimal variants of the corpus. In particular, we show that filtering in terms of alignment thresholds and length-difference outliers has a significant impact on translation quality. The impact of tags identifying comparable data in the training datasets is also evaluated, with results indicating that this technique might be useful to help the models discriminate noisy information, in the form of informational imbalance between aligned sentences. The final corpus was prepared according to the experimental results and is made available to the scientific community for research purposes.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{zouhar-bojar-2020-outbound,
    title = "Outbound Translation User Interface Ptakop{\v{e}}t: A Pilot Study",
    author = "Zouhar, Vil{\'e}m  and
      Bojar, Ond{\v{r}}ej",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.860",
    pages = "6967--6975",
    abstract = "It is not uncommon for Internet users to have to produce a text in a foreign language they have very little knowledge of and are unable to verify the translation quality. We call the task {``}outbound translation{''} and explore it by introducing an open-source modular system Ptakop{\v{e}}t. Its main purpose is to inspect human interaction with MT systems enhanced with additional subsystems, such as backward translation and quality estimation. We follow up with an experiment on (Czech) human annotators tasked to produce questions in a language they do not speak (German), with the help of Ptakop{\v{e}}t. We focus on three real-world use cases (communication with IT support, describing administrative issues and asking encyclopedic questions) from which we gain insight into different strategies users take when faced with outbound translation tasks. Round trip translation is known to be unreliable for evaluating MT systems but our experimental evaluation documents that it works very well for users, at least on MT systems of mid-range quality.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}
@inproceedings{baliber-etal-2020-bridging,
    title = "Bridging {P}hilippine Languages With Multilingual Neural Machine Translation",
    author = "Baliber, Renz Iver  and
      Cheng, Charibeth  and
      Adlaon, Kristine Mae  and
      Mamonong, Virgion",
    editor = "Karakanta, Alina  and
      Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Abbott, Jade  and
      Ortega, John  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Lakew, Surafel Melaku  and
      Pirinen, Tommi A  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.loresmt-1.2",
    pages = "14--22",
    abstract = "The Philippines is home to more than 150 languages that is considered to be low-resourced even on its major languages. This results into a lack of pursuit in developing a translation system for the underrepresented languages. To simplify the process of developing translation system for multiple languages, and to aid in improving the translation quality of zero to low-resource languages, multilingual NMT became an active area of research. However, existing works in multilingual NMT disregards the analysis of a multilingual model on a closely related and low-resource language group in the context of pivot-based translation and zero-shot translation. In this paper, we benchmarked translation for several Philippine Languages, provided an analysis of a multilingual NMT system for morphologically rich and low-resource languages in terms of its effectiveness in translating zero-resource languages with zero-shot translations. To further evaluate the capability of the multilingual NMT model in translating unseen language pairs in training, we tested the model to translate between Tagalog and Cebuano and compared its performance with a simple NMT model that is directly trained on a parallel Tagalog and Cebuano data in which we showed that zero-shot translation outperforms a directly trained model in some instances, while utilizing English as a pivot language in translating outperform both approaches.",
}
@inproceedings{poncelas-etal-2020-using,
    title = "Using Multiple Subwords to Improve {E}nglish-{E}speranto Automated Literary Translation Quality",
    author = "Poncelas, Alberto  and
      Buts, Jan  and
      Hadley, James  and
      Way, Andy",
    editor = "Karakanta, Alina  and
      Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Abbott, Jade  and
      Ortega, John  and
      Washington, Jonathan  and
      Oco, Nathaniel  and
      Lakew, Surafel Melaku  and
      Pirinen, Tommi A  and
      Malykh, Valentin  and
      Logacheva, Varvara  and
      Zhao, Xiaobing",
    booktitle = "Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.loresmt-1.14",
    pages = "108--117",
    abstract = "Building Machine Translation (MT) systems for low-resource languages remains challenging. For many language pairs, parallel data are not widely available, and in such cases MT models do not achieve results comparable to those seen with high-resource languages. When data are scarce, it is of paramount importance to make optimal use of the limited material available. To that end, in this paper we propose employing the same parallel sentences multiple times, only changing the way the words are split each time. For this purpose we use several Byte Pair Encoding models, with various merge operations used in their configuration. In our experiments, we use this technique to expand the available data and improve an MT system involving a low-resource language pair, namely English-Esperanto. As an additional contribution, we made available a set of English-Esperanto parallel data in the literary domain.",
}
@inproceedings{pham-etal-2020-kits,
    title = "{KIT}{'}s {IWSLT} 2020 {SLT} Translation System",
    author = {Pham, Ngoc-Quan  and
      Schneider, Felix  and
      Nguyen, Tuan-Nam  and
      Ha, Thanh-Le  and
      Nguyen, Thai Son  and
      Awiszus, Maximilian  and
      St{\"u}ker, Sebastian  and
      Waibel, Alexander},
    editor = {Federico, Marcello  and
      Waibel, Alex  and
      Knight, Kevin  and
      Nakamura, Satoshi  and
      Ney, Hermann  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Wu, Dekai  and
      Mariani, Joseph  and
      Yvon, Francois},
    booktitle = "Proceedings of the 17th International Conference on Spoken Language Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.iwslt-1.4",
    doi = "10.18653/v1/2020.iwslt-1.4",
    pages = "55--61",
    abstract = "This paper describes KIT{'}s submissions to the IWSLT2020 Speech Translation evaluation campaign. We first participate in the simultaneous translation task, in which our simultaneous models are Transformer based and can be efficiently trained to obtain low latency with minimized compromise in quality. On the offline speech translation task, we applied our new Speech Transformer architecture to end-to-end speech translation. The obtained model can provide translation quality which is competitive to a complicated cascade. The latter still has the upper hand, thanks to the ability to transparently access to the transcription, and resegment the inputs to avoid fragmentation.",
}
@inproceedings{zhang-2020-review,
    title = "A Review of Discourse-level Machine Translation",
    author = "Zhang, Xiaojun",
    editor = "Liu, Qun  and
      Xiong, Deyi  and
      Ge, Shili  and
      Zhang, Xiaojun",
    booktitle = "Proceedings of the Second International Workshop of Discourse Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.iwdp-1.2",
    pages = "4--12",
    abstract = "Machine translation (MT) models usually translate a text at sentence level by considering isolated sentences, which is based on a strict assumption that the sentences in a text are independent of one another. However, the fact is that the texts at discourse level have properties going beyond individual sentences. These properties reveal texts in the frequency and distribution of words, word senses, referential forms and syntactic structures. Dissregarding dependencies across sentences will harm translation quality especially in terms of coherence, cohesion, and consistency. To solve these problems, several approaches have previously been investigated for conventional statistical machine translation (SMT). With the fast growth of neural machine translation (NMT), discourse-level NMT has drawn increasing attention from researchers. In this work, we review major works on addressing discourse related problems for both SMT and NMT models with a survey of recent trends in the fields.",
}
@inproceedings{haque-etal-2020-terminology,
    title = "Terminology-Aware Sentence Mining for {NMT} Domain Adaptation: {ADAPT}{'}s Submission to the Adap-{MT} 2020 {E}nglish-to-{H}indi {AI} Translation Shared Task",
    author = "Haque, Rejwanul  and
      Moslem, Yasmin  and
      Way, Andy",
    editor = "Sharma, Dipti Misra  and
      Ekbal, Asif  and
      Arora, Karunesh  and
      Naskar, Sudip Kumar  and
      Ganguly, Dipankar  and
      L, Sobha  and
      Mamidi, Radhika  and
      Arora, Sunita  and
      Mishra, Pruthwik  and
      Mujadia, Vandan",
    booktitle = "Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task",
    month = dec,
    year = "2020",
    address = "Patna, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2020.icon-adapmt.4",
    pages = "17--23",
    abstract = "This paper describes the ADAPT Centre{'}s submission to the Adap-MT 2020 AI Translation Shared Task for English-to-Hindi. The neural machine translation (NMT) systems that we built to translate AI domain texts are state-of-the-art Transformer models. In order to improve the translation quality of our NMT systems, we made use of both in-domain and out-of-domain data for training and employed different fine-tuning techniques for adapting our NMT systems to this task, e.g. mixed fine-tuning and on-the-fly self-training. For this, we mined parallel sentence pairs and monolingual sentences from large out-of-domain data, and the mining process was facilitated through automatic extraction of terminology from the in-domain data. This paper outlines the experiments we carried out for this task and reports the performance of our NMT systems on the evaluation test set.",
}
@inproceedings{prato-etal-2020-fully,
    title = "Fully Quantized Transformer for Machine Translation",
    author = "Prato, Gabriele  and
      Charlaix, Ella  and
      Rezagholizadeh, Mehdi",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.1",
    doi = "10.18653/v1/2020.findings-emnlp.1",
    pages = "1--14",
    abstract = "State-of-the-art neural machine translation methods employ massive amounts of parameters. Drastically reducing computational costs of such methods without affecting performance has been up to this point unsuccessful. To this end, we propose FullyQT: an all-inclusive quantization strategy for the Transformer. To the best of our knowledge, we are the first to show that it is possible to avoid any loss in translation quality with a fully quantized Transformer. Indeed, compared to full-precision, our 8-bit models score greater or equal BLEU on most tasks. Comparing ourselves to all previously proposed methods, we achieve state-of-the-art quantization results.",
}
@inproceedings{raganato-etal-2020-fixed,
    title = "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Scherrer, Yves  and
      Tiedemann, J{\"o}rg},
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.49",
    doi = "10.18653/v1/2020.findings-emnlp.49",
    pages = "556--568",
    abstract = "Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed {--} non-learnable {--} attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",
}
@inproceedings{amrhein-sennrich-2020-romanization,
    title = "On {R}omanization for Model Transfer Between Scripts in Neural Machine Translation",
    author = "Amrhein, Chantal  and
      Sennrich, Rico",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.223",
    doi = "10.18653/v1/2020.findings-emnlp.223",
    pages = "2461--2469",
    abstract = "Transfer learning is a popular strategy to improve the quality of low-resource machine translation. For an optimal transfer of the embedding layer, the child and parent model should share a substantial part of the vocabulary. This is not the case when transferring to languages with a different script. We explore the benefit of romanization in this scenario. Our results show that romanization entails information loss and is thus not always superior to simpler vocabulary transfer methods, but can improve the transfer between related languages with different scripts. We compare two romanization tools and find that they exhibit different degrees of information loss, which affects translation quality. Finally, we extend romanization to the target side, showing that this can be a successful strategy when coupled with a simple deromanization model.",
}
@inproceedings{zheng-etal-2020-fluent,
    title = "Fluent and Low-latency Simultaneous Speech-to-Speech Translation with Self-adaptive Training",
    author = "Zheng, Renjie  and
      Ma, Mingbo  and
      Zheng, Baigong  and
      Liu, Kaibo  and
      Yuan, Jiahong  and
      Church, Kenneth  and
      Huang, Liang",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.349",
    doi = "10.18653/v1/2020.findings-emnlp.349",
    pages = "3928--3937",
    abstract = "Simultaneous speech-to-speech translation is an extremely challenging but widely useful scenario that aims to generate target-language speech only a few seconds behind the source-language speech. In addition, we have to continuously translate a speech of multiple sentences, but all recent solutions merely focus on the single-sentence scenario. As a result, current approaches will accumulate more and more latencies in later sentences when the speaker talks faster and introduce unnatural pauses into translated speech when the speaker talks slower. To overcome these issues, we propose Self-Adaptive Translation which flexibly adjusts the length of translations to accommodate different source speech rates. At similar levels of translation quality (as measured by BLEU), our method generates more fluent target speech latency than the baseline, in both Zh{\textless}-{\textgreater}En directions.",
}
@inproceedings{chung-etal-2020-extremely,
    title = "Extremely Low Bit Transformer Quantization for On-Device Neural Machine Translation",
    author = "Chung, Insoo  and
      Kim, Byeongwook  and
      Choi, Yoonjung  and
      Kwon, Se Jung  and
      Jeon, Yongkweon  and
      Park, Baeseong  and
      Kim, Sangha  and
      Lee, Dongsoo",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.433",
    doi = "10.18653/v1/2020.findings-emnlp.433",
    pages = "4812--4826",
    abstract = "The deployment of widely used Transformer architecture is challenging because of heavy computation load and memory overhead during inference, especially when the target device is limited in computational resources such as mobile or edge devices. Quantization is an effective technique to address such challenges. Our analysis shows that for a given number of quantization bits, each block of Transformer contributes to translation quality and inference computations in different manners. Moreover, even inside an embedding block, each word presents vastly different contributions. Correspondingly, we propose a mixed precision quantization strategy to represent Transformer weights by an extremely low number of bits (e.g., under 3 bits). For example, for each word in an embedding block, we assign different quantization bits based on statistical property. Our quantized Transformer model achieves 11.8{\mbox{$\times$}} smaller model size than the baseline model, with less than -0.5 BLEU. We achieve 8.3{\mbox{$\times$}} reduction in run-time memory footprints and 3.5{\mbox{$\times$}} speed up (Galaxy N10+) such that our proposed compression strategy enables efficient implementation for on-device NMT.",
}
@inproceedings{bremerman-etal-2020-evaluation,
    title = "On the Evaluation of Machine Translation n-best Lists",
    author = "Bremerman, Jacob  and
      Khayrallah, Huda  and
      Oard, Douglas  and
      Post, Matt",
    editor = "Eger, Steffen  and
      Gao, Yang  and
      Peyrard, Maxime  and
      Zhao, Wei  and
      Hovy, Eduard",
    booktitle = "Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.eval4nlp-1.7",
    doi = "10.18653/v1/2020.eval4nlp-1.7",
    pages = "60--68",
    abstract = "The standard machine translation evaluation framework measures the single-best output of machine translation systems. There are, however, many situations where n-best lists are needed, yet there is no established way of evaluating them. This paper establishes a framework for addressing n-best evaluation by outlining three different questions one could consider when determining how one would define a {`}good{'} n-best list and proposing evaluation measures for each question. The first and principal contribution is an evaluation measure that characterizes the translation quality of an entire n-best list by asking whether many of the valid translations are placed near the top of the list. The second is a measure that uses gold translations with preference annotations to ask to what degree systems can produce ranked lists in preference order. The third is a measure that rewards partial matches, evaluating the closeness of the many items in an n-best list to a set of many valid references. These three perspectives make clear that having access to many references can be useful when n-best evaluation is the goal.",
}
@inproceedings{lee-etal-2020-iterative,
    title = "Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation",
    author = "Lee, Jason  and
      Shu, Raphael  and
      Cho, Kyunghyun",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.73",
    doi = "10.18653/v1/2020.emnlp-main.73",
    pages = "1006--1015",
    abstract = "We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT{'}14 En→De, WMT{'}16 Ro→En and IWSLT{'}16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT{'}14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",
}
@inproceedings{wang-etal-2020-multi,
    title = "Multi-task Learning for Multilingual Neural Machine Translation",
    author = "Wang, Yiren  and
      Zhai, ChengXiang  and
      Hassan, Hany",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.75",
    doi = "10.18653/v1/2020.emnlp-main.75",
    pages = "1022--1034",
    abstract = "While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",
}
@inproceedings{gu-etal-2020-token,
    title = "Token-level Adaptive Training for Neural Machine Translation",
    author = "Gu, Shuhao  and
      Zhang, Jinchao  and
      Meng, Fandong  and
      Feng, Yang  and
      Xie, Wanying  and
      Zhou, Jie  and
      Yu, Dong",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.76",
    doi = "10.18653/v1/2020.emnlp-main.76",
    pages = "1035--1046",
    abstract = "There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT). The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution. However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected. In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training. We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens. Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively. Further analyses show that our method can also improve the lexical diversity of translation.",
}
@inproceedings{wan-etal-2020-self,
    title = "Self-Paced Learning for Neural Machine Translation",
    author = "Wan, Yu  and
      Yang, Baosong  and
      Wong, Derek F.  and
      Zhou, Yikai  and
      Chao, Lidia S.  and
      Zhang, Haibo  and
      Chen, Boxing",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.80",
    doi = "10.18653/v1/2020.emnlp-main.80",
    pages = "1074--1080",
    abstract = "Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans. Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity. We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.",
}
@inproceedings{wu-etal-2020-generating,
    title = "Generating Diverse Translation from Model Distribution with Dropout",
    author = "Wu, Xuanfu  and
      Feng, Yang  and
      Shao, Chenze",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.82",
    doi = "10.18653/v1/2020.emnlp-main.82",
    pages = "1088--1097",
    abstract = "Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation. In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference. The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling. With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled. We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.",
}
@inproceedings{libovicky-fraser-2020-towards,
    title = "Towards Reasonably-Sized Character-Level Transformer {NMT} by Finetuning Subword Systems",
    author = "Libovick{\'y}, Jind{\v{r}}ich  and
      Fraser, Alexander",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.203",
    doi = "10.18653/v1/2020.emnlp-main.203",
    pages = "2572--2579",
    abstract = "Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train. These problems can be partially overcome by incorporating a segmentation into tokens in the model. We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation. We use only the vanilla 6-layer Transformer Base architecture. Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality. Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.",
}
@inproceedings{zhang-van-genabith-2020-translation,
    title = "Translation Quality Estimation by Jointly Learning to Score and Rank",
    author = "Zhang, Jingyi  and
      van Genabith, Josef",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.205",
    doi = "10.18653/v1/2020.emnlp-main.205",
    pages = "2592--2598",
    abstract = "The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations. Supervised learning of this QE task requires human evaluation of translation quality as training data. Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations. In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations. Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models. We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.",
}
@inproceedings{lin-etal-2020-pre,
    title = "Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information",
    author = "Lin, Zehui  and
      Pan, Xiao  and
      Wang, Mingxuan  and
      Qiu, Xipeng  and
      Feng, Jiangtao  and
      Zhou, Hao  and
      Li, Lei",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.210",
    doi = "10.18653/v1/2020.emnlp-main.210",
    pages = "2649--2663",
    abstract = "We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus. Code, data, and pre-trained models are available at \url{https://github.com/linzehui/mRASP}.",
}
@inproceedings{weng-etal-2020-towards,
    title = "Towards Enhancing Faithfulness for Neural Machine Translation",
    author = "Weng, Rongxiang  and
      Yu, Heng  and
      Wei, Xiangpeng  and
      Luo, Weihua",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.212",
    doi = "10.18653/v1/2020.emnlp-main.212",
    pages = "2675--2684",
    abstract = "Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences. Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers. In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt). During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated. Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments. Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.",
}
@inproceedings{budur-etal-2020-data,
    title = "Data and {R}epresentation for {T}urkish {N}atural {L}anguage {I}nference",
    author = {Budur, Emrah  and
      {\"O}z{\c{c}}elik, R{\i}za  and
      Gungor, Tunga  and
      Potts, Christopher},
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.662",
    doi = "10.18653/v1/2020.emnlp-main.662",
    pages = "8253--8267",
    abstract = "Large annotated datasets in NLP are overwhelmingly in English. This is an obstacle to progress in other languages. Unfortunately, obtaining new annotated resources for each task in each language would be prohibitively expensive. At the same time, commercial machine translation systems are now robust. Can we leverage these systems to translate English-language datasets automatically? In this paper, we offer a positive response for natural language inference (NLI) in Turkish. We translated two large English NLI datasets into Turkish and had a team of experts validate their translation quality and fidelity to the original labels. Using these datasets, we address core issues of representation for Turkish NLI. We find that in-language embeddings are essential and that morphological parsing can be avoided where the training set is large. Finally, we show that models trained on our machine-translated datasets are successful on human-translated evaluation sets. We share all code, models, and data publicly.",
}
@inproceedings{ma-etal-2020-simuleval,
    title = "{SIMULEVAL}: An Evaluation Toolkit for Simultaneous Translation",
    author = "Ma, Xutai  and
      Dousti, Mohammad Javad  and
      Wang, Changhan  and
      Gu, Jiatao  and
      Pino, Juan",
    editor = "Liu, Qun  and
      Schlangen, David",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.19",
    doi = "10.18653/v1/2020.emnlp-demos.19",
    pages = "144--150",
    abstract = "Simultaneous translation on both text and speech focuses on a real-time and low-latency scenario where the model starts translating before reading the complete source input. Evaluating simultaneous translation models is more complex than offline models because the latency is another factor to consider in addition to translation quality. The research community, despite its growing focus on novel modeling approaches to simultaneous translation, currently lacks a universal evaluation procedure. Therefore, we present SimulEval, an easy-to-use and general evaluation toolkit for both simultaneous text and speech translation. A server-client scheme is introduced to create a simultaneous translation scenario, where the server sends source input and receives predictions for evaluation and the client executes customized policies. Given a policy, it automatically performs simultaneous decoding and collectively reports several popular latency metrics. We also adapt latency metrics from text simultaneous translation to the speech task. Additionally, SimulEval is equipped with a visualization interface to provide better understanding of the simultaneous decoding process of a system. SimulEval has already been extensively used for the IWSLT 2020 shared task on simultaneous speech translation. Code will be released upon publication.",
}
@inproceedings{kocmi-bojar-2020-efficiently,
    title = "Efficiently Reusing Old Models Across Languages via Transfer Learning",
    author = "Kocmi, Tom  and
      Bojar, Ond{\v{r}}ej",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.3",
    pages = "19--28",
    abstract = "Recent progress in neural machine translation (NMT) is directed towards larger neural networks trained on an increasing amount of hardware resources. As a result, NMT models are costly to train, both financially, due to the electricity and hardware cost, and environmentally, due to the carbon footprint. It is especially true in transfer learning for its additional cost of training the {``}parent{''} model before transferring knowledge and training the desired {``}child{''} model. In this paper, we propose a simple method of re-using an already trained model for different language pairs where there is no need for modifications in model architecture. Our approach does not need a separate parent model for each investigated language pair, as it is typical in NMT transfer learning. To show the applicability of our method, we recycle a Transformer model trained by different researchers and use it to seed models for different language pairs. We achieve better translation quality and shorter convergence times than when training from random initialization.",
}
@inproceedings{sanchez-cartagena-etal-2020-multi,
    title = "A multi-source approach for {B}reton{--}{F}rench hybrid machine translation",
    author = "S{\'a}nchez-Cartagena, V{\'\i}ctor M.  and
      Forcada, Mikel L.  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.8",
    pages = "61--70",
    abstract = "Corpus-based approaches to machine translation (MT) have difficulties when the amount of parallel corpora to use for training is scarce, especially if the languages involved in the translation are highly inflected. This problem can be addressed from different perspectives, including data augmentation, transfer learning, and the use of additional resources, such as those used in rule-based MT. This paper focuses on the hybridisation of rule-based MT and neural MT for the Breton{--}French under-resourced language pair in an attempt to study to what extent the rule-based MT resources help improve the translation quality of the neural MT system for this particular under-resourced language pair. We combine both translation approaches in a multi-source neural MT architecture and find out that, even though the rule-based system has a low performance according to automatic evaluation metrics, using it leads to improved translation quality.",
}
@inproceedings{moon-etal-2020-revisiting,
    title = "Revisiting Round-trip Translation for Quality Estimation",
    author = "Moon, Jihyung  and
      Cho, Hyunchang  and
      Park, Eunjeong L.",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.11",
    pages = "91--104",
    abstract = "Quality estimation (QE), a task of evaluating the quality of translation automatically without human-translated reference, is one of the important challenges for machine translation (MT). As the QE methods, BLEU score for round-trip translation (RTT) had been considered. However, it was found to be a poor predictor of translation quality since BLEU was not an adequate metric to detect semantic similarity between input and RTT. Recently, the pre-trained language models have made breakthroughs in many NLP tasks by providing semantically meaningful word and sentence embeddings. In this paper, we employ the semantic embeddings to RTT-based QE metric. Our method achieves the highest correlations with human judgments compared to WMT 2019 quality estimation metric task submissions. Additionally, we observe that with semantic-level metrics, RTT-based QE is robust to the choice of a backward translation system and shows consistent performance on both SMT and NMT forward translation systems.",
}
@inproceedings{zaretskaya-etal-2020-estimation,
    title = "Estimation vs Metrics: is {QE} Useful for {MT} Model Selection?",
    author = "Zaretskaya, Anna  and
      Concei{\c{c}}{\~a}o, Jos{\'e}  and
      Bane, Frederick",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.36",
    pages = "339--346",
    abstract = "This paper presents a case study of applying machine translation quality estimation (QE) for the purpose of machine translation (MT) engine selection. The goal is to understand how well the QE predictions correlate with several MT evaluation metrics (automatic and human). Our findings show that our industry-level QE system is not reliable enough for MT selection when the MT systems have similar performance. We suggest that QE can be used with more success for other tasks relevant for translation industry such as risk prevention.",
}
@inproceedings{popovic-2020-differences,
    title = "On the differences between human translations",
    author = "Popovic, Maja",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.39",
    pages = "365--374",
    abstract = "Many studies have confirmed that translated texts exhibit different features than texts originally written in the given language. This work explores texts translated by different translators taking into account expertise and native language. A set of computational analyses was conducted on three language pairs, English-Croatian, German-French and English-Finnish, and the results show that each of the factors has certain influence on the features of the translated texts, especially on sentence length and lexical richness. The results also indicate that for translations used for machine translation evaluation, it is important to specify these factors, especially if comparing machine translation quality with human translation quality is involved.",
}
@inproceedings{dowling-etal-2020-human,
    title = "A human evaluation of {E}nglish-{I}rish statistical and neural machine translation",
    author = "Dowling, Meghan  and
      Castilho, Sheila  and
      Moorkens, Joss  and
      Lynn, Teresa  and
      Way, Andy",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.46",
    pages = "431--440",
    abstract = "With official status in both Ireland and the EU, there is a need for high-quality English-Irish (EN-GA) machine translation (MT) systems which are suitable for use in a professional translation environment. While we have seen recent research on improving both statistical MT and neural MT for the EN-GA pair, the results of such systems have always been reported using automatic evaluation metrics. This paper provides the first human evaluation study of EN-GA MT using professional translators and in-domain (public administration) data for a more accurate depiction of the translation quality available via MT.",
}
@inproceedings{stasimioti-etal-2020-machine,
    title = "Machine Translation Quality: A comparative evaluation of {SMT}, {NMT} and tailored-{NMT} outputs",
    author = "Stasimioti, Maria  and
      Sosoni, Vilelmini  and
      Kermanidis, Katia  and
      Mouratidis, Despoina",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.47",
    pages = "441--450",
    abstract = "The present study aims to compare three systems: a generic statistical machine translation (SMT), a generic neural machine translation (NMT) and a tailored-NMT system focusing on the English to Greek language pair. The comparison is carried out following a mixed-methods approach, i.e. automatic metrics, as well as side-by-side ranking, adequacy and fluency rating, measurement of actual post editing (PE) effort and human error analysis performed by 16 postgraduate Translation students. The findings reveal a higher score for both the generic NMT and the tailored-NMT outputs as regards automatic metrics and human evaluation metrics, with the tailored-NMT output faring even better than the generic NMT output.",
}
@inproceedings{soares-etal-2020-qe,
    title = "{QE} Viewer: an Open-Source Tool for Visualization of Machine Translation Quality Estimation Results",
    author = "Soares, Felipe  and
      Zaretskaya, Anna  and
      Bartolome, Diego",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.48",
    pages = "453--454",
    abstract = "QE Viewer is a web-based tool for visualizing results of a Machine Translation Quality Estimation (QE) system. It allows users to see information on the predicted post-editing distance (PED) for a given file or sentence, and highlighted words that were predicted to contain MT errors. The tool can be used in a variety of academic, educational and commercial scenarios.",
}
@inproceedings{popovic-2020-qrev,
    title = "{QR}ev: Machine Translation of User Reviews: What Influences the Translation Quality?",
    author = "Popovic, Maja",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.52",
    pages = "461--462",
    abstract = "This project aims to identify the important aspects of translation quality of user reviews which will represent a starting point for developing better automatic MT metrics and challenge test sets, and will be also helpful for developing MT systems for this genre. We work on two types of reviews: Amazon products and IMDb movies, written in English and translated into two closely related target languages, Croatian and Serbian.",
}
@inproceedings{depraetere-etal-2020-ape,
    title = "{APE}-{QUEST}: an {MT} Quality Gate",
    author = "Depraetere, Heidi  and
      Van den Bogaert, Joachim  and
      Szoc, Sara  and
      Vanallemeersch, Tom",
    editor = "Martins, Andr{\'e}  and
      Moniz, Helena  and
      Fumega, Sara  and
      Martins, Bruno  and
      Batista, Fernando  and
      Coheur, Luisa  and
      Parra, Carla  and
      Trancoso, Isabel  and
      Turchi, Marco  and
      Bisazza, Arianna  and
      Moorkens, Joss  and
      Guerberof, Ana  and
      Nurminen, Mary  and
      Marg, Lena  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation",
    month = nov,
    year = "2020",
    address = "Lisboa, Portugal",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2020.eamt-1.58",
    pages = "473--474",
    abstract = "The APE-QUEST project (2018{--}2020) sets up a quality gate and crowdsourcing workflow for the eTranslation system of EC{'}s Connecting Europe Facility to improve translation quality in specific domains. It packages these services as a translation portal for machine-to-machine and machine-to-human scenarios.",
}
@inproceedings{liu-etal-2020-meet,
    title = "Meet Changes with Constancy: Learning Invariance in Multi-Source Translation",
    author = "Liu, Jianfeng  and
      Luo, Ling  and
      Ao, Xiang  and
      Song, Yan  and
      Xu, Haoran  and
      Ye, Jian",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.97",
    doi = "10.18653/v1/2020.coling-main.97",
    pages = "1122--1132",
    abstract = "Multi-source neural machine translation aims to translate from parallel sources of information (e.g. languages, images, etc.) to a single target language, which has shown better performance than most one-to-one systems. Despite the remarkable success of existing models, they usually neglect the fact that multiple source inputs may have inconsistencies. Such differences might bring noise to the task and limit the performance of existing multi-source NMT approaches due to their indiscriminate usage of input sources for target word predictions. In this paper, we attempt to leverage the potential complementary information among distinct sources and alleviate the occasional conflicts of them. To accomplish that, we propose a source invariance network to learn the invariant information of parallel sources. Such network can be easily integrated with multi-encoder based multi-source NMT methods (e.g. multi-encoder RNN and transformer) to enhance the translation results. Extensive experiments on two multi-source translation tasks demonstrate that the proposed approach not only achieves clear gains in translation quality but also captures implicit invariance between different sources.",
}
@inproceedings{araabi-monz-2020-optimizing,
    title = "Optimizing Transformer for Low-Resource Neural Machine Translation",
    author = "Araabi, Ali  and
      Monz, Christof",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.304",
    doi = "10.18653/v1/2020.coling-main.304",
    pages = "3429--3435",
    abstract = "Language pairs with limited amounts of parallel data, also known as low-resource languages, remain a challenge for neural machine translation. While the Transformer model has achieved significant improvements for many language pairs and has become the de facto mainstream architecture, its capability under low-resource conditions has not been fully investigated yet. Our experiments on different subsets of the IWSLT14 training data show that the effectiveness of Transformer under low-resource conditions is highly dependent on the hyper-parameter settings. Our experiments show that using an optimized Transformer for low-resource conditions improves the translation quality up to 7.3 BLEU points compared to using the Transformer default settings.",
}
@inproceedings{lee-etal-2020-using-bilingual,
    title = "Using Bilingual Patents for Translation Training",
    author = "Lee, John  and
      Tsou, Benjamin  and
      Cai, Tianyuan",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.309",
    doi = "10.18653/v1/2020.coling-main.309",
    pages = "3461--3466",
    abstract = "While bilingual corpora have been instrumental for machine translation, their utility for training translators has been less explored. We investigate the use of bilingual corpora as pedagogical tools for translation in the technical domain. In a user study, novice translators revised Chinese translations of English patents through bilingual concordancing. Results show that concordancing with an in-domain bilingual corpus can yield greater improvement in translation quality of technical terms than a general-domain bilingual corpus.",
}
@inproceedings{gaido-etal-2020-breeding,
    title = "Breeding Gender-aware Direct Speech Translation Systems",
    author = "Gaido, Marco  and
      Savoldi, Beatrice  and
      Bentivogli, Luisa  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.350",
    doi = "10.18653/v1/2020.coling-main.350",
    pages = "3951--3964",
    abstract = "In automatic speech translation (ST), traditional cascade approaches involving separate transcription and translation steps are giving ground to increasingly competitive and more robust direct solutions. In particular, by translating speech audio data without intermediate transcription, direct ST models are able to leverage and preserve essential information present in the input (e.g.speaker{'}s vocal characteristics) that is otherwise lost in the cascade framework. Although such ability proved to be useful for gender translation, direct ST is nonetheless affected by gender bias just like its cascade counterpart, as well as machine translation and numerous other natural language processing applications. Moreover, direct ST systems that exclusively rely on vocal biometric features as a gender cue can be unsuitable or even potentially problematic for certain users. Going beyond speech signals, in this paper we compare different approaches to inform direct ST models about the speaker{'}s gender and test their ability to handle gender translation from English into Italian and French. To this aim, we manually annotated large datasets with speak-ers{'} gender information and used them for experiments reflecting different possible real-world scenarios. Our results show that gender-aware direct ST solutions can significantly outperform strong {--} but gender-unaware {--} direct ST models. In particular, the translation of gender-marked words can increase up to 30 points in accuracy while preserving overall translation quality.",
}
@inproceedings{chakrabarty-etal-2020-improving,
    title = "Improving Low-Resource {NMT} through Relevance Based Linguistic Features Incorporation",
    author = "Chakrabarty, Abhisek  and
      Dabre, Raj  and
      Ding, Chenchen  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.376",
    doi = "10.18653/v1/2020.coling-main.376",
    pages = "4263--4274",
    abstract = "In this study, linguistic knowledge at different levels are incorporated into the neural machine translation (NMT) framework to improve translation quality for language pairs with extremely limited data. Integrating manually designed or automatically extracted features into the NMT framework is known to be beneficial. However, this study emphasizes that the relevance of the features is crucial to the performance. Specifically, we propose two methods, 1) self relevance and 2) word-based relevance, to improve the representation of features for NMT. Experiments are conducted on translation tasks from English to eight Asian languages, with no more than twenty thousand sentences for training. The proposed methods improve translation quality for all tasks by up to 3.09 BLEU points. Discussions with visualization provide the explainability of the proposed methods where we show that the relevance methods provide weights to features thereby enhancing their impact on low-resource machine translation.",
}
@inproceedings{karakanta-etal-2020-two,
    title = "The Two Shades of Dubbing in Neural Machine Translation",
    author = "Karakanta, Alina  and
      Bhattacharya, Supratik  and
      Nayak, Shravan  and
      Baumann, Timo  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.382",
    doi = "10.18653/v1/2020.coling-main.382",
    pages = "4327--4333",
    abstract = "Dubbing has two shades; synchronisation constraints are applied only when the actor{'}s mouth is visible on screen, while the translation is unconstrained for off-screen dubbing. Consequently, different synchronisation requirements, and therefore translation strategies, are applied depending on the type of dubbing. In this work, we manually annotate an existing dubbing corpus (Heroes) for this dichotomy. We show that, even though we did not observe distinctive features between on- and off-screen dubbing at the textual level, on-screen dubbing is more difficult for MT (-4 BLEU points). Moreover, synchronisation constraints dramatically decrease translation quality for off-screen dubbing. We conclude that, distinguishing between on-screen and off-screen dubbing is necessary for determining successful strategies for dubbing-customised Machine Translation.",
}
@inproceedings{rubino-sumita-2020-intermediate,
    title = "Intermediate Self-supervised Learning for Machine Translation Quality Estimation",
    author = "Rubino, Raphael  and
      Sumita, Eiichiro",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.385",
    doi = "10.18653/v1/2020.coling-main.385",
    pages = "4355--4360",
    abstract = "Pre-training sentence encoders is effective in many natural language processing tasks including machine translation (MT) quality estimation (QE), due partly to the scarcity of annotated QE data required for supervised learning. In this paper, we investigate the use of an intermediate self-supervised learning task for sentence encoder aiming at improving QE performances at the sentence and word levels. Our approach is motivated by a problem inherent to QE: mistakes in translation caused by wrongly inserted and deleted tokens. We modify the translation language model (TLM) training objective of the cross-lingual language model (XLM) to orientate the pre-trained model towards the target task. The proposed method does not rely on annotated data and is complementary to QE methods involving pre-trained sentence encoders and domain adaptation. Experiments on English-to-German and English-to-Russian translation directions show that intermediate learning improves over domain adaptated models. Additionally, our method reaches results in par with state-of-the-art QE models without requiring the combination of several approaches and outperforms similar methods based on pre-trained sentence encoders.",
}
@inproceedings{ding-etal-2020-context,
    title = "Context-Aware Cross-Attention for Non-Autoregressive Translation",
    author = "Ding, Liang  and
      Wang, Longyue  and
      Wu, Di  and
      Tao, Dacheng  and
      Tu, Zhaopeng",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.389",
    doi = "10.18653/v1/2020.coling-main.389",
    pages = "4396--4402",
    abstract = "Non-autoregressive translation (NAT) significantly accelerates the inference process by predicting the entire target sequence. However, due to the lack of target dependency modelling in the decoder, the conditional generation process heavily depends on the cross-attention. In this paper, we reveal a localness perception problem in NAT cross-attention, for which it is difficult to adequately capture source context. To alleviate this problem, we propose to enhance signals of neighbour source tokens into conventional cross-attention. Experimental results on several representative datasets show that our approach can consistently improve translation quality over strong NAT baselines. Extensive analyses demonstrate that the enhanced cross-attention achieves better exploitation of source contexts by leveraging both local and global information.",
}
@inproceedings{mino-etal-2020-effective,
    title = "Effective Use of Target-side Context for Neural Machine Translation",
    author = "Mino, Hideya  and
      Ito, Hitoshi  and
      Goto, Isao  and
      Yamada, Ichiro  and
      Tokunaga, Takenobu",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.396",
    doi = "10.18653/v1/2020.coling-main.396",
    pages = "4483--4494",
    abstract = "In this paper, we deal with two problems in Japanese-English machine translation of news articles. The first problem is the quality of parallel corpora. Neural machine translation (NMT) systems suffer degraded performance when trained with noisy data. Because there is no clean Japanese-English parallel data for news articles, we build a novel parallel news corpus consisting of Japanese news articles translated into English in a content-equivalent manner. This is the first content-equivalent Japanese-English news corpus translated specifically for training NMT systems. The second problem involves the domain-adaptation technique. NMT systems suffer degraded performance when trained with mixed data having different features, such as noisy data and clean data. Though the existing methods try to overcome this problem by using tags for distinguishing the differences between corpora, it is not sufficient. We thus extend a domain-adaptation method using multi-tags to train an NMT model effectively with the clean corpus and existing parallel news corpora with some types of noise. Experimental results show that our corpus increases the translation quality, and that our domain-adaptation method is more effective for learning with the multiple types of corpora than existing domain-adaptation methods are.",
}
@inproceedings{zhao-etal-2020-knowledge,
    title = "Knowledge Graph Enhanced Neural Machine Translation via Multi-task Learning on Sub-entity Granularity",
    author = "Zhao, Yang  and
      Xiang, Lu  and
      Zhu, Junnan  and
      Zhang, Jiajun  and
      Zhou, Yu  and
      Zong, Chengqing",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.397",
    doi = "10.18653/v1/2020.coling-main.397",
    pages = "4495--4505",
    abstract = "Previous studies combining knowledge graph (KG) with neural machine translation (NMT) have two problems: i) Knowledge under-utilization: they only focus on the entities that appear in both KG and training sentence pairs, making much knowledge in KG unable to be fully utilized. ii) Granularity mismatch: the current KG methods utilize the entity as the basic granularity, while NMT utilizes the sub-word as the granularity, making the KG different to be utilized in NMT. To alleviate above problems, we propose a multi-task learning method on sub-entity granularity. Specifically, we first split the entities in KG and sentence pairs into sub-entity granularity by using joint BPE. Then we utilize the multi-task learning to combine the machine translation task and knowledge reasoning task. The extensive experiments on various translation tasks have demonstrated that our method significantly outperforms the baseline models in both translation quality and handling the entities.",
}
@inproceedings{yao-etal-2020-domain,
    title = "Domain Transfer based Data Augmentation for Neural Query Translation",
    author = "Yao, Liang  and
      Yang, Baosong  and
      Zhang, Haibo  and
      Chen, Boxing  and
      Luo, Weihua",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.399",
    doi = "10.18653/v1/2020.coling-main.399",
    pages = "4521--4533",
    abstract = "Query translation (QT) serves as a critical factor in successful cross-lingual information retrieval (CLIR). Due to the lack of parallel query samples, neural-based QT models are usually optimized with synthetic data which are derived from large-scale monolingual queries. Nevertheless, such kind of pseudo corpus is mostly produced by a general-domain translation model, making it be insufficient to guide the learning of QT model. In this paper, we extend the data augmentation with a domain transfer procedure, thus to revise synthetic candidates to search-aware examples. Specifically, the domain transfer model is built upon advanced Transformer, in which layer coordination and mixed attention are exploited to speed up the refining process and leverage parameters from a pre-trained cross-lingual language model. In order to examine the effectiveness of the proposed method, we collected French-to-English and Spanish-to-English QT test sets, each of which consists of 10,000 parallel query pairs with careful manual-checking. Qualitative and quantitative analyses reveal that our model significantly outperforms strong baselines and the related domain transfer methods on both translation quality and retrieval accuracy.",
}
@inproceedings{elbayad-etal-2020-online,
    title = "Online Versus Offline {NMT} Quality: An In-depth Analysis on {E}nglish-{G}erman and {G}erman-{E}nglish",
    author = "Elbayad, Maha  and
      Ustaszewski, Michael  and
      Esperan{\c{c}}a-Rodier, Emmanuelle  and
      Brunet-Manquat, Francis  and
      Verbeek, Jakob  and
      Besacier, Laurent",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.443",
    doi = "10.18653/v1/2020.coling-main.443",
    pages = "5047--5058",
    abstract = "We conduct in this work an evaluation study comparing offline and online neural machine translation architectures. Two sequence-to-sequence models: convolutional Pervasive Attention (Elbayad et al. 2018) and attention-based Transformer (Vaswani et al. 2017) are considered. We investigate, for both architectures, the impact of online decoding constraints on the translation quality through a carefully designed human evaluation on English-German and German-English language pairs, the latter being particularly sensitive to latency constraints. The evaluation results allow us to identify the strengths and shortcomings of each model when we shift to the online setup.",
}
@inproceedings{ranasinghe-etal-2020-transquest,
    title = "{T}rans{Q}uest: Translation Quality Estimation with Cross-lingual Transformers",
    author = "Ranasinghe, Tharindu  and
      Orasan, Constantin  and
      Mitkov, Ruslan",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.445",
    doi = "10.18653/v1/2020.coling-main.445",
    pages = "5070--5081",
    abstract = "Recent years have seen big advances in the field of sentence-level quality estimation (QE), largely as a result of using neural-based architectures. However, the majority of these methods work only on the language pair they are trained on and need retraining for new language pairs. This process can prove difficult from a technical point of view and is usually computationally expensive. In this paper we propose a simple QE framework based on cross-lingual transformers, and we use it to implement and evaluate two different neural architectures. Our evaluation shows that the proposed methods achieve state-of-the-art results outperforming current open-source quality estimation frameworks when trained on datasets from WMT. In addition, the framework proves very useful in transfer learning settings, especially when dealing with low-resourced languages, allowing us to obtain very competitive results.",
}
@article{vazquez-etal-2020-systematic,
    title = "A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation",
    author = {V{\'a}zquez, Ra{\'u}l  and
      Raganato, Alessandro  and
      Creutz, Mathias  and
      Tiedemann, J{\"o}rg},
    journal = "Computational Linguistics",
    volume = "46",
    number = "2",
    month = jun,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-2.5",
    doi = "10.1162/coli_a_00377",
    pages = "387--424",
    abstract = "Neural machine translation has considerably improved the quality of automatic translations by learning good representations of input sentences. In this article, we explore a multilingual translation model capable of producing fixed-size sentence representations by incorporating an intermediate crosslingual shared layer, which we refer to as attention bridge. This layer exploits the semantics from each language and develops into a language-agnostic meaning representation that can be efficiently used for transfer learning. We systematically study the impact of the size of the attention bridge and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that there is no conflict between translation performance and the use of sentence representations in downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. Nevertheless, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. Similarly, we show that trainable downstream tasks benefit from multilingual models, whereas additional language signals do not improve performance in non-trainable benchmarks. This is an important insight that helps to properly design models for specific applications. Finally, we also include an in-depth analysis of the proposed attention bridge and its ability to encode linguistic properties. We carefully analyze the information that is captured by individual attention heads and identify interesting patterns that explain the performance of specific settings in linguistic probing tasks.",
}
@inproceedings{lu-etal-2020-mixed,
    title = "A Mixed Learning Objective for Neural Machine Translation",
    author = "Lu, Wenjie  and
      Zhou, Leiying  and
      Liu, Gongshen  and
      Zhang, Quanhai",
    editor = "Sun, Maosong  and
      Li, Sujian  and
      Zhang, Yue  and
      Liu, Yang",
    booktitle = "Proceedings of the 19th Chinese National Conference on Computational Linguistics",
    month = oct,
    year = "2020",
    address = "Haikou, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2020.ccl-1.90",
    pages = "974--983",
    abstract = "Evaluation discrepancy and overcorrection phenomenon are two common problems in neural machine translation (NMT). NMT models are generally trained with word-level learning objective, but evaluated by sentence-level metrics. Moreover, the cross-entropy loss function discourages model to generate synonymous predictions and overcorrect them to ground truth words. To address these two drawbacks, we adopt multi-task learning and propose a mixed learning objective (MLO) which combines the strength of word-level and sentence-level evaluation without modifying model structure. At word-level, it calculates semantic similarity between predicted and ground truth words. At sentence-level, it computes probabilistic n-gram matching scores of generated translations. We also combine a loss-sensitive scheduled sampling decoding strategy with MLO to explore its extensibility. Experimental results on IWSLT 2016 German-English and WMT 2019 English-Chinese datasets demonstrate that our methodology can significantly promote translation quality. The ablation study shows that both word-level and sentence-level learning objectives can improve BLEU scores. Furthermore, MLO is consistent with state-of-the-art scheduled sampling methods and can achieve further promotion.",
    language = "English",
}
@inproceedings{zhang-zhang-2020-dynamic,
    title = "Dynamic Sentence Boundary Detection for Simultaneous Translation",
    author = "Zhang, Ruiqing  and
      Zhang, Chuanqiang",
    editor = "Wu, Hua  and
      Cherry, Colin  and
      Huang, Liang  and
      He, Zhongjun  and
      Liberman, Mark  and
      Cross, James  and
      Liu, Yang",
    booktitle = "Proceedings of the First Workshop on Automatic Simultaneous Translation",
    month = jul,
    year = "2020",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.autosimtrans-1.1",
    doi = "10.18653/v1/2020.autosimtrans-1.1",
    pages = "1--9",
    abstract = "Simultaneous Translation is a great challenge in which translation starts before the source sentence finished. Most studies take transcription as input and focus on balancing translation quality and latency for each sentence. However, most ASR systems can not provide accurate sentence boundaries in realtime. Thus it is a key problem to segment sentences for the word streaming before translation. In this paper, we propose a novel method for sentence boundary detection that takes it as a multi-class classification task under the end-to-end pre-training framework. Experiments show significant improvements both in terms of translation quality and latency.",
}
@inproceedings{renduchintala-genzel-2020-machine,
    title = "Machine Translation quality across demographic dialectal variation in Social Media.",
    author = "Renduchintala, Adithya  and
      Genzel, Dmitriy",
    editor = "Campbell, Janice  and
      Genzel, Dmitriy  and
      Huyck, Ben  and
      O{'}Neill-Brown, Patricia",
    booktitle = "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2020.amta-user.8",
    pages = "162--189",
}
@inproceedings{casas-etal-2020-combining,
    title = "Combining Subword Representations into Word-level Representations in the Transformer Architecture",
    author = "Casas, Noe  and
      Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.10",
    doi = "10.18653/v1/2020.acl-srw.10",
    pages = "66--71",
    abstract = "In Neural Machine Translation, using word-level tokens leads to degradation in translation quality. The dominant approaches use subword-level tokens, but this increases the length of the sequences and makes it difficult to profit from word-level information such as POS tags or semantic dependencies. We propose a modification to the Transformer model to combine subword-level representations into word-level ones in the first layers of the encoder, reducing the effective length of the sequences in the following layers and providing a natural point to incorporate extra word-level information. Our experiments show that this approach maintains the translation quality with respect to the normal Transformer model when no extra word-level information is injected and that it is superior to the currently dominant method for incorporating word-level source language information to models based on subword-level vocabularies.",
}
@inproceedings{song-etal-2020-pre,
    title = "Pre-training via Leveraging Assisting Languages for Neural Machine Translation",
    author = "Song, Haiyue  and
      Dabre, Raj  and
      Mao, Zhuoyuan  and
      Cheng, Fei  and
      Kurohashi, Sadao  and
      Sumita, Eiichiro",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.37",
    doi = "10.18653/v1/2020.acl-srw.37",
    pages = "279--285",
    abstract = "Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios.",
}
@inproceedings{pandramish-sharma-2020-checkpoint,
    title = "Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems",
    author = "Pandramish, Vinay  and
      Sharma, Dipti Misra",
    editor = "Rijhwani, Shruti  and
      Liu, Jiangming  and
      Wang, Yizhong  and
      Dror, Rotem",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-srw.38",
    doi = "10.18653/v1/2020.acl-srw.38",
    pages = "286--291",
    abstract = "In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the $N$-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder{'}s ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well.",
}
@inproceedings{guo-etal-2020-jointly,
    title = "Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation",
    author = "Guo, Junliang  and
      Xu, Linli  and
      Chen, Enhong",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.36",
    doi = "10.18653/v1/2020.acl-main.36",
    pages = "376--385",
    abstract = "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation{\textasciitilde}(NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an $n$-gram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with $5+$ times speed up compared with an autoregressive model.",
}
@inproceedings{wei-etal-2020-multiscale,
    title = "Multiscale Collaborative Deep Models for Neural Machine Translation",
    author = "Wei, Xiangpeng  and
      Yu, Heng  and
      Hu, Yue  and
      Zhang, Yue  and
      Weng, Rongxiang  and
      Luo, Weihua",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.40",
    doi = "10.18653/v1/2020.acl-main.40",
    pages = "414--426",
    abstract = "Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient back-propagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2 +3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English-to-German task that significantly outperforms state-of-the-art deep NMT models. We have included the source code in supplementary materials.",
}
@inproceedings{zheng-etal-2020-opportunistic,
    title = "Opportunistic Decoding with Timely Correction for Simultaneous Translation",
    author = "Zheng, Renjie  and
      Ma, Mingbo  and
      Zheng, Baigong  and
      Liu, Kaibo  and
      Huang, Liang",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.42",
    doi = "10.18653/v1/2020.acl-main.42",
    pages = "437--442",
    abstract = "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently. Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative. We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information. At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality. Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8{\%} in Chinese-to-English and English-to-Chinese translation.",
}
@inproceedings{zenkel-etal-2020-end,
    title = "End-to-End Neural Word Alignment Outperforms {GIZA}++",
    author = "Zenkel, Thomas  and
      Wuebker, Joern  and
      DeNero, John",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.146",
    doi = "10.18653/v1/2020.acl-main.146",
    pages = "1605--1617",
    abstract = "Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.",
}
@inproceedings{bugliarello-okazaki-2020-enhancing,
    title = "Enhancing Machine Translation with Dependency-Aware Self-Attention",
    author = "Bugliarello, Emanuele  and
      Okazaki, Naoaki",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.147",
    doi = "10.18653/v1/2020.acl-main.147",
    pages = "1618--1627",
    abstract = "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.",
}
@inproceedings{ding-etal-2020-self,
    title = "Self-Attention with Cross-Lingual Position Representation",
    author = "Ding, Liang  and
      Wang, Longyue  and
      Tao, Dacheng",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.153",
    doi = "10.18653/v1/2020.acl-main.153",
    pages = "1679--1685",
    abstract = "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences. However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently. Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem. In this paper, we augment SANs with \textit{cross-lingual position representations} to model the bilingually aware latent structure for the input sentence. Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments. Experimental results on WMT{'}14 English$\Rightarrow$German, WAT{'}17 Japanese$\Rightarrow$English, and WMT{'}17 Chinese$\Leftrightarrow$English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines. Extensive analyses confirm that the performance gains come from the cross-lingual information.",
}
@inproceedings{provilkov-etal-2020-bpe,
    title = "{BPE}-Dropout: Simple and Effective Subword Regularization",
    author = "Provilkov, Ivan  and
      Emelianenko, Dmitrii  and
      Voita, Elena",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.170",
    doi = "10.18653/v1/2020.acl-main.170",
    pages = "1882--1892",
    abstract = "Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.",
}
@inproceedings{siddhant-etal-2020-leveraging,
    title = "Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation",
    author = "Siddhant, Aditya  and
      Bapna, Ankur  and
      Cao, Yuan  and
      Firat, Orhan  and
      Chen, Mia  and
      Kudugunta, Sneha  and
      Arivazhagan, Naveen  and
      Wu, Yonghui",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.252",
    doi = "10.18653/v1/2020.acl-main.252",
    pages = "2827--2835",
    abstract = "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",
}
@inproceedings{edunov-etal-2020-evaluation,
    title = "On The Evaluation of Machine Translation Systems Trained With Back-Translation",
    author = "Edunov, Sergey  and
      Ott, Myle  and
      Ranzato, Marc{'}Aurelio  and
      Auli, Michael",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.253",
    doi = "10.18653/v1/2020.acl-main.253",
    pages = "2836--2846",
    abstract = "Back-translation is a widely used data augmentation technique which leverages target monolingual data. However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese. This is believed to be due to translationese inputs better matching the back-translated training data. In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency.",
}
@inproceedings{zheng-etal-2020-simultaneous,
    title = "Simultaneous Translation Policies: From Fixed to Adaptive",
    author = "Zheng, Baigong  and
      Liu, Kaibo  and
      Zheng, Renjie  and
      Ma, Mingbo  and
      Liu, Hairong  and
      Huang, Liang",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.254",
    doi = "10.18653/v1/2020.acl-main.254",
    pages = "2847--2853",
    abstract = "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information. But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies. We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies. Experiments on Chinese -{\textgreater} English and German -{\textgreater} English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.",
}
@inproceedings{yao-wan-2020-multimodal,
    title = "Multimodal Transformer for Multimodal Machine Translation",
    author = "Yao, Shaowei  and
      Wan, Xiaojun",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.400",
    doi = "10.18653/v1/2020.acl-main.400",
    pages = "4346--4350",
    abstract = "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality. Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities. Equally treating all modalities may encode too much useless information from less important modalities. In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT. The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",
}
@inproceedings{wong-etal-2020-contextual,
    title = "Contextual Neural Machine Translation Improves Translation of Cataphoric Pronouns",
    author = "Wong, KayYen  and
      Maruf, Sameen  and
      Haffari, Gholamreza",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.530",
    doi = "10.18653/v1/2020.acl-main.530",
    pages = "5971--5978",
    abstract = "The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns. Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation. In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context. Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context. We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.",
}
@inproceedings{marie-etal-2020-tagged,
    title = "Tagged Back-translation Revisited: Why Does It Really Work?",
    author = "Marie, Benjamin  and
      Rubino, Raphael  and
      Fujita, Atsushi",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.532",
    doi = "10.18653/v1/2020.acl-main.532",
    pages = "5990--5997",
    abstract = "In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts. Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts. Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training. We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations. We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.",
}
@inproceedings{sun-etal-2020-estimating,
    title = "Are we Estimating or Guesstimating Translation Quality?",
    author = "Sun, Shuo  and
      Guzm{\'a}n, Francisco  and
      Specia, Lucia",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.558",
    doi = "10.18653/v1/2020.acl-main.558",
    pages = "6262--6267",
    abstract = "Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation. A carefully engineered ensemble of such models won the QE shared task at WMT19. Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels. Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.",
}
@inproceedings{saleh-pecina-2020-document,
    title = "Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain",
    author = "Saleh, Shadi  and
      Pecina, Pavel",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.613",
    doi = "10.18653/v1/2020.acl-main.613",
    pages = "6849--6860",
    abstract = "We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013{--}2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.",
}
@inproceedings{zhou-etal-2020-uncertainty,
    title = "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
    author = "Zhou, Yikai  and
      Yang, Baosong  and
      Wong, Derek F.  and
      Wan, Yu  and
      Chao, Lidia S.",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.620",
    doi = "10.18653/v1/2020.acl-main.620",
    pages = "6934--6944",
    abstract = "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
}
@inproceedings{you-etal-2020-hard,
    title = "Hard-Coded {G}aussian Attention for Neural Machine Translation",
    author = "You, Weiqiu  and
      Sun, Simeng  and
      Iyyer, Mohit",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.687",
    doi = "10.18653/v1/2020.acl-main.687",
    pages = "7689--7700",
    abstract = "Recent work has questioned the importance of the Transformer{'}s multi-headed attention for achieving high translation quality. We push further in this direction by developing a {``}hard-coded{''} attention variant without any learned parameters. Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs. However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention. Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer. Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.",
}
@inproceedings{saunders-byrne-2020-reducing,
    title = "Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem",
    author = "Saunders, Danielle  and
      Byrne, Bill",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.690",
    doi = "10.18653/v1/2020.acl-main.690",
    pages = "7724--7736",
    abstract = "Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men. In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender. The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge. Rather than attempt to create a {`}balanced{'} dataset, we use transfer learning on a small set of trusted, gender-balanced examples. This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch. A known pitfall of transfer learning on new domains is {`}catastrophic forgetting{'}, which we address at adaptation and inference time. During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction. At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU. We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.",
}
@inproceedings{mccarthy-etal-2020-addressing,
    title = "Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation",
    author = "McCarthy, Arya D.  and
      Li, Xian  and
      Gu, Jiatao  and
      Dong, Ning",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.753",
    doi = "10.18653/v1/2020.acl-main.753",
    pages = "8512--8525",
    abstract = "This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).",
}
@inproceedings{niu-etal-2020-evaluating,
    title = "Evaluating Robustness to Input Perturbations for Neural Machine Translation",
    author = "Niu, Xing  and
      Mathur, Prashant  and
      Dinu, Georgiana  and
      Al-Onaizan, Yaser",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.755",
    doi = "10.18653/v1/2020.acl-main.755",
    pages = "8538--8544",
    abstract = "Neural Machine Translation (NMT) models are sensitive to small perturbations in the input. Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input. This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input. We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed. Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.",
}
@inproceedings{aulamo-etal-2020-opusfilter,
    title = "{O}pus{F}ilter: A Configurable Parallel Corpus Filtering Toolbox",
    author = {Aulamo, Mikko  and
      Virpioja, Sami  and
      Tiedemann, J{\"o}rg},
    editor = "Celikyilmaz, Asli  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.20",
    doi = "10.18653/v1/2020.acl-demos.20",
    pages = "150--156",
    abstract = "This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora. It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters. Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data. We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data. Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set. Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.",
}
@inproceedings{moradi-etal-2020-training,
    title = "Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation",
    author = "Moradi, Pooya  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    editor = "Shmueli, Boaz  and
      Huang, Yin Jou",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-srw.14",
    pages = "93--100",
    abstract = "Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases.",
}
@inproceedings{kunilovskaya-lapshinova-koltunski-2019-translationese,
    title = "Translationese Features as Indicators of Quality in {E}nglish-{R}ussian Human Translation",
    author = "Kunilovskaya, Maria  and
      Lapshinova-Koltunski, Ekaterina",
    booktitle = "Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "Incoma Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/W19-8706",
    doi = "10.26615/issn.2683-0078.2019_006",
    pages = "47--56",
    abstract = "We use a range of morpho-syntactic features inspired by research in register studies (e.g. Biber, 1995; Neumann, 2013) and translation studies (e.g. Ilisei et al., 2010; Zanettin, 2013; Kunilovskaya and Kutuzov, 2018) to reveal the association between translationese and human translation quality. Translationese is understood as any statistical deviations of translations from non-translations (Baker, 1993) and is assumed to affect the fluency of translations, rendering them foreign-sounding and clumsy of wording and structure. This connection is often posited or implied in the studies of translationese or translational varieties (De Sutter et al., 2017), but is rarely directly tested. Our 45 features include frequencies of selected morphological forms and categories, some types of syntactic structures and relations, as well as several overall text measures extracted from Universal Dependencies annotation. The research corpora include English-to-Russian professional and student translations of informational or argumentative newspaper texts and a comparable corpus of non-translated Russian. Our results indicate lack of direct association between translationese and quality in our data: while our features distinguish translations and non-translations with the near perfect accuracy, the performance of the same algorithm on the quality classes barely exceeds the chance level.",
}
@inproceedings{mouratidis-kermanidis-2019-comparing,
    title = "Comparing a Hand-crafted to an Automatically Generated Feature Set for Deep Learning: Pairwise Translation Evaluation",
    author = "Mouratidis, Despoina  and
      Kermanidis, Katia Lida",
    booktitle = "Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "Incoma Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/W19-8708",
    doi = "10.26615/issn.2683-0078.2019_008",
    pages = "66--74",
    abstract = "The automatic evaluation of machine translation (MT) has proven to be a very significant research topic. Most automatic evaluation methods focus on the evaluation of the output of MT as they compute similarity scores that represent translation quality. This work targets on the performance of MT evaluation. We present a general scheme for learning to classify parallel translations, using linguistic information, of two MT model outputs and one human (reference) translation. We present three experiments to this scheme using neural networks (NN). One using string based hand-crafted features (Exp1), the second using automatically trained embeddings from the reference and the two MT outputs (one from a statistical machine translation (SMT) model and the other from a neural ma-chine translation (NMT) model), which are learned using NN (Exp2), and the third experiment (Exp3) that combines information from the other two experiments. The languages involved are English (EN), Greek (GR) and Italian (IT) segments are educational in domain. The proposed language-independent learning scheme which combines information from the two experiments (experiment 3) achieves higher classification accuracy compared with models using BLEU score information as well as other classification approaches, such as Random Forest (RF) and Support Vector Machine (SVM).",
}
@inproceedings{petrova-2019-translation,
    title = "Translation Quality Assessment Tools and Processes in Relation to {CAT} Tools",
    author = "Petrova, Viktoriya",
    booktitle = "Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "Incoma Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/W19-8711",
    doi = "10.26615/issn.2683-0078.2019_011",
    pages = "89--97",
    abstract = "Modern translation QA tools are the latest attempt to overcome the inevitable subjective component of human revisers. This paper analyzes the current situation in the translation industry in respect to those tools and their relationship with CAT tools. The adoption of international standards has set the basic frame that defines {``}quality{''}. Because of the clear impossibility to develop a universal QA tool, all of the existing ones have in common a wide variety of settings for the user to choose from. A brief comparison is made between most popular standalone QA tools. In order to verify their results in practice, QA outputs from two of those tools have been compared. Polls that cover a period of 12 years have been collected. Their participants explained what practices they adopted in order to guarantee quality.",
}
@inproceedings{marg-yanishevsky-2019-deep,
    title = "A Deep Learning Curve for Post-Editing 2",
    author = "Marg, Lena  and
      Yanishevsky, Alex{\textgreater}",
    editor = "Rossi, Laura",
    booktitle = "Proceedings of Machine Translation Summit XVII: Tutorial Abstracts",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/W19-7603",
    abstract = "In the last couple of years, machine translation technology has seen major changes with the breakthrough of neural machine translation (NMT), a growing number of providers and translation platforms. Machine Translation generally is experiencing a peak in demand from translation buyers, thanks to Machine Learning and AI being omnipresent in the media and at industry events. At the same time, new models for defining translation quality are becoming more widely adopted. These changes have profound implications for translators, LSPs and translation buyers: translators have to adjust their post-editing approaches, while LSPs and translation buyers are faced with decisions on selecting providers, best approaches for updating MT systems, financial investments, integrating tools, and getting the timing for implementation right for an optimum ROI.In this tutorial on MT and post-editing we would like to continue sharing the latest trends in the field of MT technologies, and discuss their impact on post-editing practices as well as integrating MT on large, multi-language translation programs. We will look at tool compatibility, different use cases of MT and dynamic quality models, and share our experience of measuring performance.",
}
@inproceedings{vardaro-etal-2019-translation,
    title = "Translation Quality and Effort Prediction in Professional Machine Translation Post-Editing",
    author = "Vardaro, Jennifer  and
      Schaeffer, Moritz  and
      Hansen-Schirra, Silvia",
    editor = "Carl, Michael  and
      Hansen-Schirra, Silvia",
    booktitle = "Proceedings of the Second MEMENTO workshop on Modelling Parameters of Cognitive Effort in Translation Production",
    month = aug,
    year = "2019",
    address = "Dublin, Ireland",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/W19-7004",
    pages = "7--8",
}
@inproceedings{kepler-etal-2019-unbabels,
    title = "Unbabel{'}s Participation in the {WMT}19 Translation Quality Estimation Shared Task",
    author = "Kepler, Fabio  and
      Tr{\'e}nous, Jonay  and
      Treviso, Marcos  and
      Vera, Miguel  and
      G{\'o}is, Ant{\'o}nio  and
      Farajian, M. Amin  and
      Lopes, Ant{\'o}nio V.  and
      Martins, Andr{\'e} F. T.",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5406",
    doi = "10.18653/v1/W19-5406",
    pages = "78--84",
    abstract = "We present the contribution of the Unbabel team to the WMT 2019 Shared Task on Quality Estimation. We participated on the word, sentence, and document-level tracks, encompassing 3 language pairs: English-German, English-Russian, and English-French. Our submissions build upon the recent OpenKiwi framework: We combine linear, neural, and predictor-estimator systems with new transfer learning approaches using BERT and XLM pre-trained models. We compare systems individually and propose new ensemble techniques for word and sentence-level predictions. We also propose a simple technique for converting word labels into document-level predictions. Overall, our submitted systems achieve the best results on all tracks and language pairs by a considerable margin.",
}
@inproceedings{kim-etal-2019-qe,
    title = "{QE} {BERT}: Bilingual {BERT} Using Multi-task Learning for Neural Quality Estimation",
    author = "Kim, Hyun  and
      Lim, Joon-Ho  and
      Kim, Hyun-Ki  and
      Na, Seung-Hoon",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5407",
    doi = "10.18653/v1/W19-5407",
    pages = "85--89",
    abstract = "For translation quality estimation at word and sentence levels, this paper presents a novel approach based on BERT that recently has achieved impressive results on various natural language processing tasks. Our proposed model is re-purposed BERT for the translation quality estimation and uses multi-task learning for the sentence-level task and word-level subtasks (i.e., source word, target word, and target gap). Experimental results on Quality Estimation shared task of WMT19 show that our systems show competitive results and provide significant improvements over the baseline.",
}
@inproceedings{yankovskaya-etal-2019-quality,
    title = "Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings",
    author = {Yankovskaya, Elizaveta  and
      T{\"a}ttar, Andre  and
      Fishel, Mark},
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5410",
    doi = "10.18653/v1/W19-5410",
    pages = "101--105",
    abstract = "We propose the use of pre-trained embeddings as features of a regression model for sentence-level quality estimation of machine translation. In our work we combine freely available BERT and LASER multilingual embeddings to train a neural-based regression model. In the second proposed method we use as an input features not only pre-trained embeddings, but also log probability of any machine translation (MT) system. Both methods are applied to several language pairs and are evaluated both as a classical quality estimation system (predicting the HTER score) as well as an MT metric (predicting human judgements of translation quality).",
}
@inproceedings{zhou-etal-2019-source,
    title = "{SOURCE}: {SOUR}ce-Conditional Elmo-style Model for Machine Translation Quality Estimation",
    author = "Zhou, Junpei  and
      Zhang, Zhisong  and
      Hu, Zecong",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5411",
    doi = "10.18653/v1/W19-5411",
    pages = "106--111",
    abstract = "Quality estimation (QE) of machine translation (MT) systems is a task of growing importance. It reduces the cost of post-editing, allowing machine-translated text to be used in formal occasions. In this work, we describe our submission system in WMT 2019 sentence-level QE task. We mainly explore the utilization of pre-trained translation models in QE and adopt a bi-directional translation-like strategy. The strategy is similar to ELMo, but additionally conditions on source sentences. Experiments on WMT QE dataset show that our strategy, which makes the pre-training slightly harder, can bring improvements for QE. In WMT-2019 QE task, our system ranked in the second place on En-De NMT dataset and the third place on En-Ru NMT dataset.",
}
@inproceedings{barrault-etal-2019-findings,
    title = "Findings of the 2019 Conference on Machine Translation ({WMT}19)",
    author = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Koehn, Philipp  and
      Malmasi, Shervin  and
      Monz, Christof  and
      M{\"u}ller, Mathias  and
      Pal, Santanu  and
      Post, Matt  and
      Zampieri, Marcos},
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5301",
    doi = "10.18653/v1/W19-5301",
    pages = "1--61",
    abstract = "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.",
}
@inproceedings{bawden-etal-2019-university,
    title = "The {U}niversity of {E}dinburgh{'}s Submissions to the {WMT}19 News Translation Task",
    author = "Bawden, Rachel  and
      Bogoychev, Nikolay  and
      Germann, Ulrich  and
      Grundkiewicz, Roman  and
      Kirefu, Faheem  and
      Miceli Barone, Antonio Valerio  and
      Birch, Alexandra",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5304",
    doi = "10.18653/v1/W19-5304",
    pages = "103--115",
    abstract = "The University of Edinburgh participated in the WMT19 Shared Task on News Translation in six language directions: English↔Gujarati, English↔Chinese, German→English, and English→Czech. For all translation directions, we created or used back-translations of monolingual data in the target language as additional synthetic training data. For English↔Gujarati, we also explored semi-supervised MT with cross-lingual language model pre-training, and translation pivoting through Hindi. For translation to and from Chinese, we investigated character-based tokenisation vs. sub-word segmentation of Chinese text. For German→English, we studied the impact of vast amounts of back-translated training data on translation quality, gaining a few additional insights over Edunov et al. (2018). For English→Czech, we compared different preprocessing and tokenisation regimes.",
}
@inproceedings{mondal-etal-2019-ju,
    title = "{JU}-{S}aarland Submission to the {WMT}2019 {E}nglish{--}{G}ujarati Translation Shared Task",
    author = "Mondal, Riktim  and
      Nayek, Shankha Raj  and
      Chowdhury, Aditya  and
      Pal, Santanu  and
      Naskar, Sudip Kumar  and
      van Genabith, Josef",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5332",
    doi = "10.18653/v1/W19-5332",
    pages = "308--313",
    abstract = "In this paper we describe our joint submission (JU-Saarland) from Jadavpur University and Saarland University in the WMT 2019 news translation shared task for English{--}Gujarati language pair within the translation task sub-track. Our baseline and primary submissions are built using Recurrent neural network (RNN) based neural machine translation (NMT) system which follows attention mechanism. Given the fact that the two languages belong to different language families and there is not enough parallel data for this language pair, building a high quality NMT system for this language pair is a difficult task. We produced synthetic data through back-translation from available monolingual data. We report the translation quality of our English{--}Gujarati and Gujarati{--}English NMT systems trained at word, byte-pair and character encoding levels where RNN at word level is considered as the baseline and used for comparison purpose. Our English{--}Gujarati system ranked in the second position in the shared task.",
}
@inproceedings{stojanovski-fraser-2019-combining,
    title = "Combining Local and Document-Level Context: The {LMU} {M}unich Neural Machine Translation System at {WMT}19",
    author = "Stojanovski, Dario  and
      Fraser, Alexander",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5345",
    doi = "10.18653/v1/W19-5345",
    pages = "400--406",
    abstract = "We describe LMU Munich{'}s machine translation system for English→German translation which was used to participate in the WMT19 shared task on supervised news translation. We specifically participated in the document-level MT track. The system used as a primary submission is a context-aware Transformer capable of both rich modeling of limited contextual information and integration of large-scale document-level context with a less rich representation. We train this model by fine-tuning a big Transformer baseline. Our experimental results show that document-level context provides for large improvements in translation quality, and adding a rich representation of the previous sentence provides a small additional gain.",
}
@inproceedings{raganato-etal-2019-mucow,
    title = "The {M}u{C}o{W} Test Suite at {WMT} 2019: Automatically Harvested Multilingual Contrastive Word Sense Disambiguation Test Sets for Machine Translation",
    author = {Raganato, Alessandro  and
      Scherrer, Yves  and
      Tiedemann, J{\"o}rg},
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5354",
    doi = "10.18653/v1/W19-5354",
    pages = "470--480",
    abstract = "Supervised Neural Machine Translation (NMT) systems currently achieve impressive translation quality for many language pairs. One of the key features of a correct translation is the ability to perform word sense disambiguation (WSD), i.e., to translate an ambiguous word with its correct sense. Existing evaluation benchmarks on WSD capabilities of translation systems rely heavily on manual work and cover only few language pairs and few word types. We present MuCoW, a multilingual contrastive test suite that covers 16 language pairs with more than 200 thousand contrastive sentence pairs, automatically built from word-aligned parallel corpora and the wide-coverage multilingual sense inventory of BabelNet. We evaluate the quality of the ambiguity lexicons and of the resulting test suite on all submissions from 9 language pairs presented in the WMT19 news shared translation task, plus on other 5 language pairs using NMT pretrained models. The MuCoW test suite is available at \url{http://github.com/Helsinki-NLP/MuCoW}.",
}
@inproceedings{lo-2019-yisi,
    title = "{Y}i{S}i - a Unified Semantic {MT} Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources",
    author = "Lo, Chi-kiu",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5358",
    doi = "10.18653/v1/W19-5358",
    pages = "507--513",
    abstract = "We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1{'}s scores with human judgment is made by using contextual embeddings in multilingual BERT{--}Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available.",
}
@inproceedings{dabre-sumita-2019-nicts-supervised,
    title = "{NICT}{'}s Supervised Neural Machine Translation Systems for the {WMT}19 Translation Robustness Task",
    author = "Dabre, Raj  and
      Sumita, Eiichiro",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5362",
    doi = "10.18653/v1/W19-5362",
    pages = "533--536",
    abstract = "In this paper we describe our neural machine translation (NMT) systems for Japanese↔English translation which we submitted to the translation robustness task. We focused on leveraging transfer learning via fine tuning to improve translation quality. We used a fairly well established domain adaptation technique called Mixed Fine Tuning (MFT) (Chu et. al., 2017) to improve translation quality for Japanese↔English. We also trained bi-directional NMT models instead of uni-directional ones as the former are known to be quite robust, especially in low-resource scenarios. However, given the noisy nature of the in-domain training data, the improvements we obtained are rather modest.",
}
@inproceedings{helcl-etal-2019-cuni,
    title = "{CUNI} System for the {WMT}19 Robustness Task",
    author = "Helcl, Jind{\v{r}}ich  and
      Libovick{\'y}, Jind{\v{r}}ich  and
      Popel, Martin",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5364",
    doi = "10.18653/v1/W19-5364",
    pages = "539--543",
    abstract = "We present our submission to the WMT19 Robustness Task. Our baseline system is the Charles University (CUNI) Transformer system trained for the WMT18 shared task on News Translation. Quantitative results show that the CUNI Transformer system is already far more robust to noisy input than the LSTM-based baseline provided by the task organizers. We further improved the performance of our model by fine-tuning on the in-domain noisy data without influencing the translation quality on the news domain.",
}
@inproceedings{zhang-toral-2019-effect,
    title = "The Effect of Translationese in Machine Translation Test Sets",
    author = "Zhang, Mike  and
      Toral, Antonio",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5208",
    doi = "10.18653/v1/W19-5208",
    pages = "73--81",
    abstract = "The effect of translationese has been studied in the field of machine translation (MT), mostly with respect to training data. We study in depth the effect of translationese on test data, using the test sets from the last three editions of WMT{'}s news shared task, containing 17 translation directions. We show evidence that (i) the use of translationese in test sets results in inflated human evaluation scores for MT systems; (ii) in some cases system rankings do change and (iii) the impact translationese has on a translation direction is inversely correlated to the translation quality attainable by state-of-the-art MT systems for that direction.",
}
@inproceedings{raganato-etal-2019-evaluation,
    title = "An Evaluation of Language-Agnostic Inner-Attention-Based Representations in Machine Translation",
    author = {Raganato, Alessandro  and
      V{\'a}zquez, Ra{\'u}l  and
      Creutz, Mathias  and
      Tiedemann, J{\"o}rg},
    editor = "Augenstein, Isabelle  and
      Gella, Spandana  and
      Ruder, Sebastian  and
      Kann, Katharina  and
      Can, Burcu  and
      Welbl, Johannes  and
      Conneau, Alexis  and
      Ren, Xiang  and
      Rei, Marek",
    booktitle = "Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4304",
    doi = "10.18653/v1/W19-4304",
    pages = "27--32",
    abstract = "In this paper, we explore a multilingual translation model with a cross-lingually shared layer that can be used as fixed-size sentence representation in different downstream tasks. We systematically study the impact of the size of the shared layer and the effect of including additional languages in the model. In contrast to related previous work, we demonstrate that the performance in translation does correlate with trainable downstream tasks. In particular, we show that larger intermediate layers not only improve translation quality, especially for long sentences, but also push the accuracy of trainable classification tasks. On the other hand, shorter representations lead to increased compression that is beneficial in non-trainable similarity tasks. We hypothesize that the training procedure on the downstream task enables the model to identify the encoded information that is useful for the specific task whereas non-trainable benchmarks can be confused by other types of information also encoded in the representation of a sentence.",
}
@inproceedings{lala-etal-2019-grounded,
    title = "Grounded Word Sense Translation",
    author = "Lala, Chiraag  and
      Madhyastha, Pranava  and
      Specia, Lucia",
    editor = "Bernardi, Raffaella  and
      Fernandez, Raquel  and
      Gella, Spandana  and
      Kafle, Kushal  and
      Kanan, Christopher  and
      Lee, Stefan  and
      Nabi, Moin",
    booktitle = "Proceedings of the Second Workshop on Shortcomings in Vision and Language",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-1808",
    doi = "10.18653/v1/W19-1808",
    pages = "78--85",
    abstract = "Recent work on visually grounded language learning has focused on broader applications of grounded representations, such as visual question answering and multimodal machine translation. In this paper we consider grounded word sense translation, i.e. the task of correctly translating an ambiguous source word given the corresponding textual and visual context. Our main objective is to investigate the extent to which images help improve word-level (lexical) translation quality. We do so by first studying the dataset for this task to understand the scope and challenges of the task. We then explore different data settings, image features, and ways of grounding to investigate the gain from using images in each of the combinations. We find that grounding on the image is specially beneficial in weaker unidirectional recurrent translation models. We observe that adding structured image information leads to stronger gains in lexical translation accuracy.",
}
@inproceedings{ahmadnia-dorr-2019-bilingual,
    title = "Bilingual Low-Resource Neural Machine Translation with Round-Tripping: The Case of {P}ersian-{S}panish",
    author = "Ahmadnia, Benyamin  and
      Dorr, Bonnie",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1003",
    doi = "10.26615/978-954-452-056-4_003",
    pages = "18--24",
    abstract = "The quality of Neural Machine Translation (NMT), as a data-driven approach, massively depends on quantity, quality, and relevance of the training dataset. Such approaches have achieved promising results for bilingually high-resource scenarios but are inadequate for low-resource conditions. This paper describes a round-trip training approach to bilingual low-resource NMT that takes advantage of monolingual datasets to address training data scarcity, thus augmenting translation quality. We conduct detailed experiments on Persian-Spanish as a bilingually low-resource scenario. Experimental results demonstrate that this competitive approach outperforms the baselines.",
}
@inproceedings{ahmadnia-dorr-2019-enhancing,
    title = "Enhancing Phrase-Based Statistical Machine Translation by Learning Phrase Representations Using Long Short-Term Memory Network",
    author = "Ahmadnia, Benyamin  and
      Dorr, Bonnie",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1004",
    doi = "10.26615/978-954-452-056-4_004",
    pages = "25--32",
    abstract = "Phrases play a key role in Machine Translation (MT). In this paper, we apply a Long Short-Term Memory (LSTM) model over conventional Phrase-Based Statistical MT (PBSMT). The core idea is to use an LSTM encoder-decoder to score the phrase table generated by the PBSMT decoder. Given a source sequence, the encoder and decoder are jointly trained in order to maximize the conditional probability of a target sequence. Analytically, the performance of a PBSMT system is enhanced by using the conditional probabilities of phrase pairs computed by an LSTM encoder-decoder as an additional feature in the existing log-linear model. We compare the performance of the phrase tables in the PBSMT to the performance of the proposed LSTM and observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set.",
}
@inproceedings{beloucif-etal-2019-naive,
    title = "Naive Regularizers for Low-Resource Neural Machine Translation",
    author = "Beloucif, Meriem  and
      Gonzalez, Ana Valeria  and
      Bollmann, Marcel  and
      S{\o}gaard, Anders",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://aclanthology.org/R19-1013",
    doi = "10.26615/978-954-452-056-4_013",
    pages = "102--111",
    abstract = "Neural machine translation models have little inductive bias, which can be a disadvantage in low-resource scenarios. Neural models have to be trained on large amounts of data and have been shown to perform poorly when only limited data is available. We show that using naive regularization methods, based on sentence length, punctuation and word frequencies, to penalize translations that are very different from the input sentences, consistently improves the translation quality across multiple low-resource languages. We experiment with 12 language pairs, varying the training data size between 17k to 230k sentence pairs. Our best regularizer achieves an average increase of 1.5 BLEU score and 1.0 TER score across all the language pairs. For example, we achieve a BLEU score of 26.70 on the IWSLT15 English{--}Vietnamese translation task simply by using relative differences in punctuation as a regularizer.",
}
@inproceedings{kepler-etal-2019-openkiwi,
    title = "{O}pen{K}iwi: An Open Source Framework for Quality Estimation",
    author = "Kepler, Fabio  and
      Tr{\'e}nous, Jonay  and
      Treviso, Marcos  and
      Vera, Miguel  and
      Martins, Andr{\'e} F. T.",
    editor = "Costa-juss{\`a}, Marta R.  and
      Alfonseca, Enrique",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-3020",
    doi = "10.18653/v1/P19-3020",
    pages = "117--122",
    abstract = "We introduce OpenKiwi, a Pytorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015{--}18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentence-level tasks.",
}
@inproceedings{akoury-etal-2019-syntactically,
    title = "Syntactically Supervised Transformers for Faster Neural Machine Translation",
    author = "Akoury, Nader  and
      Krishna, Kalpesh  and
      Iyyer, Mohit",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1122",
    doi = "10.18653/v1/P19-1122",
    pages = "1269--1281",
    abstract = "Standard decoders for neural machine translation autoregressively generate a single target token per timestep, which slows inference especially for long outputs. While architectural advances such as the Transformer fully parallelize the decoder computations at training time, inference still proceeds sequentially. Recent developments in non- and semi-autoregressive decoding produce multiple tokens per timestep independently of the others, which improves inference speed but deteriorates translation quality. In this work, we propose the syntactically supervised Transformer (SynST), which first autoregressively predicts a chunked parse tree before generating all of the target tokens in one shot conditioned on the predicted parse. A series of controlled experiments demonstrates that SynST decodes sentences {\textasciitilde}5x faster than the baseline autoregressive Transformer while achieving higher BLEU scores than most competing methods on En-De and En-Fr datasets.",
}
@inproceedings{wei-etal-2019-imitation,
    title = "Imitation Learning for Non-Autoregressive Neural Machine Translation",
    author = "Wei, Bingzhen  and
      Wang, Mingxuan  and
      Zhou, Hao  and
      Lin, Junyang  and
      Sun, Xu",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1125",
    doi = "10.18653/v1/P19-1125",
    pages = "1304--1312",
    abstract = "Non-autoregressive translation models (NAT) have achieved impressive inference speedup. A potential issue of the existing NAT algorithms, however, is that the decoding is conducted in parallel, without directly considering previous context. In this paper, we propose an imitation learning framework for non-autoregressive machine translation, which still enjoys the fast translation speed but gives comparable translation performance compared to its auto-regressive counterpart. We conduct experiments on the IWSLT16, WMT14 and WMT16 datasets. Our proposed model achieves a significant speedup over the autoregressive models, while keeping the translation quality comparable to the autoregressive models. By sampling sentence length in parallel at inference time, we achieve the performance of 31.85 BLEU on WMT16 Ro$\rightarrow$En and 30.68 BLEU on IWSLT16 En$\rightarrow$De.",
}
@inproceedings{shu-etal-2019-generating,
    title = "Generating Diverse Translations with Sentence Codes",
    author = "Shu, Raphael  and
      Nakayama, Hideki  and
      Cho, Kyunghyun",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1177",
    doi = "10.18653/v1/P19-1177",
    pages = "1823--1827",
    abstract = "Users of machine translation systems may desire to obtain multiple candidates translated in different ways. In this work, we attempt to obtain diverse translations by using sentence codes to condition the sentence generation. We describe two methods to extract the codes, either with or without the help of syntax information. For diverse generation, we sample multiple candidates, each of which conditioned on a unique code. Experiments show that the sampled translations have much higher diversity scores when using reasonable sentence codes, where the translation quality is still on par with the baselines even under strong constraint imposed by the codes. In qualitative analysis, we show that our method is able to generate paraphrase translations with drastically different structures. The proposed approach can be easily adopted to existing translation systems as no modification to the model is required.",
}
@inproceedings{fu-etal-2019-reference,
    title = "Reference Network for Neural Machine Translation",
    author = "Fu, Han  and
      Liu, Chenghao  and
      Sun, Jianling",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1287",
    doi = "10.18653/v1/P19-1287",
    pages = "3002--3012",
    abstract = "Neural Machine Translation (NMT) has achieved notable success in recent years. Such a framework usually generates translations in isolation. In contrast, human translators often refer to reference data, either rephrasing the intricate sentence fragments with common terms in source language, or just accessing to the golden translation directly. In this paper, we propose a Reference Network to incorporate referring process into translation decoding of NMT. To construct a reference book, an intuitive way is to store the detailed translation history with extra memory, which is computationally expensive. Instead, we employ Local Coordinates Coding (LCC) to obtain global context vectors containing monolingual and bilingual contextual information for NMT decoding. Experimental results on Chinese-English and English-German tasks demonstrate that our proposed model is effective in improving the translation quality with lightweight computation cost.",
}
@inproceedings{liu-etal-2019-robust,
    title = "Robust Neural Machine Translation with Joint Textual and Phonetic Embedding",
    author = "Liu, Hairong  and
      Ma, Mingbo  and
      Huang, Liang  and
      Xiong, Hao  and
      He, Zhongjun",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1291",
    doi = "10.18653/v1/P19-1291",
    pages = "3044--3049",
    abstract = "Neural machine translation (NMT) is notoriously sensitive to noises, but noises are almost inevitable in practice. One special kind of noise is the \textit{homophone noise}, where words are replaced by other words with similar pronunciations. We propose to improve the robustness of NMT to homophone noises by 1) jointly embedding both textual and phonetic information of source sentences, and 2) augmenting the training dataset with homophone noises. Interestingly, to achieve better translation quality and more robustness, we found that most (though not all) weights should be put on the phonetic rather than textual information. Experiments show that our method not only significantly improves the robustness of NMT to homophone noises, but also surprisingly improves the translation quality on some clean test sets.",
}
@inproceedings{wu-etal-2019-depth,
    title = "Depth Growing for Neural Machine Translation",
    author = "Wu, Lijun  and
      Wang, Yiren  and
      Xia, Yingce  and
      Tian, Fei  and
      Gao, Fei  and
      Qin, Tao  and
      Lai, Jianhuang  and
      Liu, Tie-Yan",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1558",
    doi = "10.18653/v1/P19-1558",
    pages = "5558--5563",
    abstract = "While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English$\to$German and English$\to$French translation tasks.",
}
@inproceedings{xia-etal-2019-generalized,
    title = "Generalized Data Augmentation for Low-Resource Translation",
    author = "Xia, Mengzhou  and
      Kong, Xiang  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1579",
    doi = "10.18653/v1/P19-1579",
    pages = "5786--5796",
    abstract = "Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.",
}
@inproceedings{cohn-gordon-goodman-2019-lost,
    title = "Lost in Machine Translation: A Method to Reduce Meaning Loss",
    author = "Cohn-Gordon, Reuben  and
      Goodman, Noah",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1042",
    doi = "10.18653/v1/N19-1042",
    pages = "437--441",
    abstract = "A desideratum of high-quality translation systems is that they preserve meaning, in the sense that two sentences with different meanings should not translate to one and the same sentence in another language. However, state-of-the-art systems often fail in this regard, particularly in cases where the source and target languages partition the {``}meaning space{''} in different ways. For instance, {``}I cut my finger.{''} and {``}I cut my finger off.{''} describe different states of the world but are translated to French (by both Fairseq and Google Translate) as {``}Je me suis coup{\'e} le doigt.{''}, which is ambiguous as to whether the finger is detached. More generally, translation systems are typically many-to-one (non-injective) functions from source to target language, which in many cases results in important distinctions in meaning being lost in translation. Building on Bayesian models of informative utterance production, we present a method to define a less ambiguous translation system in terms of an underlying pre-trained neural sequence-to-sequence model. This method increases injectivity, resulting in greater preservation of meaning as measured by improvement in cycle-consistency, without impeding translation quality (measured by BLEU score).",
}
@inproceedings{li-etal-2019-understanding-improving,
    title = "{U}nderstanding and {I}mproving {H}idden {R}epresentations for {N}eural {M}achine {T}ranslation",
    author = "Li, Guanlin  and
      Liu, Lemao  and
      Li, Xintong  and
      Zhu, Conghui  and
      Zhao, Tiejun  and
      Shi, Shuming",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1046",
    doi = "10.18653/v1/N19-1046",
    pages = "466--477",
    abstract = "Multilayer architectures are currently the gold standard for large-scale neural machine translation. Existing works have explored some methods for understanding the hidden representations, however, they have not sought to improve the translation quality rationally according to their understanding. Towards understanding for performance improvement, we first artificially construct a sequence of nested relative tasks and measure the feature generalization ability of the learned hidden representation over these tasks. Based on our understanding, we then propose to regularize the layer-wise representations with all tree-induced tasks. To overcome the computational bottleneck resulting from the large number of regularization terms, we design efficient approximation methods by selecting a few coarse-to-fine tasks for regularization. Extensive experiments on two widely-used datasets demonstrate the proposed methods only lead to small extra overheads in training but no additional overheads in testing, and achieve consistent improvements (up to +1.3 BLEU) compared to the state-of-the-art translation model.",
}
@inproceedings{marie-fujita-2019-unsupervised,
    title = "Unsupervised Extraction of Partial Translations for Neural Machine Translation",
    author = "Marie, Benjamin  and
      Fujita, Atsushi",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1384",
    doi = "10.18653/v1/N19-1384",
    pages = "3834--3844",
    abstract = "In neural machine translation (NMT), monolingual data are usually exploited through a so-called back-translation: sentences in the target language are translated into the source language to synthesize new parallel data. While this method provides more training data to better model the target language, on the source side, it only exploits translations that the NMT system is already able to generate using a model trained on existing parallel data. In this work, we assume that new translation knowledge can be extracted from monolingual data, without relying at all on existing parallel data. We propose a new algorithm for extracting from monolingual data what we call partial translations: pairs of source and target sentences that contain sequences of tokens that are translations of each other. Our algorithm is fully unsupervised and takes only source and target monolingual data as input. Our empirical evaluation points out that our partial translations can be used in combination with back-translation to further improve NMT models. Furthermore, while partial translations are particularly useful for low-resource language pairs, they can also be successfully exploited in resource-rich scenarios to improve translation quality.",
}
@inproceedings{murthy-etal-2019-addressing,
    title = "Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages",
    author = "Murthy, Rudra  and
      Kunchukuttan, Anoop  and
      Bhattacharyya, Pushpak",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1387",
    doi = "10.18653/v1/N19-1387",
    pages = "3868--3873",
    abstract = "Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios.",
}
@inproceedings{aharoni-etal-2019-massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}
@inproceedings{aldarmaki-diab-2019-context,
    title = "Context-Aware Cross-Lingual Mapping",
    author = "Aldarmaki, Hanan  and
      Diab, Mona",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1391",
    doi = "10.18653/v1/N19-1391",
    pages = "3906--3911",
    abstract = "Cross-lingual word vectors are typically obtained by fitting an orthogonal matrix that maps the entries of a bilingual dictionary from a source to a target vector space. Word vectors, however, are most commonly used for sentence or document-level representations that are calculated as the weighted average of word embeddings. In this paper, we propose an alternative to word-level mapping that better reflects sentence-level cross-lingual similarity. We incorporate context in the transformation matrix by directly mapping the averaged embeddings of aligned sentences in a parallel corpus. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality.",
}
@inproceedings{wang-etal-2019-improving,
    title = "Improving Pre-Trained Multilingual Model with Vocabulary Expansion",
    author = "Wang, Hai  and
      Yu, Dian  and
      Sun, Kai  and
      Chen, Jianshu  and
      Yu, Dong",
    editor = "Bansal, Mohit  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K19-1030",
    doi = "10.18653/v1/K19-1030",
    pages = "316--327",
    abstract = "Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.",
}
@article{fomicheva-specia-2019-taking,
    title = "Taking {MT} Evaluation Metrics to Extremes: Beyond Correlation with Human Judgments",
    author = "Fomicheva, Marina  and
      Specia, Lucia",
    journal = "Computational Linguistics",
    volume = "45",
    number = "3",
    month = sep,
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J19-3004",
    doi = "10.1162/coli_a_00356",
    pages = "515--558",
    abstract = "Automatic Machine Translation (MT) evaluation is an active field of research, with a handful of new metrics devised every year. Evaluation metrics are generally benchmarked against manual assessment of translation quality, with performance measured in terms of overall correlation with human scores. Much work has been dedicated to the improvement of evaluation metrics to achieve a higher correlation with human judgments. However, little insight has been provided regarding the weaknesses and strengths of existing approaches and their behavior in different settings. In this work we conduct a broad meta-evaluation study of the performance of a wide range of evaluation metrics focusing on three major aspects. First, we analyze the performance of the metrics when faced with different levels of translation quality, proposing a local dependency measure as an alternative to the standard, global correlation coefficient. We show that metric performance varies significantly across different levels of MT quality: Metrics perform poorly when faced with low-quality translations and are not able to capture nuanced quality distinctions. Interestingly, we show that evaluating low-quality translations is also more challenging for humans. Second, we show that metrics are more reliable when evaluating neural MT than the traditional statistical MT systems. Finally, we show that the difference in the evaluation accuracy for different metrics is maintained even if the gold standard scores are based on different criteria.",
}
@inproceedings{martinez-garcia-etal-2019-context,
    title = "Context-Aware Neural Machine Translation Decoding",
    author = "Mart{\'\i}nez Garcia, Eva  and
      Creus, Carles  and
      Espa{\~n}a-Bonet, Cristina",
    editor = "Popescu-Belis, Andrei  and
      Lo{\'a}iciga, Sharid  and
      Hardmeier, Christian  and
      Xiong, Deyi",
    booktitle = "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6502",
    doi = "10.18653/v1/D19-6502",
    pages = "13--23",
    abstract = "This work presents a decoding architecture that fuses the information from a neural translation model and the context semantics enclosed in a semantic space language model based on word embeddings. The method extends the beam search decoding process and therefore can be applied to any neural machine translation framework. With this, we sidestep two drawbacks of current document-level systems: (i) we do not modify the training process so there is no increment in training time, and (ii) we do not require document-level an-notated data. We analyze the impact of the fusion system approach and its parameters on the final translation quality for English{--}Spanish. We obtain consistent and statistically significant improvements in terms of BLEU and METEOR and we observe how the fused systems are able to handle synonyms to propose more adequate translations as well as help the system to disambiguate among several translation candidates for a word.",
}
@inproceedings{scherrer-etal-2019-analysing,
    title = "Analysing concatenation approaches to document-level {NMT} in two different domains",
    author = {Scherrer, Yves  and
      Tiedemann, J{\"o}rg  and
      Lo{\'a}iciga, Sharid},
    editor = "Popescu-Belis, Andrei  and
      Lo{\'a}iciga, Sharid  and
      Hardmeier, Christian  and
      Xiong, Deyi",
    booktitle = "Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-6506",
    doi = "10.18653/v1/D19-6506",
    pages = "51--61",
    abstract = "In this paper, we investigate how different aspects of discourse context affect the performance of recent neural MT systems. We describe two popular datasets covering news and movie subtitles and we provide a thorough analysis of the distribution of various document-level features in their domains. Furthermore, we train a set of context-aware MT models on both datasets and propose a comparative evaluation scheme that contrasts coherent context with artificially scrambled documents and absent context, arguing that the impact of discourse-aware MT models will become visible in this way. Our results show that the models are indeed affected by the manipulation of the test data, providing a different view on document-level translation quality than absolute sentence-level scores.",
}
@inproceedings{clinchant-etal-2019-use,
    title = "On the use of {BERT} for Neural Machine Translation",
    author = "Clinchant, Stephane  and
      Jung, Kweon Woo  and
      Nikoulina, Vassilina",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Konstas, Ioannis  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke  and
      Sudoh, Katsuhito",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5611",
    doi = "10.18653/v1/D19-5611",
    pages = "108--117",
    abstract = "Exploiting large pretrained models for various NMT tasks have gained a lot of visibility recently. In this work we study how BERT pretrained models could be exploited for supervised Neural Machine Translation. We compare various ways to integrate pretrained BERT model with NMT model and study the impact of the monolingual data used for BERT training on the final translation quality. We use WMT-14 English-German, IWSLT15 English-German and IWSLT14 English-Russian datasets for these experiments. In addition to standard task test set evaluation, we perform evaluation on out-of-domain test sets and noise injected test sets, in order to assess how BERT pretrained representations affect model robustness.",
}
@inproceedings{zaremoodi-haffari-2019-adaptively,
    title = "Adaptively Scheduled Multitask Learning: The Case of Low-Resource Neural Machine Translation",
    author = "Zaremoodi, Poorya  and
      Haffari, Gholamreza",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Konstas, Ioannis  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke  and
      Sudoh, Katsuhito",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5618",
    doi = "10.18653/v1/D19-5618",
    pages = "177--186",
    abstract = "Neural Machine Translation (NMT), a data-hungry technology, suffers from the lack of bilingual data in low-resource scenarios. Multitask learning (MTL) can alleviate this issue by injecting inductive biases into NMT, using auxiliary syntactic and semantic tasks. However, an effective \textit{training schedule} is required to balance the importance of tasks to get the best use of the training signal. The role of training schedule becomes even more crucial in \textit{biased-MTL} where the goal is to improve one (or a subset) of tasks the most, e.g. translation quality. Current approaches for biased-MTL are based on brittle \textit{hand-engineered} heuristics that require trial and error, and should be (re-)designed for each learning scenario. To the best of our knowledge, ours is the first work on \textit{adaptively} and \textit{dynamically} changing the training schedule in biased-MTL. We propose a rigorous approach for automatically reweighing the training data of the main and auxiliary tasks throughout the training process based on their contributions to the generalisability of the main NMT task. Our experiments on translating from English to Vietnamese/Turkish/Spanish show improvements of up to +1.2 BLEU points, compared to strong baselines. Additionally, our analyses shed light on the dynamic of needs throughout the training of NMT: from syntax to semantic.",
}
@inproceedings{ataman-etal-2019-importance,
    title = "On the Importance of Word Boundaries in Character-level Neural Machine Translation",
    author = "Ataman, Duygu  and
      Firat, Orhan  and
      Di Gangi, Mattia A.  and
      Federico, Marcello  and
      Birch, Alexandra",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Hayashi, Hiroaki  and
      Konstas, Ioannis  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke  and
      Sudoh, Katsuhito",
    booktitle = "Proceedings of the 3rd Workshop on Neural Generation and Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5619",
    doi = "10.18653/v1/D19-5619",
    pages = "187--193",
    abstract = "Neural Machine Translation (NMT) models generally perform translation using a fixed-size lexical vocabulary, which is an important bottleneck on their generalization capability and overall translation quality. The standard approach to overcome this limitation is to segment words into subword units, typically using some external tools with arbitrary heuristics, resulting in vocabulary units not optimized for the translation task. Recent studies have shown that the same approach can be extended to perform NMT directly at the level of characters, which can deliver translation accuracy on-par with subword-based models, on the other hand, this requires relatively deeper networks. In this paper, we propose a more computationally-efficient solution for character-level NMT which implements a hierarchical decoding architecture where translations are subsequently generated at the level of words and characters. We evaluate different methods for open-vocabulary NMT in the machine translation task from English into five languages with distinct morphological typology, and show that the hierarchical decoding model can reach higher translation accuracy than the subword-level NMT model using significantly fewer parameters, while demonstrating better capacity in learning longer-distance contextual and grammatical dependencies than the standard character-level NMT model.",
}
@inproceedings{nguyen-daume-iii-2019-global,
    title = "{G}lobal {V}oices: Crossing Borders in Automatic News Summarization",
    author = "Nguyen, Khanh  and
      Daum{\'e} III, Hal",
    editor = "Wang, Lu  and
      Cheung, Jackie Chi Kit  and
      Carenini, Giuseppe  and
      Liu, Fei",
    booktitle = "Proceedings of the 2nd Workshop on New Frontiers in Summarization",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5411",
    doi = "10.18653/v1/D19-5411",
    pages = "90--97",
    abstract = "We construct Global Voices, a multilingual dataset for evaluating cross-lingual summarization methods. We extract social-network descriptions of Global Voices news articles to cheaply collect evaluation data for into-English and from-English summarization in 15 languages. Especially, for the into-English summarization task, we crowd-source a high-quality evaluation dataset based on guidelines that emphasize accuracy, coverage, and understandability. To ensure the quality of this dataset, we collect human ratings to filter out bad summaries, and conduct a survey on humans, which shows that the remaining summaries are preferred over the social-network summaries. We study the effect of translation quality in cross-lingual summarization, comparing a translate-then-summarize approach with several baselines. Our results highlight the limitations of the ROUGE metric that are overlooked in monolingual summarization.",
}
@inproceedings{dai-yamaguchi-2019-compact,
    title = "Compact and Robust Models for {J}apanese-{E}nglish Character-level Machine Translation",
    author = "Dai, Jinan  and
      Yamaguchi, Kazunori",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5202",
    doi = "10.18653/v1/D19-5202",
    pages = "36--44",
    abstract = "Character-level translation has been proved to be able to achieve preferable translation quality without explicit segmentation, but training a character-level model needs a lot of hardware resources. In this paper, we introduced two character-level translation models which are mid-gated model and multi-attention model for Japanese-English translation. We showed that the mid-gated model achieved the better performance with respect to BLEU scores. We also showed that a relatively narrow beam of width 4 or 5 was sufficient for the mid-gated model. As for unknown words, we showed that the mid-gated model could somehow translate the one containing Katakana by coining out a close word. We also showed that the model managed to produce tolerable results for heavily noised sentences, even though the model was trained with the dataset without noise.",
}
@inproceedings{rikters-etal-2019-designing,
    title = "Designing the Business Conversation Corpus",
    author = "Rikters, Mat{\=\i}ss  and
      Ri, Ryokan  and
      Li, Tong  and
      Nakazawa, Toshiaki",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5204",
    doi = "10.18653/v1/D19-5204",
    pages = "54--61",
    abstract = "While the progress of machine translation of written text has come far in the past several years thanks to the increasing availability of parallel corpora and corpora-based training technologies, automatic translation of spoken text and dialogues remains challenging even for modern systems. In this paper, we aim to boost the machine translation quality of conversational texts by introducing a newly constructed Japanese-English business conversation parallel corpus. A detailed analysis of the corpus is provided along with challenging examples for automatic translation. We also experiment with adding the corpus in a machine translation training scenario and show how the resulting system benefits from its use.",
}
@inproceedings{mino-etal-2019-neural,
    title = "Neural Machine Translation System using a Content-equivalently Translated Parallel Corpus for the Newswire Translation Tasks at {WAT} 2019",
    author = "Mino, Hideya  and
      Ito, Hitoshi  and
      Goto, Isao  and
      Yamada, Ichiro  and
      Tanaka, Hideki  and
      Tokunaga, Takenobu",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5212",
    doi = "10.18653/v1/D19-5212",
    pages = "106--111",
    abstract = "This paper describes NHK and NHK Engineering System (NHK-ES){'}s submission to the newswire translation tasks of WAT 2019 in both directions of Japanese→English and English→Japanese. In addition to the JIJI Corpus that was officially provided by the task organizer, we developed a corpus of 0.22M sentence pairs by manually, translating Japanese news sentences into English content- equivalently. The content-equivalent corpus was effective for improving translation quality, and our systems achieved the best human evaluation scores in the newswire translation tasks at WAT 2019.",
}
@inproceedings{imankulova-etal-2019-japanese,
    title = "{J}apanese-{R}ussian {TMU} Neural Machine Translation System using Multilingual Model for {WAT} 2019",
    author = "Imankulova, Aizhan  and
      Kaneko, Masahiro  and
      Komachi, Mamoru",
    editor = "Nakazawa, Toshiaki  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Kunchukuttan, Anoop  and
      Doi, Nobushige  and
      Oda, Yusuke  and
      Bojar, Ond{\v{r}}ej  and
      Parida, Shantipriya  and
      Goto, Isao  and
      Mino, Hidaya",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5221",
    doi = "10.18653/v1/D19-5221",
    pages = "165--170",
    abstract = "We introduce our system that is submitted to the News Commentary task (Japanese{\textless}-{\textgreater}Russian) of the 6th Workshop on Asian Translation. The goal of this shared task is to study extremely low resource situations for distant language pairs. It is known that using parallel corpora of different language pair as training data is effective for multilingual neural machine translation model in extremely low resource scenarios. Therefore, to improve the translation quality of Japanese{\textless}-{\textgreater}Russian language pair, our method leverages other in-domain Japanese-English and English-Russian parallel corpora as additional training data for our multilingual NMT model.",
}
@inproceedings{yang-etal-2019-latent,
    title = "Latent Part-of-Speech Sequences for Neural Machine Translation",
    author = "Yang, Xuewen  and
      Liu, Yingru  and
      Xie, Dongliang  and
      Wang, Xin  and
      Balasubramanian, Niranjan",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1072",
    doi = "10.18653/v1/D19-1072",
    pages = "780--790",
    abstract = "Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the co-dependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.",
}
@inproceedings{bi-etal-2019-multi,
    title = "Multi-agent Learning for Neural Machine Translation",
    author = "Bi, Tianchi  and
      Xiong, Hao  and
      He, Zhongjun  and
      Wu, Hua  and
      Wang, Haifeng",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1079",
    doi = "10.18653/v1/D19-1079",
    pages = "856--865",
    abstract = "Conventional Neural Machine Translation (NMT) models benefit from the training with an additional agent, e.g., dual learning, and bidirectional decoding with one agent decod- ing from left to right and the other decoding in the opposite direction. In this paper, we extend the training framework to the multi-agent sce- nario by introducing diverse agents in an in- teractive updating process. At training time, each agent learns advanced knowledge from others, and they work together to improve translation quality. Experimental results on NIST Chinese-English, IWSLT 2014 German- English, WMT 2014 English-German and large-scale Chinese-English translation tasks indicate that our approach achieves absolute improvements over the strong baseline sys- tems and shows competitive performance on all tasks.",
}
@inproceedings{popel-federmann-2019-domain,
    title = "Domain Adaptation of Document-Level {NMT} in {IWSLT}19",
    author = "Popel, Martin  and
      Federmann, Christian",
    editor = {Niehues, Jan  and
      Cattoni, Rolando  and
      St{\"u}ker, Sebastian  and
      Negri, Matteo  and
      Turchi, Marco  and
      Ha, Thanh-Le  and
      Salesky, Elizabeth  and
      Sanabria, Ramon  and
      Barrault, Loic  and
      Specia, Lucia  and
      Federico, Marcello},
    booktitle = "Proceedings of the 16th International Conference on Spoken Language Translation",
    month = nov # " 2-3",
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2019.iwslt-1.8",
    abstract = "We describe our four NMT systems submitted to the IWSLT19 shared task in English→Czech text-to-text translation of TED talks. The goal of this study is to understand the interactions between document-level NMT and domain adaptation. All our systems are based on the Transformer model implemented in the Tensor2Tensor framework. Two of the systems serve as baselines, which are not adapted to the TED talks domain: SENTBASE is trained on single sen- tences, DOCBASE on multi-sentence (document-level) sequences. The other two submitted systems are adapted to TED talks: SENTFINE is fine-tuned on single sentences, DOCFINE is fine-tuned on multi-sentence sequences. We present both automatic-metrics evaluation and manual analysis of the translation quality, focusing on the differences between the four systems.",
}
@inproceedings{scarton-etal-2019-estimating,
    title = "Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of {MT} quality",
    author = "Scarton, Scarton  and
      Forcada, Mikel L.  and
      Espl{\`a}-Gomis, Miquel  and
      Specia, Lucia",
    editor = {Niehues, Jan  and
      Cattoni, Rolando  and
      St{\"u}ker, Sebastian  and
      Negri, Matteo  and
      Turchi, Marco  and
      Ha, Thanh-Le  and
      Salesky, Elizabeth  and
      Sanabria, Ramon  and
      Barrault, Loic  and
      Specia, Lucia  and
      Federico, Marcello},
    booktitle = "Proceedings of the 16th International Conference on Spoken Language Translation",
    month = nov # " 2-3",
    year = "2019",
    address = "Hong Kong",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2019.iwslt-1.23",
    abstract = "Devising metrics to assess translation quality has always been at the core of machine translation (MT) research. Traditional automatic reference-based metrics, such as BLEU, have shown correlations with human judgements of adequacy and fluency and have been paramount for the advancement of MT system development. Crowd-sourcing has popularised and enabled the scalability of metrics based on human judgments, such as subjective direct assessments (DA) of adequacy, that are believed to be more reliable than reference-based automatic metrics. Finally, task-based measurements, such as post-editing time, are expected to provide a more de- tailed evaluation of the usefulness of translations for a specific task. Therefore, while DA averages adequacy judgements to obtain an appraisal of (perceived) quality independently of the task, and reference-based automatic metrics try to objectively estimate quality also in a task-independent way, task-based metrics are measurements obtained either during or after performing a specific task. In this paper we argue that, although expensive, task-based measurements are the most reliable when estimating MT quality in a specific task; in our case, this task is post-editing. To that end, we report experiments on a dataset with newly-collected post-editing indicators and show their usefulness when estimating post-editing effort. Our results show that task-based metrics comparing machine-translated and post-edited versions are the best at tracking post-editing effort, as expected. These metrics are followed by DA, and then by metrics comparing the machine-translated version and independent references. We suggest that MT practitioners should be aware of these differences and acknowledge their implications when decid- ing how to evaluate MT for post-editing purposes.",
}
@inproceedings{bojar-etal-2018-findings,
    title = "Findings of the 2018 Conference on Machine Translation ({WMT}18)",
    author = "Bojar, Ond{\v{r}}ej  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Koehn, Philipp  and
      Monz, Christof",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6401",
    doi = "10.18653/v1/W18-6401",
    pages = "272--303",
    abstract = "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018. Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation.",
}
@inproceedings{casas-etal-2018-talp,
    title = "The {TALP}-{UPC} Machine Translation Systems for {WMT}18 News Shared Translation Task",
    author = "Casas, Noe  and
      Escolano, Carlos  and
      Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6406",
    doi = "10.18653/v1/W18-6406",
    pages = "355--360",
    abstract = "In this article we describe the TALP-UPC research group participation in the WMT18 news shared translation task for Finnish-English and Estonian-English within the multi-lingual subtrack. All of our primary submissions implement an attention-based Neural Machine Translation architecture. Given that Finnish and Estonian belong to the same language family and are similar, we use as training data the combination of the datasets of both language pairs to paliate the data scarceness of each individual pair. We also report the translation quality of systems trained on individual language pair data to serve as baseline and comparison reference.",
}
@inproceedings{gronroos-etal-2018-cognate,
    title = "Cognate-aware morphological segmentation for multilingual neural translation",
    author = {Gr{\"o}nroos, Stig-Arne  and
      Virpioja, Sami  and
      Kurimo, Mikko},
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6410",
    doi = "10.18653/v1/W18-6410",
    pages = "386--393",
    abstract = "This article describes the Aalto University entry to the WMT18 News Translation Shared Task. We participate in the multilingual subtrack with a system trained under the constrained condition to translate from English to both Finnish and Estonian. The system is based on the Transformer model. We focus on improving the consistency of morphological segmentation for words that are similar orthographically, semantically, and distributionally; such words include etymological cognates, loan words, and proper names. For this, we introduce Cognate Morfessor, a multilingual variant of the Morfessor method. We show that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model.",
}
@inproceedings{bojar-etal-2018-evald,
    title = "{E}val{D} Reference-Less Discourse Evaluation for {WMT}18",
    author = "Bojar, Ond{\v{r}}ej  and
      M{\'\i}rovsk{\'y}, Ji{\v{r}}{\'\i}  and
      Rysov{\'a}, Kate{\v{r}}ina  and
      Rysov{\'a}, Magdal{\'e}na",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6432",
    doi = "10.18653/v1/W18-6432",
    pages = "541--545",
    abstract = "We present the results of automatic evaluation of discourse in machine translation (MT) outputs using the EVALD tool. EVALD was originally designed and trained to assess the quality of \textit{human} writing, for native speakers and foreign-language learners. MT has seen a tremendous leap in translation quality at the level of sentences and it is thus interesting to see if the human-level evaluation is becoming relevant.",
}
@inproceedings{khan-etal-2018-hunter,
    title = "Hunter {NMT} System for {WMT}18 Biomedical Translation Task: Transfer Learning in Neural Machine Translation",
    author = "Khan, Abdul  and
      Panda, Subhadarshi  and
      Xu, Jia  and
      Flokas, Lampros",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6447",
    doi = "10.18653/v1/W18-6447",
    pages = "655--661",
    abstract = "This paper describes the submission of Hunter Neural Machine Translation (NMT) to the WMT{'}18 Biomedical translation task from English to French. The discrepancy between training and test data distribution brings a challenge to translate text in new domains. Beyond the previous work of combining in-domain with out-of-domain models, we found accuracy and efficiency gain in combining different in-domain models. We conduct extensive experiments on NMT with \textit{transfer learning}. We train on different in-domain Biomedical datasets one after another. That means parameters of the previous training serve as the initialization of the next one. Together with a pre-trained out-of-domain News model, we enhanced translation quality with 3.73 BLEU points over the baseline. Furthermore, we applied ensemble learning on training models of intermediate epochs and achieved an improvement of 4.02 BLEU points over the baseline. Overall, our system is 11.29 BLEU points above the best system of last year on the EDP 2017 test set.",
}
@inproceedings{hu-etal-2018-contextual,
    title = "Contextual Encoding for Translation Quality Estimation",
    author = "Hu, Junjie  and
      Chang, Wei-Cheng  and
      Wu, Yuexin  and
      Neubig, Graham",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6462",
    doi = "10.18653/v1/W18-6462",
    pages = "788--793",
    abstract = "The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong. In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach. The first part uses an embedding layer to represent words and their part-of-speech tags in both languages. The second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.",
}
@inproceedings{sanchez-martinez-etal-2018-ualacant,
    title = "{UA}lacant machine translation quality estimation at {WMT} 2018: a simple approach using phrase tables and feed-forward neural networks",
    author = "S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Forcada, Mikel L.",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6464",
    doi = "10.18653/v1/W18-6464",
    pages = "801--808",
    abstract = "We describe the Universitat d{'}Alacant submissions to the word- and sentence-level machine translation (MT) quality estimation (QE) shared task at WMT 2018. Our approach to word-level MT QE builds on previous work to mark the words in the machine-translated sentence as \textit{OK} or \textit{BAD}, and is extended to determine if a word or sequence of words need to be inserted in the gap after each word. Our sentence-level submission simply uses the edit operations predicted by the word-level approach to approximate TER. The method presented ranked first in the sub-task of identifying insertions in gaps for three out of the six datasets, and second in the rest of them.",
}
@inproceedings{wang-etal-2018-alibaba,
    title = "{A}libaba Submission for {WMT}18 Quality Estimation Task",
    author = "Wang, Jiayi  and
      Fan, Kai  and
      Li, Bo  and
      Zhou, Fengming  and
      Chen, Boxing  and
      Shi, Yangbin  and
      Si, Luo",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6465",
    doi = "10.18653/v1/W18-6465",
    pages = "809--815",
    abstract = "The goal of WMT 2018 Shared Task on Translation Quality Estimation is to investigate automatic methods for estimating the quality of machine translation results without reference translations. This paper presents the QE Brain system, which proposes the neural Bilingual Expert model as a feature extractor based on conditional target language model with a bidirectional transformer and then processes the semantic representations of source and the translation output with a Bi-LSTM predictive model for automatic quality estimation. The system has been applied to the sentence-level scoring and ranking tasks as well as the word-level tasks for finding errors for each word in translations. An extensive set of experimental results have shown that our system outperformed the best results in WMT 2017 Quality Estimation tasks and obtained top results in WMT 2018.",
}
@inproceedings{lu-etal-2018-alibaba,
    title = "{A}libaba Submission to the {WMT}18 Parallel Corpus Filtering Task",
    author = "Lu, Jun  and
      Lv, Xiaoyu  and
      Shi, Yangbin  and
      Chen, Boxing",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6482",
    doi = "10.18653/v1/W18-6481",
    pages = "917--922",
    abstract = "This paper describes the Alibaba Machine Translation Group submissions to the WMT 2018 Shared Task on Parallel Corpus Filtering. While evaluating the quality of the parallel corpus, the three characteristics of the corpus are investigated, i.e. 1) the bilingual/translation quality, 2) the monolingual quality and 3) the corpus diversity. Both rule-based and model-based methods are adapted to score the parallel sentence pairs. The final parallel corpus filtering system is reliable, easy to build and adapt to other language pairs.",
}
@inproceedings{pinnis-2018-tildes,
    title = "Tilde{'}s Parallel Corpus Filtering Methods for {WMT} 2018",
    author = "Pinnis, M{\=a}rcis",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6486",
    doi = "10.18653/v1/W18-6486",
    pages = "939--945",
    abstract = "The paper describes parallel corpus filtering methods that allow reducing noise of noisy {``}parallel{''} corpora from a level where the corpora are not usable for neural machine translation training (i.e., the resulting systems fail to achieve reasonable translation quality; well below 10 BLEU points) up to a level where the trained systems show decent (over 20 BLEU points on a 10 million word dataset and up to 30 BLEU points on a 100 million word dataset). The paper also documents Tilde{'}s submissions to the WMT 2018 shared task on parallel corpus filtering.",
}
@inproceedings{muller-etal-2018-large,
    title = "A Large-Scale Test Set for the Evaluation of Context-Aware Pronoun Translation in Neural Machine Translation",
    author = {M{\"u}ller, Mathias  and
      Rios, Annette  and
      Voita, Elena  and
      Sennrich, Rico},
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6307",
    doi = "10.18653/v1/W18-6307",
    pages = "61--72",
    abstract = "The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.",
}
@inproceedings{pappas-etal-2018-beyond,
    title = "Beyond Weight Tying: Learning Joint Input-Output Embeddings for Neural Machine Translation",
    author = "Pappas, Nikolaos  and
      Miculicich, Lesly  and
      Henderson, James",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Specia, Lucia  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6308",
    doi = "10.18653/v1/W18-6308",
    pages = "73--83",
    abstract = "Tying the weights of the target word embeddings with the target word classifiers of neural machine translation models leads to faster training and often to better translation quality. Given the success of this parameter sharing, we investigate other forms of sharing in between no sharing and hard equality of parameters. In particular, we propose a \textit{structure-aware} output layer which captures the semantic structure of the output space of words within a joint input-output embedding. The model is a generalized form of \textit{weight tying} which shares parameters but allows learning a more flexible relationship with input word embeddings and allows the effective capacity of the output layer to be controlled. In addition, the model shares weights across output classifiers and translation contexts which allows it to better leverage prior knowledge about them. Our evaluation on English-to-Finnish and English-to-German datasets shows the effectiveness of the method against strong encoder-decoder baselines trained with or without \textit{weight tying}.",
}
@inproceedings{raganato-tiedemann-2018-analysis,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Tiedemann, J{\"o}rg},
    editor = "Linzen, Tal  and
      Chrupa{\l}a, Grzegorz  and
      Alishahi, Afra",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5431",
    doi = "10.18653/v1/W18-5431",
    pages = "287--297",
    abstract = "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the \textit{Transformer} is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.",
}
@inproceedings{hoang-etal-2018-iterative,
    title = "Iterative Back-Translation for Neural Machine Translation",
    author = "Hoang, Vu Cong Duy  and
      Koehn, Philipp  and
      Haffari, Gholamreza  and
      Cohn, Trevor",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2703",
    doi = "10.18653/v1/W18-2703",
    pages = "18--24",
    abstract = "We present iterative back-translation, a method for generating increasingly better synthetic parallel data from monolingual data to train neural machine translation systems. Our proposed method is very simple yet effective and highly applicable in practice. We demonstrate improvements in neural machine translation quality in both high and low resourced scenarios, including the best reported BLEU scores for the WMT 2017 German↔English tasks.",
}
@inproceedings{bisk-tran-2018-inducing,
    title = "Inducing Grammars with and for Neural Machine Translation",
    author = "Bisk, Yonatan  and
      Tran, Ke",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2704",
    doi = "10.18653/v1/W18-2704",
    pages = "25--35",
    abstract = "Machine translation systems require semantic knowledge and grammatical understanding. Neural machine translation (NMT) systems often assume this information is captured by an attention mechanism and a decoder that ensures fluency. Recent work has shown that incorporating explicit syntax alleviates the burden of modeling both types of knowledge. However, requiring parses is expensive and does not explore the question of what syntax a model needs during translation. To address both of these issues we introduce a model that simultaneously translates while inducing dependency trees. In this way, we leverage the benefits of structure while investigating what syntax NMT must induce to maximize performance. We show that our dependency trees are 1. language pair dependent and 2. improve translation quality.",
}
@inproceedings{imamura-etal-2018-enhancement,
    title = "Enhancement of Encoder and Attention Using Target Monolingual Corpora in Neural Machine Translation",
    author = "Imamura, Kenji  and
      Fujita, Atsushi  and
      Sumita, Eiichiro",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2707",
    doi = "10.18653/v1/W18-2707",
    pages = "55--63",
    abstract = "A large-scale parallel corpus is required to train encoder-decoder neural machine translation. The method of using synthetic parallel texts, in which target monolingual corpora are automatically translated into source sentences, is effective in improving the decoder, but is unreliable for enhancing the encoder. In this paper, we propose a method that enhances the encoder and attention using target monolingual corpora by generating multiple source sentences via sampling. By using multiple source sentences, diversity close to that of humans is achieved. Our experimental results show that the translation quality is improved by increasing the number of synthetic source sentences for each given target sentence, and quality close to that using a manually created parallel corpus was achieved.",
}
@inproceedings{imamura-sumita-2018-nict,
    title = "{NICT} Self-Training Approach to Neural Machine Translation at {NMT}-2018",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    editor = "Birch, Alexandra  and
      Finch, Andrew  and
      Luong, Thang  and
      Neubig, Graham  and
      Oda, Yusuke",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-2713",
    doi = "10.18653/v1/W18-2713",
    pages = "110--115",
    abstract = "This paper describes the NICT neural machine translation system submitted at the NMT-2018 shared task. A characteristic of our approach is the introduction of self-training. Since our self-training does not change the model structure, it does not influence the efficiency of translation, such as the translation speed. The experimental results showed that the translation quality improved not only in the sequence-to-sequence (seq-to-seq) models but also in the transformer models.",
}
@proceedings{ws-2018-amta-2018,
    title = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2100",
}
@inproceedings{ueffing-2018-automatic,
    title = "Automatic Post-Editing and Machine Translation Quality Estimation at e{B}ay",
    author = "Ueffing, Nicola",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2101",
    pages = "1--34",
}
@inproceedings{knowles-koehn-2018-lightweight,
    title = "Lightweight Word-Level Confidence Estimation for Neural Interactive Translation Prediction",
    author = "Knowles, Rebecca  and
      Koehn, Philipp",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2102",
    pages = "35--40",
}
@inproceedings{graca-2018-unbabel,
    title = "{U}nbabel: How to combine {AI} with the crowd to scale professional-quality translation",
    author = "Gra{\c{c}}a, Jo{\~a}o",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2103",
    pages = "41--85",
}
@inproceedings{khalilov-2018-machine,
    title = "Machine translation at Booking.com: what{'}s next?",
    author = "Khalilov, Maxim",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2104",
    pages = "86--143",
}
@inproceedings{junczys-dowmunt-2018-experiencing,
    title = "Are we experiencing the Golden Age of Automatic Post-Editing?",
    author = "Junczys-Dowmunt, Marcin",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2105",
    pages = "144--206",
}
@inproceedings{federico-2018-challenges,
    title = "Challenges in Adaptive Neural Machine Translation",
    author = "Federico, Marcello",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2106",
    pages = "207--242",
}
@inproceedings{avramidis-etal-2018-fine,
    title = "Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically motivated Test Suite",
    author = "Avramidis, Eleftherios  and
      Macketanz, Vivien  and
      Lommel, Arle  and
      Uszkoreit, Hans",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2107",
    pages = "243--248",
}
@inproceedings{knowles-etal-2018-comparison,
    title = "A Comparison of Machine Translation Paradigms for Use in Black-Box Fuzzy-Match Repair",
    author = "Knowles, Rebecca  and
      Ortega, John  and
      Koehn, Philipp",
    editor = "Astudillo, Ram{\'o}n  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2108",
    pages = "249--255",
}
@inproceedings{rivers-2018-translation,
    title = "Translation Quality Standards",
    author = "Rivers, Bill",
    editor = "DeCamp, Jennifer",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on The Role of Authoritative Standards in the {MT} Environment",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2004",
    pages = "56--68",
}
@inproceedings{lommel-2018-translation,
    title = "Translation Quality Metrics",
    author = "Lommel, Arle",
    editor = "DeCamp, Jennifer",
    booktitle = "Proceedings of the {AMTA} 2018 Workshop on The Role of Authoritative Standards in the {MT} Environment",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-2005",
    pages = "69--94",
}
@inproceedings{lommel-melby-2018-tutorial,
    title = "{T}utorial: {MQM}-{DQF}: A Good Marriage (Translation Quality for the 21st Century)",
    author = "Lommel, Arle  and
      Melby, Alan",
    editor = "Campbell, Janice  and
      Yanishevsky, Alex  and
      Doyon, Jennifer  and
      Jones, Doug",
    booktitle = "Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 2: User Track)",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-1925",
}
@inproceedings{lohar-etal-2018-balancing,
    title = "Balancing Translation Quality and Sentiment Preservation (Non-archival Extended Abstract)",
    author = "Lohar, Pintu  and
      Afli, Haithem  and
      Way, Andy",
    editor = "Cherry, Colin  and
      Neubig, Graham",
    booktitle = "Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 1: Research Track)",
    month = mar,
    year = "2018",
    address = "Boston, MA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/W18-1808",
    pages = "81--88",
}
@article{zheng-etal-2018-modeling,
    title = "Modeling Past and Future for Neural Machine Translation",
    author = "Zheng, Zaixiang  and
      Zhou, Hao  and
      Huang, Shujian  and
      Mou, Lili  and
      Dai, Xinyu  and
      Chen, Jiajun  and
      Tu, Zhaopeng",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1011",
    doi = "10.1162/tacl_a_00011",
    pages = "145--157",
    abstract = "Existing neural machine translation systems do not explicitly model what has been translated and what has not during the decoding phase. To address this problem, we propose a novel mechanism that separates the source information into two parts: translated Past contents and untranslated Future contents, which are modeled by two additional recurrent layers. The Past and Future contents are fed to both the attention model and the decoder states, which provides Neural Machine Translation (NMT) systems with the knowledge of translated and untranslated contents. Experimental results show that the proposed approach significantly improves the performance in Chinese-English, German-English, and English-German translation tasks. Specifically, the proposed model outperforms the conventional coverage model in terms of both the translation quality and the alignment error rate.",
}
@inproceedings{kawara-etal-2018-recursive,
    title = "Recursive Neural Network Based Preordering for {E}nglish-to-{J}apanese Machine Translation",
    author = "Kawara, Yuki  and
      Chu, Chenhui  and
      Arase, Yuki",
    editor = "Shwartz, Vered  and
      Tabassum, Jeniya  and
      Voigt, Rob  and
      Che, Wanxiang  and
      de Marneffe, Marie-Catherine  and
      Nissim, Malvina",
    booktitle = "Proceedings of {ACL} 2018, Student Research Workshop",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-3004",
    doi = "10.18653/v1/P18-3004",
    pages = "21--27",
    abstract = "The word order between source and target languages significantly influences the translation quality. Preordering can effectively address this problem. Previous preordering methods require a manual feature design, making language dependent design difficult. In this paper, we propose a preordering method with recursive neural networks that learn features from raw inputs. Experiments show the proposed method is comparable to the state-of-the-art method but without a manual feature design.",
}
@inproceedings{ren-etal-2018-triangular,
    title = "Triangular Architecture for Rare Language Translation",
    author = "Ren, Shuo  and
      Chen, Wenhu  and
      Liu, Shujie  and
      Li, Mu  and
      Zhou, Ming  and
      Ma, Shuai",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1006",
    doi = "10.18653/v1/P18-1006",
    pages = "56--65",
    abstract = "Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.",
}
@inproceedings{cifka-bojar-2018-bleu,
    title = "Are {BLEU} and Meaning Representation in Opposition?",
    author = "C{\'\i}fka, Ond{\v{r}}ej  and
      Bojar, Ond{\v{r}}ej",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1126",
    doi = "10.18653/v1/P18-1126",
    pages = "1362--1371",
    abstract = "One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",
}
@inproceedings{cheng-etal-2018-towards,
    title = "Towards Robust Neural Machine Translation",
    author = "Cheng, Yong  and
      Tu, Zhaopeng  and
      Meng, Fandong  and
      Zhai, Junjie  and
      Liu, Yang",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1163",
    doi = "10.18653/v1/P18-1163",
    pages = "1756--1766",
    abstract = "Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.",
}
@inproceedings{hewitt-etal-2018-learning,
    title = "Learning Translations via Images with a Massively Multilingual Image Dataset",
    author = "Hewitt, John  and
      Ippolito, Daphne  and
      Callahan, Brendan  and
      Kriz, Reno  and
      Wijaya, Derry Tanti  and
      Callison-Burch, Chris",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1239",
    doi = "10.18653/v1/P18-1239",
    pages = "2566--2576",
    abstract = "We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. {\%}We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at \url{http://multilingual-images.org/}.",
}
@inproceedings{ueffing-etal-2018-quality,
    title = "Quality Estimation for Automatically Generated Titles of e{C}ommerce Browse Pages",
    author = "Ueffing, Nicola  and
      C. de Souza, Jos{\'e} G.  and
      Leusch, Gregor",
    editor = "Bangalore, Srinivas  and
      Chu-Carroll, Jennifer  and
      Li, Yunyao",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans - Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-3007",
    doi = "10.18653/v1/N18-3007",
    pages = "52--59",
    abstract = "At eBay, we are automatically generating a large amount of natural language titles for eCommerce browse pages using machine translation (MT) technology. While automatic approaches can generate millions of titles very fast, they are prone to errors. We therefore develop quality estimation (QE) methods which can automatically detect titles with low quality in order to prevent them from going live. In this paper, we present different approaches: The first one is a Random Forest (RF) model that explores hand-crafted, robust features, which are a mix of established features commonly used in Machine Translation Quality Estimation (MTQE) and new features developed specifically for our task. The second model is based on Siamese Networks (SNs) which embed the metadata input sequence and the generated title in the same space and do not require hand-crafted features at all. We thoroughly evaluate and compare those approaches on in-house data. While the RF models are competitive for scenarios with smaller amounts of training data and somewhat more robust, they are clearly outperformed by the SN models when the amount of training data is larger.",
}
@inproceedings{kreutzer-etal-2018-neural,
    title = "Can Neural Machine Translation be Improved with User Feedback?",
    author = "Kreutzer, Julia  and
      Khadivi, Shahram  and
      Matusov, Evgeny  and
      Riezler, Stefan",
    editor = "Bangalore, Srinivas  and
      Chu-Carroll, Jennifer  and
      Li, Yunyao",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans - Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-3012",
    doi = "10.18653/v1/N18-3012",
    pages = "92--105",
    abstract = "We present the first real-world application of methods for improving neural machine translation (NMT) with human reinforcement, based on explicit and implicit user feedback collected on the eBay e-commerce platform. Previous work has been confined to simulation experiments, whereas in this paper we work with real logged feedback for offline bandit learning of NMT parameters. We conduct a thorough analysis of the available explicit user judgments{---}five-star ratings of translation quality{---}and show that they are not reliable enough to yield significant improvements in bandit learning. In contrast, we successfully utilize implicit task-based feedback collected in a cross-lingual search task to improve task-specific and machine translation quality metrics.",
}
@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}
@inproceedings{macketanz-etal-2018-tq,
    title = "{TQ}-{A}uto{T}est {--} An Automated Test Suite for (Machine) Translation Quality",
    author = "Macketanz, Vivien  and
      Ai, Renlong  and
      Burchardt, Aljoscha  and
      Uszkoreit, Hans",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1142",
}
@inproceedings{yuan-sharoff-2018-investigating,
    title = "Investigating the Influence of Bilingual {MWU} on Trainee Translation Quality",
    author = "Yuan, Yu  and
      Sharoff, Serge",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1312",
}
@inproceedings{yuan-etal-2018-cross,
    title = "Cross-lingual Terminology Extraction for Translation Quality Estimation",
    author = "Yuan, Yu  and
      Gao, Yuze  and
      Zhang, Yue  and
      Sharoff, Serge",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Hasida, Koiti  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tokunaga, Takenobu",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L18-1596",
}
@inproceedings{peris-casacuberta-2018-active,
    title = "Active Learning for Interactive Neural Machine Translation of Data Streams",
    author = "Peris, {\'A}lvaro  and
      Casacuberta, Francisco",
    editor = "Korhonen, Anna  and
      Titov, Ivan",
    booktitle = "Proceedings of the 22nd Conference on Computational Natural Language Learning",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K18-1015",
    doi = "10.18653/v1/K18-1015",
    pages = "151--160",
    abstract = "We study the application of active learning techniques to the translation of unbounded data streams via interactive neural machine translation. The main idea is to select, from an unbounded stream of source sentences, those worth to be supervised by a human agent. The user will interactively translate those samples. Once validated, these data is useful for adapting the neural machine translation model. We propose two novel methods for selecting the samples to be validated. We exploit the information from the attention mechanism of a neural machine translation system. Our experiments show that the inclusion of active learning techniques into this pipeline allows to reduce the effort required during the process, while increasing the quality of the translation system. Moreover, it enables to balance the human effort required for achieving a certain translation quality. Moreover, our neural system outperforms classical approaches by a large margin.",
}
@inproceedings{wang-etal-2018-cytonmt,
    title = "{C}yton{MT}: an Efficient Neural Machine Translation Open-source Toolkit Implemented in {C}++",
    author = "Wang, Xiaolin  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Blanco, Eduardo  and
      Lu, Wei",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2023",
    doi = "10.18653/v1/D18-2023",
    pages = "133--138",
    abstract = "This paper presents an open-source neural machine translation toolkit named CytonMT. The toolkit is built from scratch only using C++ and NVIDIA{'}s GPU-accelerated libraries. The toolkit features training efficiency, code simplicity and translation quality. Benchmarks show that cytonMT accelerates the training speed by 64.5{\%} to 110.8{\%} on neural networks of various sizes, and achieves competitive translation quality.",
}
@inproceedings{chen-etal-2018-stable,
    title = "A Stable and Effective Learning Strategy for Trainable Greedy Decoding",
    author = "Chen, Yun  and
      Li, Victor O.K.  and
      Cho, Kyunghyun  and
      Bowman, Samuel",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1035",
    doi = "10.18653/v1/D18-1035",
    pages = "380--390",
    abstract = "Beam search is a widely used approximate search strategy for neural network decoders, and it generally outperforms simple greedy decoding on tasks like machine translation. However, this improvement comes at substantial computational cost. In this paper, we propose a flexible new method that allows us to reap nearly the full benefits of beam search with nearly no additional computational cost. The method revolves around a small neural network actor that is trained to observe and manipulate the hidden state of a previously-trained decoder. To train this actor network, we introduce the use of a pseudo-parallel corpus built using the output of beam search on a base model, ranked by a target quality metric like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system.",
}
@inproceedings{zhao-etal-2018-addressing,
    title = "Addressing Troublesome Words in Neural Machine Translation",
    author = "Zhao, Yang  and
      Zhang, Jiajun  and
      He, Zhongjun  and
      Zong, Chengqing  and
      Wu, Hua",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1036",
    doi = "10.18653/v1/D18-1036",
    pages = "391--400",
    abstract = "One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.",
}
@inproceedings{fadaee-monz-2018-back,
    title = "Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation",
    author = "Fadaee, Marzieh  and
      Monz, Christof",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1040",
    doi = "10.18653/v1/D18-1040",
    pages = "436--446",
    abstract = "Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.",
}
@inproceedings{wang-etal-2018-semi-autoregressive,
    title = "Semi-Autoregressive Neural Machine Translation",
    author = "Wang, Chunqi  and
      Zhang, Ji  and
      Chen, Haiqing",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1044",
    doi = "10.18653/v1/D18-1044",
    pages = "479--488",
    abstract = "Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation {---} the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT{'}14 English-German translation, the SAT achieves 5.58{\mbox{$\times$}} speedup while maintaining 88{\%} translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1{\%} degeneration in BLEU score).",
}
@inproceedings{wuebker-etal-2018-compact,
    title = "Compact Personalized Models for Neural Machine Translation",
    author = "Wuebker, Joern  and
      Simianer, Patrick  and
      DeNero, John",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1104",
    doi = "10.18653/v1/D18-1104",
    pages = "881--886",
    abstract = "We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture{--}combining a state-of-the-art self-attentive model with compact domain adaptation{--}provides high quality personalized machine translation that is both space and time efficient.",
}
@inproceedings{bisazza-tump-2018-lazy,
    title = "The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation",
    author = "Bisazza, Arianna  and
      Tump, Clara",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1313",
    doi = "10.18653/v1/D18-1313",
    pages = "2871--2876",
    abstract = "Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.",
}
@inproceedings{vanmassenhove-etal-2018-getting,
    title = "Getting Gender Right in Neural Machine Translation",
    author = "Vanmassenhove, Eva  and
      Hardmeier, Christian  and
      Way, Andy",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1334",
    doi = "10.18653/v1/D18-1334",
    pages = "3003--3008",
    abstract = "Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying {``}I am happy{''} in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained/recovered. The same sentence would become either {``}Je suis heureux{''}, for a male speaker or {``}Je suis heureuse{''} for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is two-fold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.",
}
@inproceedings{libovicky-helcl-2018-end,
    title = "End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification",
    author = "Libovick{\'y}, Jind{\v{r}}ich  and
      Helcl, Jind{\v{r}}ich",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1336",
    doi = "10.18653/v1/D18-1336",
    pages = "3016--3021",
    abstract = "Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.",
}
@inproceedings{alinejad-etal-2018-prediction,
    title = "Prediction Improves Simultaneous Neural Machine Translation",
    author = "Alinejad, Ashkan  and
      Siahbani, Maryam  and
      Sarkar, Anoop",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1337",
    doi = "10.18653/v1/D18-1337",
    pages = "3022--3027",
    abstract = "Simultaneous speech translation aims to maintain translation quality while minimizing the delay between reading input and incrementally producing the output. We propose a new general-purpose prediction action which predicts future words in the input to improve quality and minimize delay in simultaneous translation. We train this agent using reinforcement learning with a novel reward function. Our agent with prediction has better translation quality and less delay compared to an agent-based simultaneous translation system without prediction.",
}
@inproceedings{yang-etal-2018-breaking,
    title = "Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation",
    author = "Yang, Yilin  and
      Huang, Liang  and
      Ma, Mingbo",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1342",
    doi = "10.18653/v1/D18-1342",
    pages = "3054--3059",
    abstract = "Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.",
}
@inproceedings{zhang-etal-2018-simplifying,
    title = "Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks",
    author = "Zhang, Biao  and
      Xiong, Deyi  and
      Su, Jinsong  and
      Lin, Qian  and
      Zhang, Huiji",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1459",
    doi = "10.18653/v1/D18-1459",
    pages = "4273--4283",
    abstract = "In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",
}
@inproceedings{zhang-etal-2018-speeding,
    title = "Speeding Up Neural Machine Translation Decoding by Cube Pruning",
    author = "Zhang, Wen  and
      Huang, Liang  and
      Feng, Yang  and
      Shen, Lei  and
      Liu, Qun",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1460",
    doi = "10.18653/v1/D18-1460",
    pages = "4284--4294",
    abstract = "Although neural machine translation has achieved promising results, it suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3x on GPUs and 3.5x on CPUs.",
}
@inproceedings{zhang-etal-2018-exploring,
    title = "Exploring Recombination for Efficient Decoding of Neural Machine Translation",
    author = "Zhang, Zhisong  and
      Wang, Rui  and
      Utiyama, Masao  and
      Sumita, Eiichiro  and
      Zhao, Hai",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1511",
    doi = "10.18653/v1/D18-1511",
    pages = "4785--4790",
    abstract = "In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the {``}equivalence{''} of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.",
}
@inproceedings{lakew-etal-2018-comparison,
    title = "A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation",
    author = "Lakew, Surafel Melaku  and
      Cettolo, Mauro  and
      Federico, Marcello",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1054",
    pages = "641--652",
    abstract = "Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.",
}
@inproceedings{blackwood-etal-2018-multilingual,
    title = "Multilingual Neural Machine Translation with Task-Specific Attention",
    author = "Blackwood, Graeme  and
      Ballesteros, Miguel  and
      Ward, Todd",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1263",
    pages = "3112--3122",
    abstract = "Multilingual machine translation addresses the task of translating between multiple source and target languages. We propose task-specific attention models, a simple but effective technique for improving the quality of sequence-to-sequence neural multilingual translation. Our approach seeks to retain as much of the parameter sharing generalization of NMT models as possible, while still allowing for language-specific specialization of the attention model to a particular language-pair or task. Our experiments on four languages of the Europarl corpus show that using a target-specific model of attention provides consistent gains in translation quality for all possible translation directions, compared to a model in which all parameters are shared. We observe improved translation quality even in the (extreme) low-resource zero-shot translation directions for which the model never saw explicitly paired parallel data.",
}
@inproceedings{poncelas-etal-2018-adapt,
    title = "The {ADAPT} System Description for the {IWSLT} 2018 {B}asque to {E}nglish Translation Task",
    author = "Poncelas, Alberto  and
      Way, Andy  and
      Sarasola, Kepa",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.11",
    pages = "76--82",
    abstract = "In this paper we present the ADAPT system built for the Basque to English Low Resource MT Evaluation Campaign. Basque is a low-resourced, morphologically-rich language. This poses a challenge for Neural Machine Translation models which usually achieve better performance when trained with large sets of data. Accordingly, we used synthetic data to improve the translation quality produced by a model built using only authentic data. Our proposal uses back-translated data to: (a) create new sentences, so the system can be trained with more data; and (b) translate sentences that are close to the test set, so the model can be fine-tuned to the document to be translated.",
}
@inproceedings{sanchez-cartagena-2018-prompsit-,
    title = "Prompsit{'}s Submission to the {IWSLT} 2018 Low Resource Machine Translation Task",
    author = "S{\'a}nchez-Cartagena, V{\'\i}ctor M.",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.14",
    pages = "95--103",
    abstract = "This paper presents Prompsit Language Engineering{'}s submission to the IWSLT 2018 Low Resource Machine Translation task. Our submission is based on cross-lingual learning: a multilingual neural machine translation system was created with the sole purpose of improving translation quality on the Basque-to-English language pair. The multilingual system was trained on a combination of in-domain data, pseudo in-domain data obtained via cross-entropy data selection and backtranslated data. We morphologically segmented Basque text with a novel approach that only requires a dictionary such as those used by spell checkers and proved that this segmentation approach outperforms the widespread byte pair encoding strategy for this task.",
}
@inproceedings{bach-etal-2018-alibaba,
    title = "{A}libaba Speech Translation Systems for {IWSLT} 2018",
    author = "Bach, Nguyen  and
      Chen, Hongjie  and
      Fan, Kai  and
      Leung, Cheung-Chi  and
      Li, Bo  and
      Ni, Chongjia  and
      Tong, Rong  and
      Zhang, Pei  and
      Chen, Boxing  and
      Ma, Bin  and
      Huang, Fei",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.20",
    pages = "136--141",
    abstract = "This work describes the En→De Alibaba speech translation system developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2018. In order to improve ASR performance, multiple ASR models including conventional and end-to-end models are built, then we apply model fusion in the final step. ASR pre and post-processing techniques such as speech segmentation, punctuation insertion, and sentence splitting are found to be very useful for MT. We also employed most techniques that have proven effective during the WMT 2018 evaluation, such as BPE, back translation, data selection, model ensembling and reranking. These ASR and MT techniques, combined, improve the speech translation quality significantly.",
}
@inproceedings{effendi-etal-2018-multi,
    title = "Multi-paraphrase Augmentation to Leverage Neural Caption Translation",
    author = "Effendi, Johanes  and
      Sakti, Sakriani  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.27",
    pages = "181--188",
    abstract = "Paraphrasing has been proven to improve translation quality in machine translation (MT) and has been widely studied alongside with the development of statistical MT (SMT). In this paper, we investigate and utilize neural paraphrasing to improve translation quality in neural MT (NMT), which has not yet been much explored. Our first contribution is to propose a new way of creating a multi-paraphrase corpus through visual description. After that, we also proposed to construct neural paraphrase models which initiate expert models and utilize them to leverage NMT. Here, we diffuse the image information by using image-based paraphrasing without using the image itself. Our proposed image-based multi-paraphrase augmentation strategies showed improvement against a vanilla NMT baseline.",
}
@inproceedings{osamura-etal-2018-using,
    title = "Using Spoken Word Posterior Features in Neural Machine Translation",
    author = "Osamura, Kaho  and
      Kano, Takatomo  and
      Sakti, Sakriani  and
      Sudoh, Katsuhito  and
      Nakamura, Satoshi",
    editor = "Turchi, Marco  and
      Niehues, Jan  and
      Frederico, Marcello",
    booktitle = "Proceedings of the 15th International Conference on Spoken Language Translation",
    month = oct # " 29-30",
    year = "2018",
    address = "Brussels",
    publisher = "International Conference on Spoken Language Translation",
    url = "https://aclanthology.org/2018.iwslt-1.28",
    pages = "189--195",
    abstract = "A spoken language translation (ST) system consists of at least two modules: an automatic speech recognition (ASR) system and a machine translation (MT) system. In most cases, an MT is only trained and optimized using error-free text data. If the ASR makes errors, the translation accuracy will be greatly reduced. Existing studies have shown that training MT systems with ASR parameters or word lattices can improve the translation quality. However, such an extension requires a large change in standard MT systems, resulting in a complicated model that is hard to train. In this paper, a neural sequence-to-sequence ASR is used as feature processing that is trained to produce word posterior features given spoken utterances. The resulting probabilistic features are used to train a neural MT (NMT) with only a slight modification. Experimental results reveal that the proposed method improved up to 5.8 BLEU scores with synthesized speech or 4.3 BLEU scores with the natural speech in comparison with a conventional cascaded-based ST system that translates from the 1-best ASR candidates.",
}
@inproceedings{aranberri-pascual-2018-towards,
    title = "Towards a post-editing recommendation system for {S}panish-{B}asque machine translation",
    author = "Aranberri, Nora  and
      Pascual, Jose A.",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.2",
    pages = "41--50",
    abstract = "The overall machine translation quality available for professional translators working with the Spanish{--}Basque pair is rather poor, which is a deterrent for its adoption. This work investigates the plausibility of building a comprehensive recommendation system to speed up decision time between post-editing or translation from scratch using the very limited training data available. First, we build a set of regression models that predict the post-editing effort in terms of overall quality, time and edits. Secondly, we build classification models that recommend the most efficient editing approach using post-editing effort features on top of linguistic features. Results show high correlations between the predictions of the regression models and the expected HTER, time and edit number values. Similarly, the results for the classifiers show that they are able to predict with high accuracy whether it is more efficient to translate or to post-edit a new segment.",
}
@inproceedings{beloucif-wu-2018-srl,
    title = "{SRL} for low resource languages isn{'}t needed for semantic {SMT}",
    author = "Beloucif, Meriem  and
      Wu, Dekai",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.6",
    pages = "79--88",
    abstract = "Previous attempts at injecting semantic frame biases into SMT training for low resource languages failed because either (a) no semantic parser is available for the low resource input language; or (b) the output English language semantic parses excise relevant parts of the alignment space too aggressively. We present the first semantic SMT model to succeed in significantly improving translation quality across many low resource input languages for which no automatic SRL is available {---}consistently and across all common MT metrics. The results we report are the best by far to date for this type of approach; our analyses suggest that in general, easier approaches toward including semantics in training SMT models may be more feasible than generally assumed even for low resource languages where semantic parsers remain scarce. While recent proposals to use the crosslingual evaluation metric XMEANT during inversion transduction grammar (ITG) induction are inapplicable to low resource languages that lack semantic parsers, we break the bottleneck via a vastly improved method of biasing ITG induction toward learning more semantically correct alignments using the monolingual semantic evaluation metric MEANT. Unlike XMEANT, MEANT requires only a readily-available English (output language) semantic parser. The advances we report here exploit the novel realization that MEANT represents an excellent way to semantically bias expectationmaximization induction even for low resource languages. We test our systems on challenging languages including Amharic, Uyghur, Tigrinya and Oromo. Results show that our model influences the learning towards more semantically correct alignments, leading to better translation quality than both the standard ITG or GIZA++ based SMT training models on different datasets.",
}
@inproceedings{chinea-rios-etal-2018-automatic,
    title = "Are Automatic Metrics Robust and Reliable in Specific Machine Translation Tasks?",
    author = "Chinea-Rios, Mara  and
      Peris, Alvaro  and
      Casacuberta, Francisco",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.9",
    pages = "109--118",
    abstract = "We present a comparison of automatic metrics against human evaluations of translation quality in several scenarios which were unexplored up to now. Our experimentation was conducted on translation hypotheses that were problematic for the automatic metrics, as the results greatly diverged from one metric to another. We also compared three different translation technologies. Our evaluation shows that in most cases, the metrics capture the human criteria. However, we face failures of the automatic metrics when applied to some domains and systems. Interestingly, we find that automatic metrics applied to the neural machine translation hypotheses provide the most reliable results. Finally, we provide some advice when dealing with these problematic domains.",
}
@inproceedings{chinea-rios-etal-2018-creating,
    title = "Creating the best development corpus for Statistical Machine Translation systems",
    author = "Chinea-Rios, Mara  and
      Sanchis-Trilles, Germ{\'a}n  and
      Casacuberta, Francisco",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.10",
    pages = "119--128",
    abstract = "We propose and study three different novel approaches for tackling the problem of development set selection in Statistical Machine Translation. We focus on a scenario where a machine translation system is leveraged for translating a specific test set, without further data from the domain at hand. Such test set stems from a real application of machine translation, where the texts of a specific e-commerce were to be translated. For developing our development-set selection techniques, we first conducted experiments in a controlled scenario, where labelled data from different domains was available, and evaluated the techniques both with classification and translation quality metrics. Then, the bestperforming techniques were evaluated on the e-commerce data at hand, yielding consistent improvements across two language directions.",
}
@inproceedings{farajian-etal-2018-evaluation,
    title = "Evaluation of Terminology Translation in Instance-Based Neural {MT} Adaptation",
    author = "Farajian, M. Amin  and
      Bertoldi, Nicola  and
      Negri, Matteo  and
      Turchi, Marco  and
      Federico, Marcello",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.15",
    pages = "169--178",
    abstract = "We address the issues arising when a neural machine translation engine trained on generic data receives requests from a new domain that contains many specific technical terms. Given training data of the new domain, we consider two alternative methods to adapt the generic system: corpus-based and instance-based adaptation. While the first approach is computationally more intensive in generating a domain-customized network, the latter operates more efficiently at translation time and can handle on-the-fly adaptation to multiple domains. Besides evaluating the generic and the adapted networks with conventional translation quality metrics, in this paper we focus on their ability to properly handle domain-specific terms. We show that instance-based adaptation, by fine-tuning the model on-the-fly, is capable to significantly boost the accuracy of translated terms, producing translations of quality comparable to the expensive corpusbased method.",
}
@inproceedings{jhaveri-etal-2018-translation,
    title = "Translation Quality Estimation for {I}ndian Languages",
    author = "Jhaveri, Nisarg  and
      Gupta, Manish  and
      Varma, Vasudeva",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.16",
    pages = "179--188",
    abstract = "Translation Quality Estimation (QE) aims to estimate the quality of an automated machine translation (MT) output without any human intervention or reference translation. With the increasing use of MT systems in various cross-lingual applications, the need and applicability of QE systems is increasing. We study existing approaches and propose multiple neural network approaches for sentence-level QE, with a focus on MT outputs in Indian languages. For this, we also introduce five new datasets for four language pairs: two for English{--}Gujarati, and one each for English{--}Hindi, English{--}Telugu and English{--}Bengali, which includes one manually post-edited dataset for English{--} Gujarati. These Indian languages are spoken by around 689M speakers world-wide. We compare results obtained using our proposed models with multiple state-of-the-art systems including the winning system in the WMT17 shared task on QE and show that our proposed neural model which combines the discriminative power of carefully chosen features with Siamese Convolutional Neural Networks (CNNs) works best for all Indian language datasets.",
}
@inproceedings{libovicky-etal-2018-machine,
    title = "Machine Translation Evaluation beyond the Sentence Level",
    author = "Libovick{\'y}, Jind{\v{r}}ich  and
      Brovelli, Thomas  and
      Cartoni, Bruno",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.18",
    pages = "199--208",
    abstract = "Automatic machine translation evaluation was crucial for the rapid development of machine translation systems over the last two decades. So far, most attention has been paid to the evaluation metrics that work with text on the sentence level and so did the translation systems. Across-sentence translation quality depends on discourse phenomena that may not manifest at all when staying within sentence boundaries (e.g. coreference, discourse connectives, verb tense sequence etc.). To tackle this, we propose several document-level MT evaluation metrics: generalizations of sentence-level metrics, language-(pair)-independent versions of lexical cohesion scores and coreference and morphology preservation in the target texts. We measure their agreement with human judgment on a newly created dataset of pairwise paragraph comparisons for four language pairs.",
}
@inproceedings{parcheta-etal-2018-data,
    title = "Data selection for {NMT} using Infrequent n-gram Recovery",
    author = "Parcheta, Zuzanna  and
      Sanchis-Trilles, Germ{\'a}n  and
      Casacuberta, Francisco",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.22",
    pages = "239--248",
    abstract = "Neural Machine Translation (NMT) has achieved promising results comparable with Phrase-Based Statistical Machine Translation (PBSMT). However, to train a neural translation engine, much more powerful machines are required than those required to develop translation engines based on PBSMT. One solution to reduce the training cost of NMT systems is the reduction of the training corpus through data selection (DS) techniques. There are many DS techniques applied in PBSMT which bring good results. In this work, we show that the data selection technique based on infrequent n-gram occurrence described in (Gasco ́ et al., 2012) commonly used for PBSMT systems also works well for NMT systems. We focus our work on selecting data according to specific corpora using the previously mentioned technique. The specific-domain corpora used for our experiments are IT domain and medical domain. The DS technique significantly reduces the execution time required to train the model between 87{\%} and 93{\%}. Also, it improves translation quality by up to 2.8 BLEU points. The improvements are obtained with just a small fraction of the data that accounts for between 6{\%} and 20{\%} of the total data.",
}
@inproceedings{tars-fishel-2018-multi,
    title = "Multi-Domain Neural Machine Translation",
    author = "Tars, Sander  and
      Fishel, Mark",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.26",
    pages = "279--288",
    abstract = "We present an approach to neural machine translation (NMT) that supports multiple domains in a single model and allows switching between the domains when translating. The core idea is to treat text domainsasdistinctlanguagesandusemultilingual NMT methods to create multi-domain translation systems; we show that this approach results in significant translation quality gains over fine-tuning. We also explore whether the knowledge of pre-specified text domains is necessary; turns out that it is after all, but also that when it is not known quite high translation quality can be reached, and even higher than with known domains in some cases.",
}
@inproceedings{dandapat-federmann-2018-iterative,
    title = "Iterative Data Augmentation for Neural Machine Translation: a Low Resource Case Study for {E}nglish-{T}elugu",
    author = "Dandapat, Sandipan  and
      Federmann, Christian",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.29",
    pages = "307--312",
    abstract = "Telugu is the fifteenth most commonly spoken language in the world with an estimated reach of 75 million people in the Indian subcontinent. At the same time, it is a severely low resourced language. In this paper, we present work on English{--}Telugu general domain machine translation (MT) systems using small amounts of parallel data. The baseline statistical (SMT) and neural MT (NMT) systems do not yield acceptable translation quality, mostly due to limited resources. However, the use of synthetic parallel data (generated using back translation, based on an NMT engine) significantly improves translation quality and allows NMT to outperform SMT. We extend back translation and propose a new, iterative data augmentation (IDA) method. Filtering of synthetic data and IDA both further boost translation quality of our final NMT systems, as measured by BLEU scores on all test sets and based on state-of-the-art human evaluation.",
}
@inproceedings{parcheta-etal-2018-implementing,
    title = "Implementing a neural machine translation engine for mobile devices: the Lingvanex use case",
    author = "Parcheta, Zuzanna  and
      Sanchis-Trilles, Germ{\'a}n  and
      Rudak, Aliaksei  and
      Bratchenia, Siarhei",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Espl{\`a}-Gomis, Miquel  and
      Popovi{\'c}, Maja  and
      Rico, Celia  and
      Martins, Andr{\'e}  and
      Van den Bogaert, Joachim  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the 21st Annual Conference of the European Association for Machine Translation",
    month = may,
    year = "2018",
    address = "Alicante, Spain",
    url = "https://aclanthology.org/2018.eamt-main.31",
    pages = "317--322",
    abstract = "In this paper, we present the challenge entailed by implementing a mobile version of a neural machine translation system, where the goal is to maximise translation quality while minimising model size. We explain the whole process of implementing the translation engine on an English{--}Spanish example and we describe all the difficulties found and the solutions implemented. The main techniques used in this work are data selection by means of Infrequent n-gram Recovery, appending a special word at the end of each sentence, and generating additional samples without the final punctuation marks. The last two techniques were devised with the purpose of achieving a translation model that generates sentences without the final full stop, or other punctuation marks. Also, in this work, the Infrequent n-gram Recovery was used for the first time to create a new corpus, and not enlarge the in-domain dataset. Finally, we get a small size model with quality good enough to serve for daily use.",
}
@inproceedings{huang-etal-2017-learning-parenthetical,
    title = "Learning from Parenthetical Sentences for Term Translation in Machine Translation",
    author = "Huang, Guoping  and
      Zhang, Jiajun  and
      Zhou, Yu  and
      Zong, Chengqing",
    editor = "Zhang, Yue  and
      Sui, Zhifang",
    booktitle = "Proceedings of the 9th {SIGHAN} Workshop on {C}hinese Language Processing",
    month = dec,
    year = "2017",
    address = "Taiwan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-6005",
    pages = "37--45",
    abstract = "Terms extensively exist in specific domains, and term translation plays a critical role in domain-specific machine translation (MT) tasks. However, it{'}s a challenging task to translate them correctly for the huge number of pre-existing terms and the endless new terms. To achieve better term translation quality, it is necessary to inject external term knowledge into the underlying MT system. Fortunately, there are plenty of term translation knowledge in parenthetical sentences on the Internet. In this paper, we propose a simple, straightforward and effective framework to improve term translation by learning from parenthetical sentences. This framework includes: (1) a focused web crawler; (2) a parenthetical sentence filter, acquiring parenthetical sentences including bilingual term pairs; (3) a term translation knowledge extractor, extracting bilingual term translation candidates; (4) a probability learner, generating the term translation table for MT decoders. The extensive experiments demonstrate that our proposed framework significantly improves the translation quality of terms and sentences.",
}
@inproceedings{imankulova-etal-2017-improving,
    title = "Improving Low-Resource Neural Machine Translation with Filtered Pseudo-Parallel Corpus",
    author = "Imankulova, Aizhan  and
      Sato, Takayuki  and
      Komachi, Mamoru",
    editor = "Nakazawa, Toshiaki  and
      Goto, Isao",
    booktitle = "Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/W17-5704",
    pages = "70--78",
    abstract = "Large-scale parallel corpora are indispensable to train highly accurate machine translators. However, manually constructed large-scale parallel corpora are not freely available in many language pairs. In previous studies, training data have been expanded using a pseudo-parallel corpus obtained using machine translation of the monolingual corpus in the target language. However, in low-resource language pairs in which only low-accuracy machine translation systems can be used, translation quality is reduces when a pseudo-parallel corpus is used naively. To improve machine translation performance with low-resource language pairs, we propose a method to expand the training data effectively via filtering the pseudo-parallel corpus using a quality estimation based on back-translation. As a result of experiments with three language pairs using small, medium, and large parallel corpora, language pairs with fewer training data filtered out more sentence pairs and improved BLEU scores more significantly.",
}
@inproceedings{fujita-sumita-2017-japanese,
    title = "{J}apanese to {E}nglish/{C}hinese/{K}orean Datasets for Translation Quality Estimation and Automatic Post-Editing",
    author = "Fujita, Atsushi  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Goto, Isao",
    booktitle = "Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/W17-5705",
    pages = "79--88",
    abstract = "Aiming at facilitating the research on quality estimation (QE) and automatic post-editing (APE) of machine translation (MT) outputs, especially for those among Asian languages, we have created new datasets for Japanese to English, Chinese, and Korean translations. As the source text, actual utterances in Japanese were extracted from the log data of our speech translation service. MT outputs were then given by phrase-based statistical MT systems. Finally, human evaluators were employed to grade the quality of MT outputs and to post-edit them. This paper describes the characteristics of the created datasets and reports on our benchmarking experiments on word-level QE, sentence-level QE, and APE conducted using the created datasets.",
}
@inproceedings{imamura-sumita-2017-ensemble,
    title = "Ensemble and Reranking: Using Multiple Models in the {NICT}-2 Neural Machine Translation System at {WAT}2017",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Goto, Isao",
    booktitle = "Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/W17-5711",
    pages = "127--134",
    abstract = "In this paper, we describe the NICT-2 neural machine translation system evaluated at WAT2017. This system uses multiple models as an ensemble and combines models with opposite decoding directions by reranking (called bi-directional reranking). In our experimental results on small data sets, the translation quality improved when the number of models was increased to 32 in total and did not saturate. In the experiments on large data sets, improvements of 1.59-3.32 BLEU points were achieved when six-model ensembles were combined by the bi-directional reranking.",
}
@inproceedings{matsumura-komachi-2017-tokyo,
    title = "{T}okyo Metropolitan University Neural Machine Translation System for {WAT} 2017",
    author = "Matsumura, Yukio  and
      Komachi, Mamoru",
    editor = "Nakazawa, Toshiaki  and
      Goto, Isao",
    booktitle = "Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/W17-5716",
    pages = "160--166",
    abstract = "In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT and uses long short-term memories (LSTM) as RNN. We implemented beam search and ensemble decoding in the NMT system. The system was tested on the 4th Workshop on Asian Translation (WAT 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of beam search and ensemble decoding can effectively improve the translation quality.",
}
@inproceedings{tiedemann-scherrer-2017-neural,
    title = "Neural Machine Translation with Extended Context",
    author = {Tiedemann, J{\"o}rg  and
      Scherrer, Yves},
    editor = {Webber, Bonnie  and
      Popescu-Belis, Andrei  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Third Workshop on Discourse in Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4811",
    doi = "10.18653/v1/W17-4811",
    pages = "82--92",
    abstract = "We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.",
}
@inproceedings{chen-etal-2017-improving-machine,
    title = "Improving Machine Translation Quality Estimation with Neural Network Features",
    author = "Chen, Zhiming  and
      Tan, Yiming  and
      Zhang, Chenlin  and
      Xiang, Qingyu  and
      Zhang, Lilin  and
      Li, Maoxi  and
      Wang, Mingwen",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kreutzer, Julia",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4761",
    doi = "10.18653/v1/W17-4761",
    pages = "551--555",
}
@inproceedings{martins-etal-2017-unbabels,
    title = "Unbabel{'}s Participation in the {WMT}17 Translation Quality Estimation Shared Task",
    author = "Martins, Andr{\'e} F. T.  and
      Kepler, Fabio  and
      Monteiro, Jos{\'e}",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Kreutzer, Julia",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4764",
    doi = "10.18653/v1/W17-4764",
    pages = "569--574",
}
@inproceedings{freitag-al-onaizan-2017-beam,
    title = "Beam Search Strategies for Neural Machine Translation",
    author = "Freitag, Markus  and
      Al-Onaizan, Yaser",
    editor = "Luong, Thang  and
      Birch, Alexandra  and
      Neubig, Graham  and
      Finch, Andrew",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3207",
    doi = "10.18653/v1/W17-3207",
    pages = "56--60",
    abstract = "The basic concept in Neural Machine Translation (NMT) is to train a large Neural Network that maximizes the translation performance on a given parallel corpus. NMT is then using a simple left-to-right beam-search decoder to generate new translations that approximately maximize the trained conditional probability. The current beam search strategy generates the target sentence word by word from left-to-right while keeping a fixed amount of active candidates at each time step. First, this simple search is less adaptive as it also expands candidates whose scores are much worse than the current best. Secondly, it does not expand hypotheses if they are not within the best scoring candidates, even if their scores are close to the best one. The latter one can be avoided by increasing the beam size until no performance improvement can be observed. While you can reach better performance, this has the drawback of a slower decoding speed. In this paper, we concentrate on speeding up the decoder by applying a more flexible beam search strategy whose candidate size may vary at each time step depending on the candidate scores. We speed up the original decoder by up to 43{\%} for the two language pairs German to English and Chinese to English without losing any translation quality.",
}
@inproceedings{carpuat-etal-2017-detecting,
    title = "Detecting Cross-Lingual Semantic Divergence for Neural Machine Translation",
    author = "Carpuat, Marine  and
      Vyas, Yogarshi  and
      Niu, Xing",
    editor = "Luong, Thang  and
      Birch, Alexandra  and
      Neubig, Graham  and
      Finch, Andrew",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3209",
    doi = "10.18653/v1/W17-3209",
    pages = "69--79",
    abstract = "Parallel corpora are often not as parallel as one might assume: non-literal translations and noisy translations abound, even in curated corpora routinely used for training and evaluation. We use a cross-lingual textual entailment system to distinguish sentence pairs that are parallel in meaning from those that are not, and show that filtering out divergent examples from training improves translation quality.",
}
@inproceedings{madhyastha-espana-bonet-2017-learning,
    title = "Learning Bilingual Projections of Embeddings for Vocabulary Expansion in Machine Translation",
    author = "Madhyastha, Pranava Swaroop  and
      Espa{\~n}a-Bonet, Cristina",
    editor = "Blunsom, Phil  and
      Bordes, Antoine  and
      Cho, Kyunghyun  and
      Cohen, Shay  and
      Dyer, Chris  and
      Grefenstette, Edward  and
      Hermann, Karl Moritz  and
      Rimell, Laura  and
      Weston, Jason  and
      Yih, Scott",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2617",
    doi = "10.18653/v1/W17-2617",
    pages = "139--145",
    abstract = "We propose a simple log-bilinear softmax-based model to deal with vocabulary expansion in machine translation. Our model uses word embeddings trained on significantly large unlabelled monolingual corpora and learns over a fairly small, word-to-word bilingual dictionary. Given an out-of-vocabulary source word, the model generates a probabilistic list of possible translations in the target language using the trained bilingual embeddings. We integrate these translation options into a standard phrase-based statistical machine translation system and obtain consistent improvements in translation quality on the English{--}Spanish language pair. When tested over an out-of-domain testset, we get a significant improvement of 3.9 BLEU points.",
}
@inproceedings{barancikova-kettnerova-2017-paradi,
    title = "{P}ara{D}i: Dictionary of Paraphrases of {C}zech Complex Predicates with Light Verbs",
    author = "Baran{\v{c}}{\'\i}kov{\'a}, Petra  and
      Kettnerov{\'a}, V{\'a}clava",
    editor = "Markantonatou, Stella  and
      Ramisch, Carlos  and
      Savary, Agata  and
      Vincze, Veronika",
    booktitle = "Proceedings of the 13th Workshop on Multiword Expressions ({MWE} 2017)",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1701",
    doi = "10.18653/v1/W17-1701",
    pages = "1--10",
    abstract = "We present a new freely available dictionary of paraphrases of Czech complex predicates with light verbs, ParaDi. Candidates for single predicative paraphrases of selected complex predicates have been extracted automatically from large monolingual data using word2vec. They have been manually verified and further refined. We demonstrate one of many possible applications of ParaDi in an experiment with improving machine translation quality.",
}
@inproceedings{sharoff-2017-toward,
    title = "Toward Pan-{S}lavic {NLP}: Some Experiments with Language Adaptation",
    author = "Sharoff, Serge",
    editor = "Erjavec, Toma{\v{z}}  and
      Piskorski, Jakub  and
      Pivovarova, Lidia  and
      {\v{S}}najder, Jan  and
      Steinberger, Josef  and
      Yangarber, Roman",
    booktitle = "Proceedings of the 6th Workshop on {B}alto-{S}lavic Natural Language Processing",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-1401",
    doi = "10.18653/v1/W17-1401",
    pages = "1--2",
    abstract = "There is great variation in the amount of NLP resources available for Slavonic languages. For example, the Universal Dependency treebank (Nivre et al., 2016) has about 2 MW of training resources for Czech, more than 1 MW for Russian, while only 950 words for Ukrainian and nothing for Belorussian, Bosnian or Macedonian. Similarly, the Autodesk Machine Translation dataset only covers three Slavonic languages (Czech, Polish and Russian). In this talk I will discuss a general approach, which can be called Language Adaptation, similarly to Domain Adaptation. In this approach, a model for a particular language processing task is built by lexical transfer of cognate words and by learning a new feature representation for a lesser-resourced (recipient) language starting from a better-resourced (donor) language. More specifically, I will demonstrate how language adaptation works in such training scenarios as Translation Quality Estimation, Part-of-Speech tagging and Named Entity Recognition.",
}
@inproceedings{semmar-laib-2017-building,
    title = "Building Multiword Expressions Bilingual Lexicons for Domain Adaptation of an Example-Based Machine Translation System",
    author = "Semmar, Nasredine  and
      Laib, Mariama",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_085",
    doi = "10.26615/978-954-452-049-6_085",
    pages = "661--670",
    abstract = "We describe in this paper a hybrid ap-proach to build automatically bilingual lexicons of Multiword Expressions (MWEs) from parallel corpora. We more specifically investigate the impact of using a domain-specific bilingual lexicon of MWEs on domain adaptation of an Example-Based Machine Translation (EBMT) system. We conducted experiments on the English-French language pair and two kinds of texts: in-domain texts from Europarl (European Parliament proceedings) and out-of-domain texts from Emea (European Medicines Agency documents) and Ecb (European Central Bank corpus). The obtained results indicate that integrating domain-specific bilingual lexicons of MWEs improves translation quality of the EBMT system when texts to translate are related to the specific domain and induces a relatively slight deterioration of translation quality when translating general-purpose texts.",
}
@article{martins-etal-2017-pushing,
    title = "Pushing the Limits of Translation Quality Estimation",
    author = "Martins, Andr{\'e} F. T.  and
      Junczys-Dowmunt, Marcin  and
      Kepler, Fabio N.  and
      Astudillo, Ram{\'o}n  and
      Hokamp, Chris  and
      Grundkiewicz, Roman",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q17-1015",
    doi = "10.1162/tacl_a_00056",
    pages = "205--218",
    abstract = "Translation quality estimation is a task of growing importance in NLP, due to its potential to reduce post-editing human effort in disruptive ways. However, this potential is currently limited by the relatively low accuracy of existing systems. In this paper, we achieve remarkable improvements by exploiting synergies between the related tasks of word-level quality estimation and automatic post-editing. First, we stack a new, carefully engineered, neural model into a rich feature-based word-level quality estimation system. Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16: a word-level FMULT1 score of 57.47{\%} (an absolute gain of +7.95{\%} over the current state of the art), and a Pearson correlation score of 65.56{\%} for sentence-level HTER prediction (an absolute gain of +13.36{\%}).",
}
@inproceedings{fadaee-etal-2017-data,
    title = "Data Augmentation for Low-Resource Neural Machine Translation",
    author = "Fadaee, Marzieh  and
      Bisazza, Arianna  and
      Monz, Christof",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2090",
    doi = "10.18653/v1/P17-2090",
    pages = "567--573",
    abstract = "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.",
}
@inproceedings{zhang-etal-2017-incorporating,
    title = "Incorporating Word Reordering Knowledge into Attention-based Neural Machine Translation",
    author = "Zhang, Jinchao  and
      Wang, Mingxuan  and
      Liu, Qun  and
      Zhou, Jie",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1140",
    doi = "10.18653/v1/P17-1140",
    pages = "1524--1534",
    abstract = "This paper proposes three distortion models to explicitly incorporate the word reordering knowledge into attention-based Neural Machine Translation (NMT) for further improving translation performance. Our proposed models enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve significant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality.",
}
@inproceedings{hokamp-liu-2017-lexically,
    title = "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
    author = "Hokamp, Chris  and
      Liu, Qun",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1141",
    doi = "10.18653/v1/P17-1141",
    pages = "1535--1546",
    abstract = "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model{'}s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.",
}
@inproceedings{chen-etal-2017-top,
    title = "Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation",
    author = "Chen, Huadong  and
      Huang, Shujian  and
      Chiang, David  and
      Dai, Xinyu  and
      Chen, Jiajun",
    editor = "Levy, Roger  and
      Specia, Lucia",
    booktitle = "Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K17-1011",
    doi = "10.18653/v1/K17-1011",
    pages = "90--99",
    abstract = "Pairwise ranking methods are the most widely used discriminative training approaches for structure prediction problems in natural language processing (NLP). Decomposing the problem of ranking hypotheses into pairwise comparisons enables simple and efficient solutions. However, neglecting the global ordering of the hypothesis list may hinder learning. We propose a listwise learning framework for structure prediction problems such as machine translation. Our framework directly models the entire translation list{'}s ordering to learn parameters which may better fit the given listwise samples. Furthermore, we propose top-rank enhanced loss functions, which are more sensitive to ranking errors at higher positions. Experiments on a large-scale Chinese-English translation task show that both our listwise learning framework and top-rank enhanced listwise losses lead to significant improvements in translation quality.",
}
@article{joty-etal-2017-discourse,
    title = "Discourse Structure in Machine Translation Evaluation",
    author = "Joty, Shafiq  and
      Guzm{\'a}n, Francisco  and
      M{\`a}rquez, Llu{\'\i}s  and
      Nakov, Preslav",
    journal = "Computational Linguistics",
    volume = "43",
    number = "4",
    month = dec,
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J17-4001",
    doi = "10.1162/COLI_a_00298",
    pages = "683--722",
    abstract = "In this article, we explore the potential of using sentence-level discourse structure for machine translation evaluation. We first design discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory (RST). Then, we show that a simple linear combination with these measures can help improve various existing machine translation evaluation metrics regarding correlation with human judgments both at the segment level and at the system level. This suggests that discourse information is complementary to the information used by many of the existing evaluation metrics, and thus it could be taken into account when developing richer evaluation metrics, such as the WMT-14 winning combined metric DiscoTKparty. We also provide a detailed analysis of the relevance of various discourse elements and relations from the RST parse trees for machine translation evaluation. In particular, we show that (i) all aspects of the RST tree are relevant, (ii) nuclearity is more useful than relation type, and (iii) the similarity of the translation RST tree to the reference RST tree is positively correlated with translation quality.",
}
@article{irvine-callison-burch-2017-comprehensive,
    title = "A Comprehensive Analysis of Bilingual Lexicon Induction",
    author = "Irvine, Ann  and
      Callison-Burch, Chris",
    journal = "Computational Linguistics",
    volume = "43",
    number = "2",
    month = jun,
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J17-2001",
    doi = "10.1162/COLI_a_00284",
    pages = "273--310",
    abstract = "Bilingual lexicon induction is the task of inducing word translations from monolingual corpora in two languages. In this article we present the most comprehensive analysis of bilingual lexicon induction to date. We present experiments on a wide range of languages and data sizes. We examine translation into English from 25 foreign languages: Albanian, Azeri, Bengali, Bosnian, Bulgarian, Cebuano, Gujarati, Hindi, Hungarian, Indonesian, Latvian, Nepali, Romanian, Serbian, Slovak, Somali, Spanish, Swedish, Tamil, Telugu, Turkish, Ukrainian, Uzbek, Vietnamese, and Welsh. We analyze the behavior of bilingual lexicon induction on low-frequency words, rather than testing solely on high-frequency words, as previous research has done. Low-frequency words are more relevant to statistical machine translation, where systems typically lack translations of rare words that fall outside of their training data. We systematically explore a wide range of features and phenomena that affect the quality of the translations discovered by bilingual lexicon induction. We provide illustrative examples of the highest ranking translations for orthogonal signals of translation equivalence like contextual similarity and temporal similarity. We analyze the effects of frequency and burstiness, and the sizes of the seed bilingual dictionaries and the monolingual training corpora. Additionally, we introduce a novel discriminative approach to bilingual lexicon induction. Our discriminative model is capable of combining a wide variety of features that individually provide only weak indications of translation equivalence. When feature weights are discriminatively set, these signals produce dramatically higher translation quality than previous approaches that combined signals in an unsupervised fashion (e.g., using minimum reciprocal rank). We also directly compare our model{'}s performance against a sophisticated generative approach, the matching canonical correlation analysis (MCCA) algorithm used by Haghighi et al. (2008). Our algorithm achieves an accuracy of 42{\%} versus MCCA{'}s 15{\%}.",
}
@inproceedings{kim-etal-2017-concept,
    title = "Concept Equalization to Guide Correct Training of Neural Machine Translation",
    author = "Kim, Kangil  and
      Shin, Jong-Hun  and
      Na, Seung-Hoon  and
      Jung, SangKeun",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-2051",
    pages = "302--307",
    abstract = "Neural machine translation decoders are usually conditional language models to sequentially generate words for target sentences. This approach is limited to find the best word composition and requires help of explicit methods as beam search. To help learning correct compositional mechanisms in NMTs, we propose concept equalization using direct mapping distributed representations of source and target sentences. In a translation experiment from English to French, the concept equalization significantly improved translation quality by 3.00 BLEU points compared to a state-of-the-art NMT model.",
}
@inproceedings{belinkov-etal-2017-evaluating,
    title = "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    author = "Belinkov, Yonatan  and
      M{\`a}rquez, Llu{\'\i}s  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1001",
    pages = "1--10",
    abstract = "While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.",
}
@inproceedings{dalvi-etal-2017-understanding,
    title = "Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder",
    author = "Dalvi, Fahim  and
      Durrani, Nadir  and
      Sajjad, Hassan  and
      Belinkov, Yonatan  and
      Vogel, Stephan",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1015",
    pages = "142--151",
    abstract = "End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomenon. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology in the decoder helps it to produce better translations. To this end we present three methods: i) simultaneous translation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2{--}0.6 BLEU points.",
}
@inproceedings{zhang-etal-2017-improving,
    title = "Improving Neural Machine Translation through Phrase-based Forced Decoding",
    author = "Zhang, Jingyi  and
      Utiyama, Masao  and
      Sumita, Eiichro  and
      Neubig, Graham  and
      Nakamura, Satoshi",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1016",
    pages = "152--162",
    abstract = "Compared to traditional statistical machine translation (SMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.",
}
@inproceedings{wang-etal-2017-towards,
    title = "Towards Neural Machine Translation with Partially Aligned Corpora",
    author = "Wang, Yining  and
      Zhao, Yang  and
      Zhang, Jiajun  and
      Zong, Chengqing  and
      Xue, Zhengshan",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1039",
    pages = "384--393",
    abstract = "While neural machine translation (NMT) has become the new paradigm, the parameter optimization requires large-scale parallel data which is scarce in many domains and language pairs. In this paper, we address a new translation scenario in which there only exists monolingual corpora and phrase pairs. We propose a new method towards translation with partially aligned sentence pairs which are derived from the phrase pairs and monolingual corpora. To make full use of the partially aligned corpora, we adapt the conventional NMT training method in two aspects. On one hand, different generation strategies are designed for aligned and unaligned target words. On the other hand, a different objective function is designed to model the partially aligned parts. The experiments demonstrate that our method can achieve a relatively good result in such a translation scenario, and tiny bitexts can boost translation quality to a large extent.",
}
@inproceedings{pal-etal-2017-neural,
    title = "Neural Automatic Post-Editing Using Prior Alignment and Reranking",
    author = "Pal, Santanu  and
      Naskar, Sudip Kumar  and
      Vela, Mihaela  and
      Liu, Qun  and
      van Genabith, Josef",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2056",
    pages = "349--355",
    abstract = "We present a second-stage machine translation (MT) system based on a neural machine translation (NMT) approach to automatic post-editing (APE) that improves the translation quality provided by a first-stage MT system. Our APE system (APE{\_}Sym) is an extended version of an attention based NMT model with bilingual symmetry employing bidirectional models, mt{--}pe and pe{--}mt. APE translations produced by our system show statistically significant improvements over the first-stage MT, phrase-based APE and the best reported score on the WMT 2016 APE dataset by a previous neural APE system. Re-ranking (APE{\_}Rerank) of the n-best translations from the phrase-based APE and APE{\_}Sym systems provides further substantial improvements over the symmetric neural APE model. Human evaluation confirms that the APE{\_}Rerank generated PE translations improve on the previous best neural APE system at WMT 2016.",
}
@inproceedings{graham-etal-2017-improving,
    title = "Improving Evaluation of Document-level Machine Translation Quality Estimation",
    author = "Graham, Yvette  and
      Ma, Qingsong  and
      Baldwin, Timothy  and
      Liu, Qun  and
      Parra, Carla  and
      Scarton, Carolina",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2057",
    pages = "356--361",
    abstract = "Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard, before proposing direct human assessment as a valid alternative. Experiments show direct assessment (DA) scores for documents to be highly reliable, achieving a correlation of above 0.9 in a self-replication experiment, in addition to a substantial estimated cost reduction through quality controlled crowd-sourcing. The original gold standard based on post-edits incurs a 10{--}20 times greater cost than DA.",
}
@inproceedings{sennrich-2017-grammatical,
    title = "How Grammatical is Character-level Neural Machine Translation? Assessing {MT} Quality with Contrastive Translation Pairs",
    author = "Sennrich, Rico",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2060",
    pages = "376--382",
    abstract = "Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems model specific linguistic phenomena such as agreement over long distances, the production of novel words, and the faithful translation of polarity. The core idea is that we measure whether a reference translation is more probable under a NMT model than a contrastive translation which introduces a specific type of error. We present LingEval97, a large-scale data set of 97000 contrastive translation pairs based on the WMT English-{\textgreater}German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning.",
}
@inproceedings{yang-etal-2017-neural,
    title = "Neural Machine Translation with Recurrent Attention Modeling",
    author = "Yang, Zichao  and
      Hu, Zhiting  and
      Deng, Yuntian  and
      Dyer, Chris  and
      Smola, Alex",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2061",
    pages = "383--387",
    abstract = "Knowing which words have been attended to in previous time steps while generating a translation is a rich source of information for predicting what words will be attended to in the future. We improve upon the attention model of Bahdanau et al. (2014) by explicitly modeling the relationship between previous and subsequent attention levels for each word using one recurrent network per input word. This architecture easily captures informative features, such as fertility and regularities in relative distortion. In experiments, we show our parameterization of attention improves translation quality.",
}
@inproceedings{siahbani-sarkar-2017-lexicalized,
    title = "Lexicalized Reordering for Left-to-Right Hierarchical Phrase-based Translation",
    author = "Siahbani, Maryam  and
      Sarkar, Anoop",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2097",
    pages = "612--618",
    abstract = "Phrase-based and hierarchical phrase-based (Hiero) translation models differ radically in the way reordering is modeled. Lexicalized reordering models play an important role in phrase-based MT and such models have been added to CKY-based decoders for Hiero. Watanabe et al. (2006) proposed a promising decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English.",
}
@inproceedings{rios-gonzales-tuggener-2017-co,
    title = "Co-reference Resolution of Elided Subjects and Possessive Pronouns in {S}panish-{E}nglish Statistical Machine Translation",
    author = "Rios Gonzales, Annette  and
      Tuggener, Don",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2104",
    pages = "657--662",
    abstract = "This paper presents a straightforward method to integrate co-reference information into phrase-based machine translation to address the problems of i) elided subjects and ii) morphological underspecification of pronouns when translating from pro-drop languages. We evaluate the method for the language pair Spanish-English and find that translation quality improves with the addition of co-reference information.",
}
@inproceedings{baskaya-etal-2017-integrating,
    title = "Integrating Meaning into Quality Evaluation of Machine Translation",
    author = {Ba{\c{s}}kaya, Osman  and
      Yildiz, Eray  and
      Tunao{\u{g}}lu, Doruk  and
      Eren, Mustafa Tolga  and
      Do{\u{g}}ru{\"o}z, A. Seza},
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1020",
    pages = "210--219",
    abstract = "Machine translation (MT) quality is evaluated through comparisons between MT outputs and the human translations (HT). Traditionally, this evaluation relies on form related features (e.g. lexicon and syntax) and ignores the transfer of meaning reflected in HT outputs. Instead, we evaluate the quality of MT outputs through meaning related features (e.g. polarity, subjectivity) with two experiments. In the first experiment, the meaning related features are compared to human rankings individually. In the second experiment, combinations of meaning related features and other quality metrics are utilized to predict the same human rankings. The results of our experiments confirm the benefit of these features in predicting human evaluation of translation quality in addition to traditional metrics which focus mainly on form.",
}
@inproceedings{britz-etal-2017-massive,
    title = "Massive Exploration of Neural Machine Translation Architectures",
    author = "Britz, Denny  and
      Goldie, Anna  and
      Luong, Minh-Thang  and
      Le, Quoc",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1151",
    doi = "10.18653/v1/D17-1151",
    pages = "1442--1451",
    abstract = "Neural Machine Translation (NMT) has shown remarkable progress over the past few years, with production systems now being deployed to end-users. As the field is moving rapidly, it has become unclear which elements of NMT architectures have a significant impact on translation quality. In this work, we present a large-scale analysis of the sensitivity of NMT architectures to common hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on a WMT English to German translation task. Our experiments provide practical insights into the relative importance of factors such as embedding size, network depth, RNN cell type, residual connections, attention mechanism, and decoding heuristics. As part of this contribution, we also release an open-source NMT framework in TensorFlow to make it easy for others to reproduce our results and perform their own experiments.",
}
@inproceedings{stahlberg-byrne-2017-unfolding,
    title = "Unfolding and Shrinking Neural Machine Translation Ensembles",
    author = "Stahlberg, Felix  and
      Byrne, Bill",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1208",
    doi = "10.18653/v1/D17-1208",
    pages = "1946--1956",
    abstract = "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On Japanese-English we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.",
}
@inproceedings{isabelle-etal-2017-challenge,
    title = "A Challenge Set Approach to Evaluating Machine Translation",
    author = "Isabelle, Pierre  and
      Cherry, Colin  and
      Foster, George",
    editor = "Palmer, Martha  and
      Hwa, Rebecca  and
      Riedel, Sebastian",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1263",
    doi = "10.18653/v1/D17-1263",
    pages = "2486--2496",
    abstract = "Neural machine translation represents an exciting leap forward in translation quality. But what longstanding weaknesses does it resolve, and which remain? We address these questions with a challenge set approach to translation evaluation and error analysis. A challenge set consists of a small set of sentences, each hand-designed to probe a system{'}s capacity to bridge a particular structural divergence between languages. To exemplify this approach, we present an English-French challenge set, and use it to analyze phrase-based and neural systems. The resulting analysis provides not only a more fine-grained picture of the strengths of neural systems, but also insight into which linguistic phenomena remain out of reach.",
}
@inproceedings{specia-etal-2017-translation,
    title = "Translation Quality and Productivity: A Study on Rich Morphology Languages",
    author = "Specia, Lucia  and
      Harris, Kim  and
      Blain, Fr{\'e}d{\'e}ric  and
      Burchardt, Aljoscha  and
      Macketanz, Viviven  and
      Skadin, Inguna  and
      Negri, Matteo  and
      Turchi, Marco",
    editor = "Kurohashi, Sadao  and
      Fung, Pascale",
    booktitle = "Proceedings of Machine Translation Summit XVI: Research Track",
    month = sep # " 18 {--} " # sep # " 22",
    year = "2017",
    address = "Nagoya Japan",
    url = "https://aclanthology.org/2017.mtsummit-papers.5",
    pages = "55--71",
}
@inproceedings{ganci-2017-journey,
    title = "Journey around Neural Machine Translation quality",
    author = "Ganci, Marco",
    editor = "Yamada, Masaru  and
      Seligman, Mark",
    booktitle = "Proceedings of Machine Translation Summit XVI: Commercial MT Users and Translators Track",
    month = sep # " 18 {--} " # sep # " 22",
    year = "2017",
    address = "Nagoya Japan",
    url = "https://aclanthology.org/2017.mtsummit-commercial.18",
    pages = "179--205",
}
@inproceedings{sajjad-etal-2017-neural,
    title = "Neural Machine Translation Training in a Multi-Domain Scenario",
    author = "Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Belinkov, Yonatan  and
      Vogel, Stephan",
    editor = "Sakti, Sakriani  and
      Utiyama, Masao",
    booktitle = "Proceedings of the 14th International Conference on Spoken Language Translation",
    month = dec # " 14-15",
    year = "2017",
    address = "Tokyo, Japan",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2017.iwslt-1.10",
    pages = "66--73",
    abstract = "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and multi-model ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.",
}
@inproceedings{imamura-sumita-2016-nict,
    title = "{NICT}-2 Translation System for {WAT}2016: Applying Domain Adaptation to Phrase-based Statistical Machine Translation",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Goto, Isao  and
      Neubig, Graham  and
      Kurohashi, Sadao  and
      Riza, Ir. Hammam  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4611",
    pages = "126--132",
    abstract = "This paper describes the NICT-2 translation system for the 3rd Workshop on Asian Translation. The proposed system employs a domain adaptation method based on feature augmentation. We regarded the Japan Patent Office Corpus as a mixture of four domain corpora and improved the translation quality of each domain. In addition, we incorporated language models constructed from Google n-grams as external knowledge. Our domain adaptation method can naturally incorporate such external knowledge that contributes to translation quality.",
}
@inproceedings{wang-etal-2016-efficient,
    title = "An Efficient and Effective Online Sentence Segmenter for Simultaneous Interpretation",
    author = "Wang, Xiaolin  and
      Finch, Andrew  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    editor = "Nakazawa, Toshiaki  and
      Mino, Hideya  and
      Ding, Chenchen  and
      Goto, Isao  and
      Neubig, Graham  and
      Kurohashi, Sadao  and
      Riza, Ir. Hammam  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4613",
    pages = "139--148",
    abstract = "Simultaneous interpretation is a very challenging application of machine translation in which the input is a stream of words from a speech recognition engine. The key problem is how to segment the stream in an online manner into units suitable for translation. The segmentation process proceeds by calculating a confidence score for each word that indicates the soundness of placing a sentence boundary after it, and then heuristics are employed to determine the position of the boundaries. Multiple variants of the confidence scoring method and segmentation heuristics were studied. Experimental results show that the best performing strategy is not only efficient in terms of average latency per word, but also achieved end-to-end translation quality close to an offline baseline, and close to oracle segmentation.",
}
@inproceedings{rikters-2016-neural,
    title = "Neural Network Language Models for Candidate Scoring in Hybrid Multi-System Machine Translation",
    author = "Rikters, Mat{\=\i}ss",
    editor = "Lambert, Patrik  and
      Babych, Bogdan  and
      Eberle, Kurt  and
      Banchs, Rafael E.  and
      Rapp, Reinhard  and
      Costa-juss{\`a}, Marta R.",
    booktitle = "Proceedings of the Sixth Workshop on Hybrid Approaches to Translation ({H}y{T}ra6)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4502",
    pages = "8--15",
    abstract = "This paper presents the comparison of how using different neural network based language modeling tools for selecting the best candidate fragments affects the final output translation quality in a hybrid multi-system machine translation setup. Experiments were conducted by comparing perplexity and BLEU scores on common test cases using the same training data set. A 12-gram statistical language model was selected as a baseline to oppose three neural network based models of different characteristics. The models were integrated in a hybrid system that depends on the perplexity score of a sentence fragment to produce the best fitting translations. The results show a correlation between language model perplexity and BLEU scores as well as overall improvements in BLEU.",
}
@inproceedings{sudarikov-etal-2016-verb,
    title = "Verb sense disambiguation in Machine Translation",
    author = "Sudarikov, Roman  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Holub, Martin  and
      Bojar, Ond{\v{r}}ej  and
      Kr{\'\i}{\v{z}}, Vincent",
    editor = "Lambert, Patrik  and
      Babych, Bogdan  and
      Eberle, Kurt  and
      Banchs, Rafael E.  and
      Rapp, Reinhard  and
      Costa-juss{\`a}, Marta R.",
    booktitle = "Proceedings of the Sixth Workshop on Hybrid Approaches to Translation ({H}y{T}ra6)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4506",
    pages = "42--50",
    abstract = "We describe experiments in Machine Translation using word sense disambiguation (WSD) information. This work focuses on WSD in verbs, based on two different approaches {--} verbal patterns based on corpus pattern analysis and verbal word senses from valency frames. We evaluate several options of using verb senses in the source-language sentences as an additional factor for the Moses statistical machine translation system. Our results show a statistically significant translation quality improvement in terms of the BLEU metric for the valency frames approach, but in manual evaluation, both WSD methods bring improvements.",
}
@inproceedings{beloucif-etal-2016-improving,
    title = "Improving word alignment for low resource languages using {E}nglish monolingual {SRL}",
    author = "Beloucif, Meriem  and
      Saers, Markus  and
      Wu, Dekai",
    editor = "Lambert, Patrik  and
      Babych, Bogdan  and
      Eberle, Kurt  and
      Banchs, Rafael E.  and
      Rapp, Reinhard  and
      Costa-juss{\`a}, Marta R.",
    booktitle = "Proceedings of the Sixth Workshop on Hybrid Approaches to Translation ({H}y{T}ra6)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4507",
    pages = "51--60",
    abstract = "We introduce a new statistical machine translation approach specifically geared to learning translation from low resource languages, that exploits monolingual English semantic parsing to bias inversion transduction grammar (ITG) induction. We show that in contrast to conventional statistical machine translation (SMT) training methods, which rely heavily on phrase memorization, our approach focuses on learning bilingual correlations that help translating low resource languages, by using the output language semantic structure to further narrow down ITG constraints. This approach is motivated by previous research which has shown that injecting a semantic frame based objective function while training SMT models improves the translation quality. We show that including a monolingual semantic objective function during the learning of the translation model leads towards a semantically driven alignment which is more efficient than simply tuning loglinear mixture weights against a semantic frame based evaluation metric in the final stage of statistical machine translation training. We test our approach with three different language pairs and demonstrate that our model biases the learning towards more semantically correct alignments. Both GIZA++ and ITG based techniques fail to capture meaningful bilingual constituents, which is required when trying to learn translation models for low resource languages. In contrast, our proposed model not only improve translation by injecting a monolingual objective function to learn bilingual correlations during early training of the translation model, but also helps to learn more meaningful correlations with a relatively small data set, leading to a better alignment compared to either conventional ITG or traditional GIZA++ based approaches.",
}
@inproceedings{van-der-wees-etal-2016-simple,
    title = "A Simple but Effective Approach to Improve {A}rabizi-to-{E}nglish Statistical Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Monz, Christof",
    editor = "Han, Bo  and
      Ritter, Alan  and
      Derczynski, Leon  and
      Xu, Wei  and
      Baldwin, Tim",
    booktitle = "Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT})",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-3908",
    pages = "43--50",
    abstract = "A major challenge for statistical machine translation (SMT) of Arabic-to-English user-generated text is the prevalence of text written in Arabizi, or Romanized Arabic. When facing such texts, a translation system trained on conventional Arabic-English data will suffer from extremely low model coverage. In addition, Arabizi is not regulated by any official standardization and therefore highly ambiguous, which prevents rule-based approaches from achieving good translation results. In this paper, we improve Arabizi-to-English machine translation by presenting a simple but effective Arabizi-to-Arabic transliteration pipeline that does not require knowledge by experts or native Arabic speakers. We incorporate this pipeline into a phrase-based SMT system, and show that translation quality after automatically transliterating Arabizi to Arabic yields results that are comparable to those achieved after human transliteration.",
}
@inproceedings{abdelsalam-etal-2016-bilingual,
    title = "Bilingual Embeddings and Word Alignments for Translation Quality Estimation",
    author = "Abdelsalam, Amal  and
      Bojar, Ond{\v{r}}ej  and
      El-Beltagy, Samhaa",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2380",
    doi = "10.18653/v1/W16-2380",
    pages = "764--771",
}
@inproceedings{espla-gomis-etal-2016-ualacant,
    title = "{UA}lacant word-level and phrase-level machine translation quality estimation systems at {WMT} 2016",
    author = "Espl{\`a}-Gomis, Miquel  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Forcada, Mikel",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2383",
    doi = "10.18653/v1/W16-2383",
    pages = "782--786",
}
@inproceedings{kim-lee-2016-recurrent-neural,
    title = "Recurrent Neural Network based Translation Quality Estimation",
    author = "Kim, Hyun  and
      Lee, Jong-Hyeok",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2384",
    doi = "10.18653/v1/W16-2384",
    pages = "787--792",
}
@inproceedings{martins-etal-2016-unbabels,
    title = "Unbabel{'}s Participation in the {WMT}16 Word-Level Translation Quality Estimation Shared Task",
    author = "Martins, Andr{\'e} F. T.  and
      Astudillo, Ram{\'o}n  and
      Hokamp, Chris  and
      Kepler, Fabio",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2387",
    doi = "10.18653/v1/W16-2387",
    pages = "806--811",
}
@inproceedings{patel-m-2016-translation,
    title = "Translation Quality Estimation using Recurrent Neural Network",
    author = "Patel, Raj Nath  and
      M, Sasikumar",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2389",
    doi = "10.18653/v1/W16-2389",
    pages = "819--824",
}
@inproceedings{sagemo-stymne-2016-uu,
    title = "The {UU} Submission to the Machine Translation Quality Estimation Task",
    author = "Sagemo, Oscar  and
      Stymne, Sara",
    editor = {Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Guillou, Liane  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Pecina, Pavel  and
      Popel, Martin  and
      Koehn, Philipp  and
      Monz, Christof  and
      Negri, Matteo  and
      Post, Matt  and
      Specia, Lucia  and
      Verspoor, Karin  and
      Tiedemann, J{\"o}rg  and
      Turchi, Marco},
    booktitle = "Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W16-2390",
    doi = "10.18653/v1/W16-2390",
    pages = "825--830",
}
@inproceedings{logacheva-etal-2016-metrics,
    title = "Metrics for Evaluation of Word-level Machine Translation Quality Estimation",
    author = "Logacheva, Varvara  and
      Lukasik, Michal  and
      Specia, Lucia",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-2095",
    doi = "10.18653/v1/P16-2095",
    pages = "585--590",
}
@inproceedings{shah-specia-2016-large,
    title = "Large-scale Multitask Learning for Machine Translation Quality Estimation",
    author = "Shah, Kashif  and
      Specia, Lucia",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1069",
    doi = "10.18653/v1/N16-1069",
    pages = "558--567",
}
@inproceedings{sajjad-etal-2016-eyes,
    title = "Eyes Don{'}t Lie: Predicting Machine Translation Quality Using Eye Movement",
    author = "Sajjad, Hassan  and
      Guzm{\'a}n, Francisco  and
      Durrani, Nadir  and
      Abdelali, Ahmed  and
      Bouamor, Houda  and
      Temnikova, Irina  and
      Vogel, Stephan",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1125",
    doi = "10.18653/v1/N16-1125",
    pages = "1082--1088",
}
@inproceedings{costa-etal-2016-building,
    title = "Building a Corpus of Errors and Quality in Machine Translation: Experiments on Error Impact",
    author = "Costa, {\^A}ngela  and
      Correia, Rui  and
      Coheur, Lu{\'\i}sa",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1044",
    pages = "288--292",
    abstract = "In this paper we describe a corpus of automatic translations annotated with both error type and quality. The 300 sentences that we have selected were generated by Google Translate, Systran and two in-house Machine Translation systems that use Moses technology. The errors present on the translations were annotated with an error taxonomy that divides errors in five main linguistic categories (Orthography, Lexis, Grammar, Semantics and Discourse), reflecting the language level where the error is located. After the error annotation process, we accessed the translation quality of each sentence using a four point comprehension scale from 1 to 5. Both tasks of error and quality annotation were performed by two different annotators, achieving good levels of inter-annotator agreement. The creation of this corpus allowed us to use it as training data for a translation quality classifier. We concluded on error severity by observing the outputs of two machine learning classifiers: a decision tree and a regression model.",
}
@inproceedings{katris-etal-2016-using,
    title = "Using a Cross-Language Information Retrieval System based on {OHSUMED} to Evaluate the {M}oses and {K}antan{MT} Statistical Machine Translation Systems",
    author = "Katris, Nikolaos  and
      Sutcliffe, Richard  and
      Kalamboukis, Theodore",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1057",
    pages = "368--372",
    abstract = "The objective of this paper was to evaluate the performance of two statistical machine translation (SMT) systems within a cross-language information retrieval (CLIR) architecture and examine if there is a correlation between translation quality and CLIR performance. The SMT systems were KantanMT, a cloud-based machine translation (MT) platform, and Moses, an open-source MT application. First we trained both systems using the same language resources: the EMEA corpus for the translation model and language model and the QTLP corpus for tuning. Then we translated the 63 queries of the OHSUMED test collection from Greek into English using both MT systems. Next, we ran the queries on the document collection using Apache Solr to get a list of the top ten matches. The results were compared to the OHSUMED gold standard. KantanMT achieved higher average precision and F-measure than Moses, while both systems produced the same recall score. We also calculated the BLEU score for each system using the ECDC corpus. Moses achieved a higher BLEU score than KantanMT. Finally, we also tested the IR performance of the original English queries. This work overall showed that CLIR performance can be better even when BLEU score is worse.",
}
@inproceedings{saralegi-etal-2016-evaluating,
    title = "Evaluating Translation Quality and {CLIR} Performance of Query Sessions",
    author = "Saralegi, Xabier  and
      Agirre, Eneko  and
      Alegria, I{\~n}aki",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1064",
    pages = "407--411",
    abstract = "This paper presents the evaluation of the translation quality and Cross-Lingual Information Retrieval (CLIR) performance when using session information as the context of queries. The hypothesis is that previous queries provide context that helps to solve ambiguous translations in the current query. We tested several strategies on the TREC 2010 Session track dataset, which includes query reformulations grouped by generalization, specification, and drifting types. We study the Basque to English direction, evaluating both the translation quality and CLIR performance, with positive results in both cases. The results show that the quality of translation improved, reducing error rate by 12{\%} (HTER) when using session information, which improved CLIR results 5{\%} (nDCG). We also provide an analysis of the improvements across the three kinds of sessions: generalization, specification, and drifting. Translation quality improved in all three types (generalization, specification, and drifting), and CLIR improved for generalization and specification sessions, preserving the performance in drifting sessions.",
}
@inproceedings{rikters-skadina-2016-syntax,
    title = "Syntax-based Multi-system Machine Translation",
    author = "Rikters, Mat{\=\i}ss  and
      Skadi{\c{n}}a, Inguna",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1093",
    pages = "585--591",
    abstract = "This paper describes a hybrid machine translation system that explores a parser to acquire syntactic chunks of a source sentence, translates the chunks with multiple online machine translation (MT) system application program interfaces (APIs) and creates output by combining translated chunks to obtain the best possible translation. The selection of the best translation hypothesis is performed by calculating the perplexity for each translated chunk. The goal of this approach is to enhance the baseline multi-system hybrid translation (MHyT) system that uses only a language model to select best translation from translations obtained with different APIs and to improve overall English ― Latvian machine translation quality over each of the individual MT APIs. The presented syntax-based multi-system translation (SyMHyT) system demonstrates an improvement in terms of BLEU and NIST scores compared to the baseline system. Improvements reach from 1.74 up to 2.54 BLEU points.",
}
@inproceedings{s-bhattacharyya-2016-lexical,
    title = "Lexical Resources to Enrich {E}nglish {M}alayalam Machine Translation",
    author = "S, Sreelekha  and
      Bhattacharyya, Pushpak",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1098",
    pages = "620--627",
    abstract = "In this paper we present our work on the usage of lexical resources for the Machine Translation English and Malayalam. We describe a comparative performance between different Statistical Machine Translation (SMT) systems on top of phrase based SMT system as baseline. We explore different ways of utilizing lexical resources to improve the quality of English Malayalam statistical machine translation. In order to enrich the training corpus we have augmented the lexical resources in two ways (a) additional vocabulary and (b) inflected verbal forms. Lexical resources include IndoWordnet semantic relation set, lexical words and verb phrases etc. We have described case studies, evaluations and have given detailed error analysis for both Malayalam to English and English to Malayalam machine translation systems. We observed significant improvement in evaluations of translation quality. Lexical resources do help uplift performance when parallel corpora are scanty.",
}
@inproceedings{matsuzaki-etal-2016-translation,
    title = "Translation Errors and Incomprehensibility: a Case Study using Machine-Translated Second Language Proficiency Tests",
    author = "Matsuzaki, Takuya  and
      Fujita, Akira  and
      Todo, Naoya  and
      Arai, Noriko H.",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1440",
    pages = "2771--2776",
    abstract = "This paper reports on an experiment where 795 human participants answered to the questions taken from second language proficiency tests that were translated to their native language. The output of three machine translation systems and two different human translations were used as the test material. We classified the translation errors in the questions according to an error taxonomy and analyzed the participants{'} response on the basis of the type and frequency of the translation errors. Through the analysis, we identified several types of errors that deteriorated most the accuracy of the participants{'} answers, their confidence on the answers, and their overall evaluation of the translation quality.",
}
@inproceedings{li-etal-2016-uzbek,
    title = "{U}zbek-{E}nglish and {T}urkish-{E}nglish Morpheme Alignment Corpora",
    author = "Li, Xuansong  and
      Tracey, Jennifer  and
      Grimes, Stephen  and
      Strassel, Stephanie",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1467",
    pages = "2925--2930",
    abstract = "Morphologically-rich languages pose problems for machine translation (MT) systems, including word-alignment errors, data sparsity and multiple affixes. Current alignment models at word-level do not distinguish words and morphemes, thus yielding low-quality alignment and subsequently affecting end translation quality. Models using morpheme-level alignment can reduce the vocabulary size of morphologically-rich languages and overcomes data sparsity. The alignment data based on smallest units reveals subtle language features and enhances translation quality. Recent research proves such morpheme-level alignment (MA) data to be valuable linguistic resources for SMT, particularly for languages with rich morphology. In support of this research trend, the Linguistic Data Consortium (LDC) created Uzbek-English and Turkish-English alignment data which are manually aligned at the morpheme level. This paper describes the creation of MA corpora, including alignment and tagging process and approaches, highlighting annotation challenges and specific features of languages with rich morphology. The light tagging annotation on the alignment layer adds extra value to the MA data, facilitating users in flexibly tailoring the data for various MT model training.",
}
@inproceedings{yuan-etal-2016-mobil,
    title = "{M}o{B}i{L}: A Hybrid Feature Set for Automatic Human Translation Quality Assessment",
    author = "Yuan, Yu  and
      Sharoff, Serge  and
      Babych, Bogdan",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1581",
    pages = "3663--3670",
    abstract = "In this paper we introduce MoBiL, a hybrid Monolingual, Bilingual and Language modelling feature set and feature selection and evaluation framework. The set includes translation quality indicators that can be utilized to automatically predict the quality of human translations in terms of content adequacy and language fluency. We compare MoBiL with the QuEst baseline set by using them in classifiers trained with support vector machine and relevance vector machine learning algorithms on the same data set. We also report an experiment on feature selection to opt for fewer but more informative features from MoBiL. Our experiments show that classifiers trained on our feature set perform consistently better in predicting both adequacy and fluency than the classifiers trained on the baseline feature set. MoBiL also performs well when used with both support vector machine and relevance vector machine algorithms.",
}
@inproceedings{logacheva-etal-2016-marmot,
    title = "{MARMOT}: A Toolkit for Translation Quality Estimation at the Word Level",
    author = "Logacheva, Varvara  and
      Hokamp, Chris  and
      Specia, Lucia",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1582",
    pages = "3671--3674",
    abstract = "We present Marmot{\textasciitilde}― a new toolkit for quality estimation (QE) of machine translation output. Marmot contains utilities targeted at quality estimation at the word and phrase level. However, due to its flexibility and modularity, it can also be extended to work at the sentence level. In addition, it can be used as a framework for extracting features and learning models for many common natural language processing tasks. The tool has a set of state-of-the-art features for QE, and new features can easily be added. The tool is open-source and can be downloaded from \url{https://github.com/qe-team/marmot/}",
}
@inproceedings{beck-etal-2016-exploring,
    title = "Exploring Prediction Uncertainty in Machine Translation Quality Estimation",
    author = "Beck, Daniel  and
      Specia, Lucia  and
      Cohn, Trevor",
    editor = "Riezler, Stefan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1021",
    doi = "10.18653/v1/K16-1021",
    pages = "208--218",
}
@inproceedings{bentivogli-etal-2016-neural,
    title = "Neural versus Phrase-Based Machine Translation Quality: a Case Study",
    author = "Bentivogli, Luisa  and
      Bisazza, Arianna  and
      Cettolo, Mauro  and
      Federico, Marcello",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1025",
    doi = "10.18653/v1/D16-1025",
    pages = "257--267",
}
@inproceedings{russell-gillespie-2016-measuring,
    title = "Measuring the behavioral impact of machine translation quality improvements with A/{B} testing",
    author = "Russell, Ben  and
      Gillespie, Duncan",
    editor = "Su, Jian  and
      Duh, Kevin  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1251",
    doi = "10.18653/v1/D16-1251",
    pages = "2295--2299",
}
@inproceedings{zhang-etal-2016-fast,
    title = "Fast Gated Neural Domain Adaptation: Language Model as a Case Study",
    author = "Zhang, Jian  and
      Wu, Xiaofeng  and
      Way, Andy  and
      Liu, Qun",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1131",
    pages = "1386--1397",
    abstract = "Neural network training has been shown to be advantageous in many natural language processing applications, such as language modelling or machine translation. In this paper, we describe in detail a novel domain adaptation mechanism in neural network training. Instead of learning and adapting the neural network on millions of training sentences {--} which can be very time-consuming or even infeasible in some cases {--} we design a domain adaptation gating mechanism which can be used in recurrent neural networks and quickly learn the out-of-domain knowledge directly from the word vector representations with little speed overhead. In our experiments, we use the recurrent neural network language model (LM) as a case study. We show that the neural LM perplexity can be reduced by 7.395 and 12.011 using the proposed domain adaptation mechanism on the Penn Treebank and News data, respectively. Furthermore, we show that using the domain-adapted neural LM to re-rank the statistical machine translation n-best list on the French-to-English language pair can significantly improve translation quality.",
}
@inproceedings{niehues-etal-2016-pre,
    title = "Pre-Translation for Neural Machine Translation",
    author = "Niehues, Jan  and
      Cho, Eunah  and
      Ha, Thanh-Le  and
      Waibel, Alex",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1172",
    pages = "1828--1836",
    abstract = "Recently, the development of neural machine translation (NMT) has significantly improved the translation quality of automatic machine translation. While most sentences are more accurate and fluent than translations by statistical machine translation (SMT)-based systems, in some cases, the NMT system produces translations that have a completely different meaning. This is especially the case when rare words occur. When using statistical machine translation, it has already been shown that significant gains can be achieved by simplifying the input in a preprocessing step. A commonly used example is the pre-reordering approach. In this work, we used phrase-based machine translation to pre-translate the input into the target language. Then a neural machine translation system generates the final hypothesis using the pre-translation. Thereby, we use either only the output of the phrase-based machine translation (PBMT) system or a combination of the PBMT output and the source sentence. We evaluate the technique on the English to German translation task. Using this approach we are able to outperform the PBMT system as well as the baseline neural MT system by up to 2 BLEU points. We analyzed the influence of the quality of the initial system on the final result.",
}
@inproceedings{van-der-wees-etal-2016-measuring,
    title = "Measuring the Effect of Conversational Aspects on Machine Translation Quality",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Monz, Christof",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1242",
    pages = "2571--2581",
    abstract = "Research in statistical machine translation (SMT) is largely driven by formal translation tasks, while translating informal text is much more challenging. In this paper we focus on SMT for the informal genre of dialogues, which has rarely been addressed to date. Concretely, we investigate the effect of dialogue acts, speakers, gender, and text register on SMT quality when translating fictional dialogues. We first create and release a corpus of multilingual movie dialogues annotated with these four dialogue-specific aspects. When measuring translation performance for each of these variables, we find that BLEU fluctuations between their categories are often significantly larger than randomly expected. Following this finding, we hypothesize and show that SMT of fictional dialogues benefits from adaptation towards dialogue acts and registers. Finally, we find that male speakers are harder to translate and use more vulgar language than female speakers, and that vulgarity is often not preserved during translation.",
}
@inproceedings{feng-etal-2016-improving,
    title = "Improving Attention Modeling with Implicit Distortion and Fertility for Machine Translation",
    author = "Feng, Shi  and
      Liu, Shujie  and
      Yang, Nan  and
      Li, Mu  and
      Zhou, Ming  and
      Zhu, Kenny Q.",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1290",
    pages = "3082--3092",
    abstract = "In neural machine translation, the attention mechanism facilitates the translation process by producing a soft alignment between the source sentence and the target sentence. However, without dedicated distortion and fertility models seen in traditional SMT systems, the learned alignment may not be accurate, which can lead to low translation quality. In this paper, we propose two novel models to improve attention-based neural machine translation. We propose a recurrent attention mechanism as an implicit distortion model, and a fertility conditioned decoder as an implicit fertility model. We conduct experiments on large-scale Chinese{--}English translation tasks. The results show that our models significantly improve both the alignment and translation quality compared to the original attention mechanism and several other variations.",
}
@inproceedings{tang-etal-2016-improving-translation,
    title = "Improving Translation Selection with Supersenses",
    author = "Tang, Haiqing  and
      Xiong, Deyi  and
      Lopez de Lacalle, Oier  and
      Agirre, Eneko",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1293",
    pages = "3114--3123",
    abstract = "Selecting appropriate translations for source words with multiple meanings still remains a challenge for statistical machine translation (SMT). One reason for this is that most SMT systems are not good at detecting the proper sense for a polysemic word when it appears in different contexts. In this paper, we adopt a supersense tagging method to annotate source words with coarse-grained ontological concepts. In order to enable the system to choose an appropriate translation for a word or phrase according to the annotated supersense of the word or phrase, we propose two translation models with supersense knowledge: a maximum entropy based model and a supersense embedding model. The effectiveness of our proposed models is validated on a large-scale English-to-Spanish translation task. Results indicate that our method can significantly improve translation quality via correctly conveying the meaning of the source language to the target language.",
}
@inproceedings{graham-etal-2016-glitters,
    title = "Is all that Glitters in Machine Translation Quality Estimation really Gold?",
    author = "Graham, Yvette  and
      Baldwin, Timothy  and
      Dowling, Meghan  and
      Eskevich, Maria  and
      Lynn, Teresa  and
      Tounsi, Lamia",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1294",
    pages = "3124--3134",
    abstract = "Human-targeted metrics provide a compromise between human evaluation of machine translation, where high inter-annotator agreement is difficult to achieve, and fully automatic metrics, such as BLEU or TER, that lack the validity of human assessment. Human-targeted translation edit rate (HTER) is by far the most widely employed human-targeted metric in machine translation, commonly employed, for example, as a gold standard in evaluation of quality estimation. Original experiments justifying the design of HTER, as opposed to other possible formulations, were limited to a small sample of translations and a single language pair, however, and this motivates our re-evaluation of a range of human-targeted metrics on a substantially larger scale. Results show significantly stronger correlation with human judgment for HBLEU over HTER for two of the nine language pairs we include and no significant difference between correlations achieved by HTER and HBLEU for the remaining language pairs. Finally, we evaluate a range of quality estimation systems employing HTER and direct assessment (DA) of translation adequacy as gold labels, resulting in a divergence in system rankings, and propose employment of DA for future quality estimation evaluations.",
}
@inproceedings{esperanca-rodier-didier-2016-translation,
    title = "Translation quality evaluation of {MWE} from {F}rench into {E}nglish using an {SMT} system",
    author = "Esperan{\c{c}}a-Rodier, Emmanuelle  and
      Didier, Johan",
    booktitle = "Proceedings of Translating and the Computer 38",
    month = nov # " 17-18",
    year = "2016",
    address = "London, UK",
    publisher = "AsLing",
    url = "https://aclanthology.org/2016.tc-1.4",
}
@inproceedings{bawden-etal-2016-investigating,
    title = "Investigating gender adaptation for speech translation",
    author = "Bawden, Rachel  and
      Wisniewski, Guillaume  and
      Maynard, H{\'e}l{\`e}ne",
    editor = "Danlos, Laurence  and
      Hamon, Thierry",
    booktitle = "Actes de la conf{\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters)",
    month = "7",
    year = "2016",
    address = "Paris, France",
    publisher = "AFCP - ATALA",
    url = "https://aclanthology.org/2016.jeptalnrecital-poster.23",
    pages = "490--497",
    abstract = "In this paper we investigate the impact of the integration of context into dialogue translation. We present a new contextual parallel corpus of television subtitles and show how taking into account speaker gender can significantly improve machine translation quality in terms of B LEU and M ETEOR scores. We perform a manual analysis, which suggests that these improvements are not necessary related to the morphological consequences of speaker gender, but to more general linguistic divergences.",
}
@inproceedings{junczys-dowmunt-etal-2016-neural,
    title = "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions",
    author = "Junczys-Dowmunt, Marcin  and
      Dwojak, Tomasz  and
      Hoang, Hieu",
    editor = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Cattoni, Rolando  and
      Federico, Marcello},
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.5",
    abstract = "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-persecond ratios.",
}
@inproceedings{federmann-lewis-2016-microsoft,
    title = "{M}icrosoft Speech Language Translation ({MSLT}) Corpus: The {IWSLT} 2016 release for {E}nglish, {F}rench and {G}erman",
    author = "Federmann, Christian  and
      Lewis, William D.",
    editor = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Cattoni, Rolando  and
      Federico, Marcello},
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.12",
    abstract = "We describe the Microsoft Speech Language Translation (MSLT) corpus, which was created in order to evaluate end-to-end conversational speech translation quality. The corpus was created from actual conversations over Skype, and we provide details on the recording setup and the different layers of associated text data. The corpus release includes Test and Dev sets with reference transcripts for speech recognition. Additionally, cleaned up transcripts and reference translations are available for evaluation of machine translation quality. The IWSLT 2016 release described here includes the source audio, raw transcripts, cleaned up transcripts, and translations to or from English for both French and German.",
}
@inproceedings{peter-etal-2016-rwth-aachen,
    title = "The {RWTH} {A}achen Machine Translation System for {IWSLT} 2016",
    author = "Peter, Jan-Thorsten  and
      Guta, Andreas  and
      Rossenbach, Nick  and
      Gra{\c{c}}a, Miguel  and
      Ney, Hermann",
    editor = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Cattoni, Rolando  and
      Federico, Marcello},
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.22",
    abstract = "This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of International Workshop on Spoken Language Translation (IWSLT) 2016. We have participated in the MT track for the German→English language pair employing our state-of-the-art phrase-based system, neural machine translation implementation and our joint translation and reordering decoder. Furthermore, we have applied feed-forward and recurrent neural language and translation models for reranking. The attention-based approach has been used for reranking the n-best lists for both phrasebased and hierarchical setups. On top of these systems, we make use of system combination to enhance the translation quality by combining individually trained systems.",
}
@inproceedings{niu-carpuat-2016-umd,
    title = "The {UMD} Machine Translation Systems at {IWSLT} 2016: {E}nglish-to-{F}rench Translation of Speech Transcripts",
    author = "Niu, Xing  and
      Carpuat, Marine",
    editor = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Cattoni, Rolando  and
      Federico, Marcello},
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.25",
    abstract = "We describe the University of Maryland machine translation system submitted to the IWSLT 2016 Microsoft Speech Language Translation (MSLT) English-French task. Our main finding is that translating conversation transcripts turned out to not be as challenging as we expected: while translation quality is of course not perfect, a straightforward phrase-based system trained on movie subtitles yields high BLEU scores (high 40s on the development set) and manual analysis of 100 examples showed that 61 of them were correctly translated, and errors were mostly local disfluencies in the remaining examples.",
}
@inproceedings{khalilov-2016-evaluation,
    title = "Evaluation of machine translation quality in e-commerce environment",
    author = "Khalilov, Maxim",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Users' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-users.19",
    pages = "240--262",
}
@inproceedings{decamp-2016-assessing,
    title = "Assessing Translation Quality Metrics",
    author = "DeCamp, Jennifer",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Users' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-users.22",
    pages = "304--321",
}
@inproceedings{sanchez-torron-koehn-2016-machine,
    title = "Machine Translation Quality and Post-Editor Productivity",
    author = "Sanchez-Torron, Marina  and
      Koehn, Philipp",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-researchers.2",
    pages = "16--26",
    abstract = "We assessed how different machine translation (MT) systems affect the post-editing (PE) process and product of professional English{--}Spanish translators. Our model found that for each 1-point increase in BLEU, there is a PE time decrease of 0.16 seconds per word, about 3-4{\%}. The MT system with the lowest BLEU score produced the output that was post-edited to the lowest quality and with the highest PE effort, measured both in HTER and actual PE operations.",
}
@inproceedings{imamura-sumita-2016-multi,
    title = "Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation",
    author = "Imamura, Kenji  and
      Sumita, Eiichiro",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-researchers.7",
    pages = "79--92",
    abstract = "Domain adaptation is a major challenge when applying machine translation to practical tasks. In this paper, we present domain adaptation methods for machine translation that assume multiple domains. The proposed methods combine two model types: a corpus-concatenated model covering multiple domains and single-domain models that are accurate but sparse in specific domains. We combine the advantages of both models using feature augmentation for domain adaptation in machine learning. Our experimental results show that the BLEU scores of the proposed method clearly surpass those of single-domain models for low-resource domains. For high-resource domains, the scores of the proposed method were superior to those of both single-domain and corpusconcatenated models. Even in domains having a million bilingual sentences, the translation quality was at least preserved and even improved in some domains. These results demonstrate that state-of-the-art domain adaptation can be realized with appropriate settings, even when using standard log-linear models.",
}
@inproceedings{knowles-koehn-2016-neural,
    title = "Neural Interactive Translation Prediction",
    author = "Knowles, Rebecca  and
      Koehn, Philipp",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-researchers.9",
    pages = "107--120",
    abstract = "We present an interactive translation prediction method based on neural machine translation. Even with the same translation quality of the underlying machine translation systems, the neural prediction method yields much higher word prediction accuracy (61.6{\%} vs. 43.3{\%}) than the traditional method based on search graphs, mainly due to better recovery from errors. We also develop efficient means to enable practical deployment.",
}
@inproceedings{chen-etal-2016-guided,
    title = "Guided Alignment Training for Topic-Aware Neural Machine Translation",
    author = "Chen, Wenhu  and
      Matusov, Evgeny  and
      Khadivi, Shahram  and
      Peter, Jan-Thorsten",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-researchers.10",
    pages = "121--134",
    abstract = "In this paper, we propose an effective way for biasing the attention mechanism of a sequence-to-sequence neural machine translation (NMT) model towards the well-studied statistical word alignment models. We show that our novel guided alignment training approach improves translation quality on real-life e-commerce texts consisting of product titles and descriptions, overcoming the problems posed by many unknown words and a large type/token ratio. We also show that meta-data associated with input texts such as topic or category information can significantly improve translation quality when used as an additional signal to the decoder part of the network. With both novel features, the BLEU score of the NMT system on a product title set improves from 18.6 to 21.3{\%}. Even larger MT quality gains are obtained through domain adaptation of a general domain NMT system to e-commerce data. The developed NMT system also performs well on the IWSLT speech translation task, where an ensemble of four variant systems outperforms the phrase-based baseline by 2.1{\%} BLEU absolute.",
}
@inproceedings{hewitt-etal-2016-automatic,
    title = "Automatic Construction of Morphologically Motivated Translation Models for Highly Inflected, Low-Resource Languages",
    author = "Hewitt, John  and
      Post, Matt  and
      Yarowsky, David",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-researchers.14",
    pages = "177--190",
    abstract = "Statistical Machine Translation (SMT) of highly inflected, low-resource languages suffers from the problem of low bitext availability, which is exacerbated by large inflectional paradigms. When translating into English, rich source inflections have a high chance of being poorly estimated or out-of-vocabulary (OOV). We present a source language-agnostic system for automatically constructing phrase pairs from foreign-language inflections and their morphological analyses using manually constructed datasets, including Wiktionary. We then demonstrate the utility of these phrase tables in improving translation into English from Finnish, Czech, and Turkish in simulated low-resource settings, finding substantial gains in translation quality. We report up to +2.58 BLEU in a simulated low-resource setting and +1.65 BLEU in a moderateresource setting. We release our morphologically-motivated translation models, with tens of thousands of inflections in each of 8 languages.",
}
@inproceedings{sturgeon-lee-2015-translation,
    title = "Translation Quality and Effort: Options versus Post-editing",
    author = "Sturgeon, Donald  and
      Lee, John S. Y.",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Sherly, Elizabeth",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Processing",
    month = dec,
    year = "2015",
    address = "Trivandrum, India",
    publisher = "NLP Association of India",
    url = "https://aclanthology.org/W15-5949",
    pages = "343--350",
}
@inproceedings{mehta-etal-2015-investigating,
    title = "Investigating the potential of post-ordering {SMT} output to improve translation quality",
    author = "Mehta, Pratik  and
      Kunchukuttan, Anoop  and
      Bhattacharyya, Pushpak",
    editor = "Sharma, Dipti Misra  and
      Sangal, Rajeev  and
      Sherly, Elizabeth",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Processing",
    month = dec,
    year = "2015",
    address = "Trivandrum, India",
    publisher = "NLP Association of India",
    url = "https://aclanthology.org/W15-5950",
    pages = "351--356",
}
@inproceedings{gaona-sharoff-2015-large,
    title = "Large Scale Translation Quality Estimation",
    author = "Gaona, Miguel Angel Rios  and
      Sharoff, Serge",
    editor = "Haji{\v{c}}, Jan  and
      Branco, Ant{\'o}nio",
    booktitle = "Proceedings of the 1st Deep Machine Translation Workshop",
    year = "2015",
    address = "Praha, Czechia",
    publisher = "{\'U}FAL MFF UK",
    url = "https://aclanthology.org/W15-5710",
    pages = "81--88",
}
@inproceedings{espla-gomis-etal-2015-using-line,
    title = "Using on-line available sources of bilingual information for word-level machine translation quality estimation",
    author = "Espl{\`a}-Gomis, Miquel  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Forcada, Mikel L.",
    editor = {El-Kahlout, {\.I}lknur Durgar  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Hollowood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation",
    month = may,
    year = "2015",
    address = "Antalya, Turkey",
    url = "https://aclanthology.org/W15-4903",
    pages = "19--26",
}
@inproceedings{scarton-etal-2015-searching-context,
    title = "Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation",
    author = "Scarton, Carolina  and
      Zampieri, Marcos  and
      Vela, Mihaela  and
      van Genabith, Josef  and
      Specia, Lucia",
    editor = {El-Kahlout, {\.I}lknur Durgar  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Hollowood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation",
    month = may,
    year = "2015",
    address = "Antalya, Turkey",
    url = "https://aclanthology.org/W15-4916",
    pages = "121--128",
}
@inproceedings{paetzold-etal-2015-okapi-quest,
    title = "{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi",
    author = "Paetzold, Gustavo Henrique  and
      Specia, Lucia  and
      Savourel, Yves",
    editor = {El-Kahlout, {\.I}lknur Durgar  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Hollowood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation",
    month = may,
    year = "2015",
    address = "Antalya, Turkey",
    url = "https://aclanthology.org/W15-4940",
    pages = "222",
}
@inproceedings{bicici-etal-2015-referential,
    title = "Referential Translation Machines for Predicting Translation Quality and Related Statistics",
    author = "Bi{\c{c}}ici, Ergun  and
      Liu, Qun  and
      Way, Andy",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3035",
    doi = "10.18653/v1/W15-3035",
    pages = "304--308",
}
@inproceedings{espla-gomis-etal-2015-ualacant,
    title = "{UA}lacant word-level machine translation quality estimation system at {WMT} 2015",
    author = "Espl{\`a}-Gomis, Miquel  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Forcada, Mikel",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3036",
    doi = "10.18653/v1/W15-3036",
    pages = "309--315",
}
@inproceedings{kreutzer-etal-2015-quality,
    title = "{QU}ality Estimation from {S}cra{TCH} ({QUETCH}): Deep Learning for Word-level Translation Quality Estimation",
    author = "Kreutzer, Julia  and
      Schamoni, Shigehiko  and
      Riezler, Stefan",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3037",
    doi = "10.18653/v1/W15-3037",
    pages = "316--322",
}
@inproceedings{shah-etal-2015-shef,
    title = "{SHEF}-{NN}: Translation Quality Estimation with Neural Networks",
    author = "Shah, Kashif  and
      Logacheva, Varvara  and
      Paetzold, Gustavo  and
      Blain, Frederic  and
      Beck, Daniel  and
      Bougares, Fethi  and
      Specia, Lucia",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3041",
    doi = "10.18653/v1/W15-3041",
    pages = "342--347",
}
@inproceedings{tezcan-etal-2015-ugent,
    title = "{UGENT}-{LT}3 {SCATE} System for Machine Translation Quality Estimation",
    author = "Tezcan, Arda  and
      Hoste, Veronique  and
      Desmet, Bart  and
      Macken, Lieve",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3043",
    doi = "10.18653/v1/W15-3043",
    pages = "353--360",
}
@article{sennrich-2015-modelling,
    title = "Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation",
    author = "Sennrich, Rico",
    editor = "Collins, Michael  and
      Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q15-1013",
    doi = "10.1162/tacl_a_00131",
    pages = "169--182",
    abstract = "The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps. Syntactic language models have the potential to fill this modelling gap. We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order. It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models. We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian. We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.",
}
@inproceedings{specia-etal-2015-multi,
    title = "Multi-level Translation Quality Prediction with {Q}u{E}st++",
    author = "Specia, Lucia  and
      Paetzold, Gustavo  and
      Scarton, Carolina",
    editor = "Chen, Hsin-Hsi  and
      Markert, Katja",
    booktitle = "Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics and The Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/P15-4020",
    doi = "10.3115/v1/P15-4020",
    pages = "115--120",
}
@inproceedings{c-de-souza-etal-2015-online,
    title = "Online Multitask Learning for Machine Translation Quality Estimation",
    author = "C. de Souza, Jos{\'e} G.  and
      Negri, Matteo  and
      Ricci, Elisa  and
      Turchi, Marco",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1022",
    doi = "10.3115/v1/P15-1022",
    pages = "219--228",
}
@inproceedings{graham-2015-improving,
    title = "Improving Evaluation of Machine Translation Quality Estimation",
    author = "Graham, Yvette",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1174",
    doi = "10.3115/v1/P15-1174",
    pages = "1804--1813",
}
@inproceedings{shah-etal-2015-investigating,
    title = "Investigating Continuous Space Language Models for Machine Translation Quality Estimation",
    author = "Shah, Kashif  and
      Ng, Raymond W. M.  and
      Bougares, Fethi  and
      Specia, Lucia",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1125",
    doi = "10.18653/v1/D15-1125",
    pages = "1073--1078",
}
@inproceedings{rowda-2015-machine,
    title = "Machine translation quality estimation: a linguist{'}s approach",
    author = "Rowda, Juan",
    booktitle = "Proceedings of Machine Translation Summit XV: User Track",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-users.13",
}
@inproceedings{espla-gomis-etal-2015-using,
    title = "Using on-line available sources of bilingual information for word-level machine translation quality estimation",
    author = "Espl{\`a}-Gomis, Miquel  and
      S{\'a}nchez-Mart{\'\i}nez, Felipe  and
      Forcada, Mikel L.",
    editor = {El‐Kahlout, {\.I}Iknur  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez‐Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez‐S{\'a}nchez, Gema  and
      Hollywood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the European Association for Machine Translation",
    month = may # " 11{--}13",
    year = "2015",
    address = "Antalya, Turkey",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2015.eamt-1.4",
}
@inproceedings{scarton-etal-2015-searching,
    title = "Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation",
    author = "Scarton, Carolina  and
      Zampieri, Marcos  and
      Vela, Mihaela  and
      van Genabith, Josef  and
      Specia, Lucia",
    editor = {El‐Kahlout, {\.I}Iknur  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez‐Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez‐S{\'a}nchez, Gema  and
      Hollywood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the European Association for Machine Translation",
    month = may # " 11{--}13",
    year = "2015",
    address = "Antalya, Turkey",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2015.eamt-1.17",
}
@inproceedings{paetzold-etal-2015-okapi,
    title = "{O}kapi+{Q}u{E}st: Translation Quality Estimation within Okapi",
    author = "Paetzold, Gustavo Henrique  and
      Specia, Lucia  and
      Savourel, Yves",
    editor = {El‐Kahlout, {\.I}Iknur  and
      {\"O}zkan, Mehmed  and
      S{\'a}nchez‐Mart{\'\i}nez, Felipe  and
      Ram{\'\i}rez‐S{\'a}nchez, Gema  and
      Hollywood, Fred  and
      Way, Andy},
    booktitle = "Proceedings of the 18th Annual Conference of the European Association for Machine Translation",
    month = may # " 11{--}13",
    year = "2015",
    address = "Antalya, Turkey",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2015.eamt-1.41",
}
@inproceedings{yang-lepage-2014-consistent,
    title = "Consistent Improvement in Translation Quality of {C}hinese-{J}apanese Technical Texts by Adding Additional Quasi-parallel Training Data",
    author = "Yang, Wei  and
      Lepage, Yves",
    editor = "Nakazawa, Toshiaki  and
      Mino, Hideya  and
      Goto, Isao  and
      Kurohashi, Sadao  and
      Sumita, Eiichiro",
    booktitle = "Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014)",
    month = oct,
    year = "2014",
    address = "Tokyo, Japan",
    publisher = "Workshop on Asian Translation",
    url = "https://aclanthology.org/W14-7010",
    pages = "69--76",
}
@inproceedings{beck-etal-2014-shef,
    title = "{SHEF}-Lite 2.0: Sparse Multi-task {G}aussian Processes for Translation Quality Estimation",
    author = "Beck, Daniel  and
      Shah, Kashif  and
      Specia, Lucia",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Specia, Lucia",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3338",
    doi = "10.3115/v1/W14-3338",
    pages = "307--312",
}
@inproceedings{bicici-way-2014-referential,
    title = "Referential Translation Machines for Predicting Translation Quality",
    author = "Bi{\c{c}}ici, Ergun  and
      Way, Andy",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Specia, Lucia",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3339",
    doi = "10.3115/v1/W14-3339",
    pages = "313--321",
}
@inproceedings{hokamp-etal-2014-target,
    title = "Target-Centric Features for Translation Quality Estimation",
    author = "Hokamp, Chris  and
      Calixto, Iacer  and
      Wagner, Joachim  and
      Zhang, Jian",
    editor = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Specia, Lucia",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-3341",
    doi = "10.3115/v1/W14-3341",
    pages = "329--334",
}
@inproceedings{koehn-germann-2014-impact,
    title = "The Impact of Machine Translation Quality on Human Post-Editing",
    author = "Koehn, Philipp  and
      Germann, Ulrich",
    editor = "Germann, Ulrich  and
      Carl, Michael  and
      Koehn, Philipp  and
      Sanchis-Trilles, Germ{\'a}n  and
      Casacuberta, Francisco  and
      Hill, Robin  and
      O{'}Brien, Sharon",
    booktitle = "Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-0307",
    doi = "10.3115/v1/W14-0307",
    pages = "38--46",
}
@inproceedings{pushpananda-etal-2014-sinhala,
    title = "{S}inhala-{T}amil Machine Translation: Towards better Translation Quality",
    author = "Pushpananda, Randil  and
      Weerasinghe, Ruvan  and
      Niranjan, Mahesan",
    editor = "Ferraro, Gabriela  and
      Wan, Stephen",
    booktitle = "Proceedings of the Australasian Language Technology Association Workshop 2014",
    month = nov,
    year = "2014",
    address = "Melbourne, Australia",
    url = "https://aclanthology.org/U14-1018",
    pages = "129--133",
}
@article{clark-etal-2014-locally,
    title = "Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization",
    author = "Clark, Jonathan H.  and
      Dyer, Chris  and
      Lavie, Alon",
    editor = "Lin, Dekang  and
      Collins, Michael  and
      Lee, Lillian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q14-1031",
    doi = "10.1162/tacl_a_00191",
    pages = "393--404",
    abstract = "Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers.",
}
@inproceedings{aharoni-etal-2014-automatic,
    title = "Automatic Detection of Machine Translated Text and Translation Quality Estimation",
    author = "Aharoni, Roee  and
      Koppel, Moshe  and
      Goldberg, Yoav",
    editor = "Toutanova, Kristina  and
      Wu, Hua",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2048",
    doi = "10.3115/v1/P14-2048",
    pages = "289--295",
}
@inproceedings{tufis-2014-large,
    title = "Large {SMT} data-sets extracted from {W}ikipedia",
    author = "Tufi{\c{s}}, Dan",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/103_Paper.pdf",
    pages = "656--663",
    abstract = "The article presents experiments on mining Wikipedia for extracting SMT useful sentence pairs in three language pairs. Each extracted sentence pair is associated with a cross-lingual lexical similarity score based on which, several evaluations have been conducted to estimate the similarity thresholds which allow the extraction of the most useful data for training three-language pairs SMT systems. The experiments showed that for a similarity score higher than 0.7 all sentence pairs in the three language pairs were fully parallel. However, including in the training sets less parallel sentence pairs (that is with a lower similarity score) showed significant improvements in the translation quality (BLEU-based evaluations). The optimized SMT systems were evaluated on unseen test-sets also extracted from Wikipedia. As one of the main goals of our work was to help Wikipedia contributors to translate (with as little post editing as possible) new articles from major languages into less resourced languages and vice-versa, we call this type of translation experiments in-genre translation. As in the case of in-domain translation, our evaluations showed that using only in-genre training data for translating same genre new texts is better than mixing the training data with out-of-genre (even) parallel texts.",
}
@inproceedings{drexler-etal-2014-wikipedia,
    title = "A {W}ikipedia-based Corpus for Contextualized Machine Translation",
    author = "Drexler, Jennifer  and
      Rastogi, Pushpendre  and
      Aguilar, Jacqueline  and
      Van Durme, Benjamin  and
      Post, Matt",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/1217_Paper.pdf",
    pages = "3593--3596",
    abstract = "We describe a corpus for target-contextualized machine translation (MT), where the task is to improve the translation of source documents using language models built over presumably related documents in the target language. The idea presumes a situation where most of the information about a topic is in a foreign language, yet some related target-language information is known to exist. Our corpus comprises a set of curated English Wikipedia articles describing news events, along with (i) their Spanish counterparts and (ii) some of the Spanish source articles cited within them. In experiments, we translated these Spanish documents, treating the English articles as target-side context, and evaluate the effect on translation quality when including target-side language models built over this English context and interpolated with other, separately-derived language model data. We find that even under this simplistic baseline approach, we achieve significant improvements as measured by BLEU score.",
}
@inproceedings{more-climent-2014-machine,
    title = "Machine Translationness: Machine-likeness in Machine Translation Evaluation",
    author = "Mor{\'e}, Joaquim  and
      Climent, Salvador",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/506_Paper.pdf",
    pages = "54--61",
    abstract = "Machine translationness (MTness) is the linguistic phenomena that make machine translations distinguishable from human translations. This paper intends to present MTness as a research object and suggests an MT evaluation method based on determining whether the translation is machine-like instead of determining its human-likeness as in evaluation current approaches. The method rates the MTness of a translation with a metric, the MTS (Machine Translationness Score). The MTS calculation is in accordance with the results of an experimental study on machine translation perception by common people. MTS proved to correlate well with human ratings on translation quality. Besides, our approach allows the performance of cheap evaluations since expensive resources (e.g. reference translations, training corpora) are not needed. The paper points out the challenge of dealing with MTness as an everyday phenomenon caused by the massive use of MT.",
}
@inproceedings{gotti-etal-2014-hashtag,
    title = "Hashtag Occurrences, Layout and Translation: A Corpus-driven Analysis of Tweets Published by the {C}anadian Government",
    author = "Gotti, Fabrizio  and
      Langlais, Phillippe  and
      Farzindar, Atefeh",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/53_Paper.pdf",
    pages = "2254--2261",
    abstract = "We present an aligned bilingual corpus of 8758 tweet pairs in French and English, derived from Canadian government agencies. Hashtags appear in a tweet{'}s prologue, announcing its topic, or in the tweet{'}s text in lieu of traditional words, or in an epilogue. Hashtags are words prefixed with a pound sign in 80{\%} of the cases. The rest is mostly multiword hashtags, for which we describe a segmentation algorithm. A manual analysis of the bilingual alignment of 5000 hashtags shows that 5{\%} (French) to 18{\%} (English) of them don{'}t have a counterpart in their containing tweet{'}s translation. This analysis shows that 80{\%} of multiword hashtags are correctly translated by humans, and that the mistranslation of the rest may be due to incomplete translation directives regarding social media. We show how these resources and their analysis can guide the design of a machine translation pipeline, and its evaluation. A baseline system implementing a tweet-specific tokenizer yields promising results. The system is improved by translating epilogues, prologues, and text separately. We attempt to feed the SMT engine with the original hashtag and some alternatives ({``}dehashed{''} version or a segmented version of multiword hashtags), but translation quality improves at the cost of hashtag recall.",
}
@inproceedings{daems-etal-2014-origin,
    title = "On the origin of errors: A fine-grained analysis of {MT} and {PE} errors and their relationship",
    author = "Daems, Joke  and
      Macken, Lieve  and
      Vandepitte, Sonia",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/532_Paper.pdf",
    pages = "62--66",
    abstract = "In order to improve the symbiosis between machine translation (MT) system and post-editor, it is not enough to know that the output of one system is better than the output of another system. A fine-grained error analysis is needed to provide information on the type and location of errors occurring in MT and the corresponding errors occurring after post-editing (PE). This article reports on a fine-grained translation quality assessment approach which was applied to machine translated-texts and the post-edited versions of these texts, made by student post-editors. By linking each error to the corresponding source text-passage, it is possible to identify passages that were problematic in MT, but not after PE, or passages that were problematic even after PE. This method provides rich data on the origin and impact of errors, which can be used to improve post-editor training as well as machine translation systems. We present the results of a pilot experiment on the post-editing of newspaper articles and highlight the advantages of our approach.",
}
@inproceedings{herrmann-etal-2014-manual,
    title = "Manual Analysis of Structurally Informed Reordering in {G}erman-{E}nglish Machine Translation",
    author = "Herrmann, Teresa  and
      Niehues, Jan  and
      Waibel, Alex",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/569_Paper.pdf",
    pages = "4379--4386",
    abstract = "Word reordering is a difficult task for translation. Common automatic metrics such as BLEU have problems reflecting improvements in target language word order. However, it is a crucial aspect for humans when deciding on translation quality. This paper presents a detailed analysis of a structure-aware reordering approach applied in a German-to-English phrase-based machine translation system. We compare the translation outputs of two translation systems applying reordering rules based on parts-of-speech and syntax trees on a sentence-by-sentence basis. For each sentence-pair we examine the global translation performance and classify local changes in the translated sentences. This analysis is applied to three data sets representing different genres. While the improvement in BLEU differed substantially between the data sets, the manual evaluation showed that both global translation performance as well as individual types of improvements and degradations exhibit a similar behavior throughout the three data sets. We have observed that for 55-64{\%} of the sentences with different translations, the translation produced using the tree-based reordering was considered to be the better translation. As intended by the investigated reordering model, most improvements are achieved by improving the position of the verb or being able to translate a verb that could not be translated before.",
}
@inproceedings{seretan-etal-2014-large,
    title = "A Large-Scale Evaluation of Pre-editing Strategies for Improving User-Generated Content Translation",
    author = "Seretan, Violeta  and
      Bouillon, Pierrette  and
      Gerlach, Johanna",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/676_Paper.pdf",
    pages = "1793--1799",
    abstract = "The user-generated content represents an increasing share of the information available today. To make this type of content instantly accessible in another language, the ACCEPT project focuses on developing pre-editing technologies for correcting the source text in order to increase its translatability. Linguistically-informed pre-editing rules have been developed for English and French for the two domains considered by the project, namely, the technical domain and the healthcare domain. In this paper, we present the evaluation experiments carried out to assess the impact of the proposed pre-editing rules on translation quality. Results from a large-scale evaluation campaign show that pre-editing helps indeed attain a better translation quality for a high proportion of the data, the difference with the number of cases where the adverse effect is observed being statistically significant. The ACCEPT pre-editing technology is freely available online and can be used in any Web-based environment to enhance the translatability of user-generated content so that it reaches a broader audience.",
}
@inproceedings{galinskaya-etal-2014-measuring,
    title = "Measuring the Impact of Spelling Errors on the Quality of Machine Translation",
    author = "Galinskaya, Irina  and
      Gusev, Valentin  and
      Mescheryakova, Elena  and
      Shmatova, Mariya",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/708_Paper.pdf",
    pages = "2683--2689",
    abstract = "In this paper we show how different types of spelling errors influence the quality of machine translation. We also propose a method to evaluate the impact of spelling errors correction on translation quality without expensive manual work of providing reference translations.",
}
@inproceedings{kordoni-simova-2014-multiword,
    title = "Multiword Expressions in Machine Translation",
    author = "Kordoni, Valia  and
      Simova, Iliana",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/723_Paper.pdf",
    pages = "1208--1211",
    abstract = "This work describes an experimental evaluation of the significance of phrasal verb treatment for obtaining better quality statistical machine translation (SMT) results. The importance of the detection and special treatment of phrasal verbs is measured in the context of SMT, where the word-for-word translation of these units often produces incoherent results. Two ways of integrating phrasal verb information in a phrase-based SMT system are presented. Automatic and manual evaluations of the results reveal improvements in the translation quality in both experiments.",
}
@inproceedings{goto-etal-2014-crowdsourcing,
    title = "Crowdsourcing for Evaluating Machine Translation Quality",
    author = "Goto, Shinsuke  and
      Lin, Donghui  and
      Ishida, Toru",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/756_Paper.pdf",
    pages = "3456--3463",
    abstract = "The recent popularity of machine translation has increased the demand for the evaluation of translations. However, the traditional evaluation approach, manual checking by a bilingual professional, is too expensive and too slow. In this study, we confirm the feasibility of crowdsourcing by analyzing the accuracy of crowdsourcing translation evaluations. We compare crowdsourcing scores to professional scores with regard to three metrics: translation-score, sentence-score, and system-score. A Chinese to English translation evaluation task was designed using around the NTCIR-9 PATENT parallel corpus with the goal being 5-range evaluations of adequacy and fluency. The experiment shows that the average score of crowdsource workers well matches professional evaluation results. The system-score comparison strongly indicates that crowdsourcing can be used to find the best translation system given the input of 10 source sentence.",
}
@inproceedings{shah-etal-2014-efficient,
    title = "An efficient and user-friendly tool for machine translation quality estimation",
    author = "Shah, Kashif  and
      Turchi, Marco  and
      Specia, Lucia",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Loftsson, Hrafn  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
    month = may,
    year = "2014",
    address = "Reykjavik, Iceland",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/964_Paper.pdf",
    abstract = "We present a new version of QUEST ― an open source framework for machine translation quality estimation ― which brings a number of improvements: (i) it provides a Web interface and functionalities such that non-expert users, e.g. translators or lay-users of machine translations, can get quality predictions (or internal features of the framework) for translations without having to install the toolkit, obtain resources or build prediction models; (ii) it significantly improves over the previous runtime performance by keeping resources (such as language models) in memory; (iii) it provides an option for users to submit the source text only and automatically obtain translations from Bing Translator; (iv) it provides a ranking of multiple translations submitted by users for each source text according to their estimated quality. We exemplify the use of this new version through some experiments with the framework.",
}
@inproceedings{williams-koehn-2014-syntax,
    title = "Syntax-Based Statistical Machine Translation",
    author = "Williams, Philip  and
      Koehn, Philipp",
    editor = "Specia, Lucia  and
      Carreras, Xavier",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-2005",
    abstract = "The tutorial explains in detail syntax-based statistical machine translation with synchronous context free grammars (SCFG). It is aimed at researchers who have little background in this area, and gives a comprehensive overview about the main models and methods.While syntax-based models in statistical machine translation have a long history, spanning back almost 20 years, they have only recently shown superior translation quality over the more commonly used phrase-based models, and are now considered state of the art for some language pairs, such as Chinese-English (since ISI's submission to NIST 2006), and English-German (since Edinburgh's submission to WMT 2012).While the field is very dynamic, there is a core set of methods that have become dominant. Such SCFG models are implemented in the open source machine translation toolkit Moses, and the tutors draw from the practical experience of its development.The tutorial focuses on explaining core established concepts in SCFG-based approaches, which are the most popular in this area. The main goal of the tutorial is for the audience to understand how these systems work end-to-end. We review as much relevant literature as necessary, but the tutorial is not a primarily research survey.The tutorial is rounded up with open problems and advanced topics, such as computational challenges, different formalisms for syntax-based models and inclusion of semantics.",
}
@inproceedings{he-etal-2014-transformation,
    title = "Transformation from Discontinuous to Continuous Word Alignment Improves Translation Quality",
    author = "He, Zhongjun  and
      Wu, Hua  and
      Wang, Haifeng  and
      Liu, Ting",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1016",
    doi = "10.3115/v1/D14-1016",
    pages = "147--152",
}
@inproceedings{federico-etal-2014-assessing,
    title = "Assessing the Impact of Translation Errors on Machine Translation Quality with Mixed-effects Models",
    author = "Federico, Marcello  and
      Negri, Matteo  and
      Bentivogli, Luisa  and
      Turchi, Marco",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1172",
    doi = "10.3115/v1/D14-1172",
    pages = "1643--1653",
}
@inproceedings{c-de-souza-etal-2014-machine,
    title = "Machine Translation Quality Estimation Across Domains",
    author = "C. de Souza, Jos{\'e} G.  and
      Turchi, Marco  and
      Negri, Matteo",
    editor = "Tsujii, Junichi  and
      Hajic, Jan",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1040",
    pages = "409--420",
}
@inproceedings{drugan-2014-top,
    title = "Top-down or bottom-up: what do industry approaches to translation quality mean for effective integration of standards and tools?",
    author = "Drugan, Joanna",
    booktitle = "Proceedings of Translating and the Computer 36",
    month = nov # " 27-28",
    year = "2014",
    address = "London, UK",
    publisher = "AsLing",
    url = "https://aclanthology.org/2014.tc-1.14",
}
@inproceedings{hunsicker-ceausu-2014-machine,
    title = "Machine translation quality estimation adapted to the translation workflow",
    author = "Hunsicker, Sabine  and
      Ceausu, Alexandru",
    booktitle = "Proceedings of Translating and the Computer 36",
    month = nov # " 27-28",
    year = "2014",
    address = "London, UK",
    publisher = "AsLing",
    url = "https://aclanthology.org/2014.tc-1.18",
}
@inproceedings{baumann-etal-2014-towards,
    title = "Towards simultaneous interpreting: the timing of incremental machine translation and speech synthesis",
    author = "Baumann, Timo  and
      Bangalore, Srinivas  and
      Hirschberg, Julia",
    editor = {Federico, Marcello  and
      St{\"u}ker, Sebastian  and
      Yvon, Fran{\c{c}}ois},
    booktitle = "Proceedings of the 11th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 4-5",
    year = "2014",
    address = "Lake Tahoe, California",
    url = "https://aclanthology.org/2014.iwslt-papers.2",
    pages = "163--168",
    abstract = "In simultaneous interpreting, human experts incrementally construct and extend partial hypotheses about the source speaker{'}s message, and start to verbalize a corresponding message in the target language, based on a partial translation {--} which may have to be corrected occasionally. They commence the target utterance in the hope that they will be able to finish understanding the source speaker{'}s message and determine its translation in time for the unfolding delivery. Of course, both incremental understanding and translation by humans can be garden-pathed, although experts are able to optimize their delivery so as to balance the goals of minimal latency, translation quality and high speech fluency with few corrections. We investigate the temporal properties of both translation input and output to evaluate the tradeoff between low latency and translation quality. In addition, we estimate the improvements that can be gained with a tempo-elastic speech synthesizer.",
}
@inproceedings{wu-etal-2014-rule,
    title = "Rule-based preordering on multiple syntactic levels in statistical machine translation",
    author = "Wu, Ge  and
      Zhang, Yuqi  and
      Waibel, Alexander",
    editor = {Federico, Marcello  and
      St{\"u}ker, Sebastian  and
      Yvon, Fran{\c{c}}ois},
    booktitle = "Proceedings of the 11th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 4-5",
    year = "2014",
    address = "Lake Tahoe, California",
    url = "https://aclanthology.org/2014.iwslt-papers.18",
    pages = "279--286",
    abstract = "We propose a novel data-driven rule-based preordering approach, which uses the tree information of multiple syntactic levels. This approach extend the tree-based reordering from one level into multiple levels, which has the capability to process more complicated reordering cases. We have conducted experiments in English-to-Chinese and Chinese-to-English translation directions. Our results show that the approach has led to improved translation quality both when it was applied separately or when it was combined with some other reordering approaches. As our reordering approach was used alone, it showed an improvement of 1.61 in BLEU score in the English-to-Chinese translation direction and an improvement of 2.16 in BLEU score in the Chinese-to-English translation direction, in comparison with the baseline, which used no word reordering. As our preordering approach were combined with the short rule [1], long rule [2] and tree rule [3] based preordering approaches, it showed further improvements of up to 0.43 in BLEU score in the English-to-Chinese translation direction and further improvements of up to 0.3 in BLEU score in the Chinese-to-English translation direction. Through the translations that used our preordering approach, we have also found many translation examples with improved syntactic structures.",
}
@inproceedings{segal-etal-2014-limsi,
    title = "{LIMSI} {E}nglish-{F}rench speech translation system",
    author = "Segal, Natalia  and
      Bonneau-Maynard, H{\'e}l{\`e}ne  and
      Do, Quoc Khanh  and
      Allauzen, Alexandre  and
      Gauvain, Jean-Luc  and
      Lamel, Lori  and
      Yvon, Fran{\c{c}}ois",
    editor = {Federico, Marcello  and
      St{\"u}ker, Sebastian  and
      Yvon, Fran{\c{c}}ois},
    booktitle = "Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 4-5",
    year = "2014",
    address = "Lake Tahoe, California",
    url = "https://aclanthology.org/2014.iwslt-evaluation.15",
    pages = "106--112",
    abstract = "This paper documents the systems developed by LIMSI for the IWSLT 2014 speech translation task (English→French). The main objective of this participation was twofold: adapting different components of the ASR baseline system to the peculiarities of TED talks and improving the machine translation quality on the automatic speech recognition output data. For the latter task, various techniques have been considered: punctuation and number normalization, adaptation to ASR errors, as well as the use of structured output layer neural network models for speech data.",
}
@inproceedings{scarton-specia-2014-document,
    title = "Document-level translation quality estimation: exploring discourse and pseudo-references",
    author = "Scarton, Carolina  and
      Specia, Lucia",
    editor = "Cettolo, Mauro  and
      Federico, Marcello  and
      Specia, Lucia  and
      Way, Andy",
    booktitle = "Proceedings of the 17th Annual Conference of the European Association for Machine Translation",
    month = jun # " 16-18",
    year = "2014",
    address = "Dubrovnik, Croatia",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2014.eamt-1.21",
    pages = "101--108",
}
@inproceedings{ocurran-2014-translation,
    title = "Translation quality in post-edited versus human-translated segments: a case study",
    author = "O{'}Curran, Elaine",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-wptp.9",
    pages = "113--118",
    abstract = "We analyze the linguistic quality results for a post-editing productivity test that contains a 3:1 ratio of post-edited segments versus human-translated segments, in order to assess if there is a difference in the final translation quality of each segment type and also to investigate the type of errors that are found in each segment type. Overall, we find that the human-translated segments contain more errors per word than the post-edited segments and although the error categories logged are similar across the two segment types, the most notable difference is that the number of stylistic errors in the human translations is 3 times higher than in the post-edited translations.",
}
@inproceedings{specia-shah-2014-quest,
    title = "{Q}u{E}st: A framework for translation quality estimation",
    author = "Specia, Lucia  and
      Shah, Kashif",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-wptp.12",
    pages = "121",
    abstract = "We present QUEST, an open source framework for translation quality estimation. QUEST provides a wide range of feature extractors from source and translation texts and external resources and tools. These go from simple, language-independent features, to advanced, linguistically motivated features. They include features that rely on information from the translation system and features that are oblivious to the way translations were produced. In addition, it provides wrappers for a well-known machine learning toolkit, scikit-learn, including techniques for feature selection and model building, as well as parameter optimisation. We also present a Web interface and functionalities for non-expert users. Using this interface, quality predictions (or internal features of the framework) can be obtained without the installation of the toolkit and the building of prediction models. The interface also provides a ranking method for multiple translations given for the same source text according to their predicted quality.",
}
@inproceedings{alabau-etal-2014-integrating,
    title = "Integrating online and active learning in a computer-assisted translation workbench",
    author = "Alabau, Vicent  and
      Gonz{\'a}lez-Rubio, Jes{\'u}s  and
      Ortiz-Mart{\'\i}nez, Daniel  and
      Sanchis-Trilles, Germ{\'a}n  and
      Casacuberta, Francisco  and
      Garc{\'\i}a-Mart{\'\i}nez, Mercedes  and
      Mesa-Lao, Bartolom{\'e}  and
      Petersen, Dan Cheung  and
      Dragsted, Barbara  and
      Carl, Michael",
    editor = "Casacuberta, Francisco  and
      Federico, Marcello  and
      Koehn, Philipp",
    booktitle = "Workshop on interactive and adaptive machine translation",
    month = oct # " 22",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-workshop.1",
    pages = "1--8",
    abstract = "This paper describes a pilot study with a computed-assisted translation workbench aiming at testing the integration of online and active learning features. We investigate the effect of these features on translation productivity, using interactive translation prediction (ITP) as a baseline. User activity data were collected from five beta testers using key-logging and eye-tracking. User feedback was also collected at the end of the experiments in the form of retrospective think-aloud protocols. We found that OL performs better than ITP, especially in terms of translation speed. In addition, AL provides better translation quality than ITP for the same levels of user effort. We plan to incorporate these features in the final version of the workbench.",
}
@inproceedings{germann-2014-dynamic,
    title = "Dynamic phrase tables for machine translation in an interactive post-editing scenario",
    author = "Germann, Ulrich",
    editor = "Casacuberta, Francisco  and
      Federico, Marcello  and
      Koehn, Philipp",
    booktitle = "Workshop on interactive and adaptive machine translation",
    month = oct # " 22",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-workshop.3",
    pages = "20--31",
    abstract = "This paper presents a phrase table implementation for the Moses system that computes phrase table entries for phrase-based statistical machine translation (PBSMT) on demand by sampling an indexed bitext. While this approach has been used for years in hierarchical phrase-based translation, the PBSMT community has been slow to adopt this paradigm, due to concerns that this would be slow and lead to lower translation quality. The experiments conducted in the course of this work provide evidence to the contrary: without loss in translation quality, the sampling phrase table ranks second out of four in terms of speed, being slightly slower than hash table look-up (Junczys-Dowmunt, 2012) and considerably faster than current implementations of the approach suggested by Zens and Ney (2007). In addition, the underlying parallel corpus can be updated in real time, so that professionally produced translations can be used to improve the quality of the machine translation engine immediately.",
}
@inproceedings{mathur-cettolo-2014-optimized,
    title = "Optimized {MT} online learning in computer assisted translation",
    author = "Mathur, Prashant  and
      Cettolo, Mauro",
    editor = "Casacuberta, Francisco  and
      Federico, Marcello  and
      Koehn, Philipp",
    booktitle = "Workshop on interactive and adaptive machine translation",
    month = oct # " 22",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-workshop.4",
    pages = "32--41",
    abstract = "In this paper we propose a cascading framework for optimizing online learning in machine translation for a computer assisted translation scenario. With the use of online learning, several hyperparameters associated with the learning algorithm are introduced. The number of iterations of online learning can affect the translation quality as well. We discuss these issues and propose a few approaches to optimize the hyperparameters and to find the number of iterations required for online learning. We experimentally show that optimizing hyperparameters and number of iterations in online learning yields consistent improvement against baseline results.",
}
@inproceedings{cettolo-etal-2014-repetition,
    title = "The repetition rate of text as a predictor of the effectiveness of machine translation adaptation",
    author = "Cettolo, Mauro  and
      Bertoldi, Nicola  and
      Federico, Marcello",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.13",
    pages = "166--179",
    abstract = "Since the effectiveness of MT adaptation relies on the text repetitiveness, the question on how to measure repetitions in a text naturally arises. This work deals with the issue of looking for and evaluating text features that might help the prediction of the impact of MT adaptation on translation quality. In particular, the repetition rate metric, we recently proposed, is compared to other features employed in very related NLP tasks. The comparison is carried out through a regression analysis between feature values and MT performance gains by dynamically adapted versus non-adapted MT engines, on five different translation tasks. The main outcome of experiments is that the repetition rate correlates better than any other considered feature with the MT gains yielded by the online adaptation, although using all features jointly results in better predictions than with any single feature.",
}
@inproceedings{wuebker-etal-2014-comparison,
    title = "Comparison of data selection techniques for the translation of video lectures",
    author = "Wuebker, Joern  and
      Ney, Hermann  and
      Mart{\'\i}nez-Villaronga, Adri{\`a}  and
      Gim{\'e}nez, Adri{\`a}  and
      Juan, Alfons  and
      Servan, Christophe  and
      Dymetman, Marc  and
      Mirkin, Shachar",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.15",
    pages = "193--207",
    abstract = "For the task of online translation of scientific video lectures, using huge models is not possible. In order to get smaller and efficient models, we perform data selection. In this paper, we perform a qualitative and quantitative comparison of several data selection techniques, based on cross-entropy and infrequent n-gram criteria. In terms of BLEU, a combination of translation and language model cross-entropy achieves the most stable results. As another important criterion for measuring translation quality in our application, we identify the number of out-of-vocabulary words. Here, infrequent n-gram recovery shows superior performance. Finally, we combine the two selection techniques in order to benefit from both their strengths.",
}
@inproceedings{ruiz-federico-2014-assessing,
    title = "Assessing the impact of speech recognition errors on machine translation quality",
    author = "Ruiz, Nicholas  and
      Federico, Marcello",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.20",
    pages = "261--274",
    abstract = "In spoken language translation, it is crucial that an automatic speech recognition (ASR) system produces outputs that can be adequately translated by a statistical machine translation (SMT) system. While word error rate (WER) is the standard metric of ASR quality, the assumption that each ASR error type is weighted equally is violated in a SMT system that relies on structured input. In this paper, we outline a statistical framework for analyzing the impact of specific ASR error types on translation quality in a speech translation pipeline. Our approach is based on linear mixed-effects models, which allow the analysis of ASR errors on a translation quality metric. The mixed-effects models take into account the variability of ASR systems and the difficulty of each speech utterance being translated in a specific experimental setting. We use mixed-effects models to verify that the ASR errors that compose the WER metric do not contribute equally to translation quality and that interactions exist between ASR errors that cumulatively affect a SMT system{'}s ability to translate an utterance. Our experiments are carried out on the English to French language pair using eight ASR systems and seven post-edited machine translation references from the IWSLT 2013 evaluation campaign. We report significant findings that demonstrate differences in the contributions of specific ASR error types toward speech translation quality and suggest further error types that may contribute to translation difficulty.",
}
@inproceedings{specia-shah-2014-predicting,
    title = "Predicting human translation quality",
    author = "Specia, Lucia  and
      Shah, Kashif",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.22",
    pages = "288--300",
    abstract = "We present a first attempt at predicting the quality of translations produced by human, professional translators. We examine datasets annotated for quality at sentence- and word-level for four language pairs and provide experiments with prediction models for these datasets. We compare the performance of such models against that of models built from machine translations, highlighting a number of challenges in estimating quality and detecting errors in human translations.",
}
@inproceedings{mirkin-besacier-2014-data,
    title = "Data selection for compact adapted {SMT} models",
    author = "Mirkin, Shachar  and
      Besacier, Laurent",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.23",
    pages = "301--314",
    abstract = "Data selection is a common technique for adapting statistical translation models for a specific domain, which has been shown to both improve translation quality and to reduce model size. Selection relies on some in-domain data, of the same domain of the texts expected to be translated. Selecting the sentence-pairs that are most similar to the in-domain data from a pool of parallel texts has been shown to be effective; yet, this approach holds the risk of resulting in a limited coverage, when necessary n-grams that do appear in the pool are less similar to in-domain data that is available in advance. Some methods select additional data based on the actual text that needs to be translated. While useful, this is not always a practical scenario. In this work we describe an extensive exploration of data selection techniques over Arabic to French datasets, and propose methods to address both similarity and coverage considerations while maintaining a limited model size.",
}
@inproceedings{dholakia-sarkar-2014-pivot,
    title = "Pivot-based triangulation for low-resource languages",
    author = "Dholakia, Rohit  and
      Sarkar, Anoop",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.24",
    pages = "315--328",
    abstract = "This paper conducts a comprehensive study on the use of triangulation for four very low-resource languages: Mawukakan and Maninkakan, Haitian Kreyol and Malagasy. To the best of our knowledge, ours is the first effective translation system for the first two of these languages. We improve translation quality by adding data using pivot languages and exper- imentally compare previously proposed triangulation design options. Furthermore, since the low-resource language pair and pivot language pair data typically come from very different domains, we use insights from domain adaptation to tune the weighted mixture of direct and pivot based phrase pairs to improve translation quality.",
}
@inproceedings{mansour-etal-2014-automatic,
    title = "Automatic dialect classification for statistical machine translation",
    author = "Mansour, Saab  and
      Al-Onaizan, Yaser  and
      Blackwood, Graeme  and
      Tillmann, Christoph",
    editor = "Al-Onaizan, Yaser  and
      Simard, Michel",
    booktitle = "Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track",
    month = oct # " 22-26",
    year = "2014",
    address = "Vancouver, Canada",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2014.amta-researchers.26",
    pages = "342--355",
    abstract = "The training data for statistical machine translation are gathered from various sources representing a mixture of domains. In this work, we argue that when translating dialects representing varieties of the same language, a manually assigned data source is not a reliable indicator of the dialect. We resort to automatic dialect classification to refine the training corpora according to the different dialects and build improved dialect specific systems. A fairly standard classifier for Arabic developed within this work achieves state-of-the-art performance, with classification precision above 90{\%}, making it usefully accurate for our application. The classification of the data is then used to distinguish between the different dialects, split the data accordingly, and utilize the new splits for several adaptation techniques. Performing translation experiments on a large scale dialectal Arabic to English translation task, our results show that the classifier generates better contrast between the dialects and achieves superior translation quality than using the original manual corpora splits.",
}
@inproceedings{beck-etal-2013-shef,
    title = "{SHEF}-{L}ite: When Less is More for Translation Quality Estimation",
    author = "Beck, Daniel  and
      Shah, Kashif  and
      Cohn, Trevor  and
      Specia, Lucia",
    editor = "Bojar, Ondrej  and
      Buck, Christian  and
      Callison-Burch, Chris  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Saint-Amand, Herve  and
      Soricut, Radu  and
      Specia, Lucia",
    booktitle = "Proceedings of the Eighth Workshop on Statistical Machine Translation",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-2241",
    pages = "337--342",
}
@inproceedings{hajlaoui-2013-acts,
    title = "Are {ACT}{'}s Scores Increasing with Better Translation Quality?",
    author = "Hajlaoui, Najeh",
    editor = "Bojar, Ondrej  and
      Buck, Christian  and
      Callison-Burch, Chris  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Saint-Amand, Herve  and
      Soricut, Radu  and
      Specia, Lucia",
    booktitle = "Proceedings of the Eighth Workshop on Statistical Machine Translation",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-2252",
    pages = "408--413",
}
@article{wang-zong-2013-large,
    title = "Large-scale Word Alignment Using Soft Dependency Cohesion Constraints",
    author = "Wang, Zhiguo  and
      Zong, Chengqing",
    editor = "Lin, Dekang  and
      Collins, Michael",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "1",
    year = "2013",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q13-1024",
    doi = "10.1162/tacl_a_00228",
    pages = "291--300",
    abstract = "Dependency cohesion refers to the observation that phrases dominated by disjoint dependency subtrees in the source language generally do not overlap in the target language. It has been verified to be a useful constraint for word alignment. However, previous work either treats this as a hard constraint or uses it as a feature in discriminative models, which is ineffective for large-scale tasks. In this paper, we take dependency cohesion as a soft constraint, and integrate it into a generative model for large-scale word alignment experiments. We also propose an approximate EM algorithm and a Gibbs sampling algorithm to estimate model parameters in an unsupervised manner. Experiments on large-scale Chinese-English translation tasks demonstrate that our model achieves improvements in both alignment quality and translation quality.",
}
@article{bisazza-federico-2013-dynamically,
    title = "Dynamically Shaping the Reordering Search Space of Phrase-Based Statistical Machine Translation",
    author = "Bisazza, Arianna  and
      Federico, Marcello",
    editor = "Lin, Dekang  and
      Collins, Michael",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "1",
    year = "2013",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q13-1027",
    doi = "10.1162/tacl_a_00231",
    pages = "327--340",
    abstract = "Defining the reordering search space is a crucial issue in phrase-based SMT between distant languages. In fact, the optimal trade-off between accuracy and complexity of decoding is nowadays reached by harshly limiting the input permutation space. We propose a method to dynamically shape such space and, thus, capture long-range word movements without hurting translation quality nor decoding time. The space defined by loose reordering constraints is dynamically pruned through a binary classifier that predicts whether a given input word should be translated right after another. The integration of this model into a phrase-based decoder improves a strong Arabic-English baseline already including state-of-the-art early distortion cost (Moore and Quirk, 2007) and hierarchical phrase orientation models (Galley and Manning, 2008). Significant improvements in the reordering of verbs are achieved by a system that is notably faster than the baseline, while bleu and meteor remain stable, or even increase, at a very high distortion limit.",
}
@inproceedings{specia-etal-2013-quest,
    title = "{Q}u{E}st - A translation quality estimation framework",
    author = "Specia, Lucia  and
      Shah, Kashif  and
      de Souza, Jose G.C.  and
      Cohn, Trevor",
    editor = "Butt, Miriam  and
      Hussain, Sarmad",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-4014",
    pages = "79--84",
}
@inproceedings{cohn-specia-2013-modelling,
    title = "Modelling Annotator Bias with Multi-task {G}aussian Processes: An Application to Machine Translation Quality Estimation",
    author = "Cohn, Trevor  and
      Specia, Lucia",
    editor = "Schuetze, Hinrich  and
      Fung, Pascale  and
      Poesio, Massimo",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1004",
    pages = "32--42",
}
@inproceedings{wang-zhang-2013-imag,
    title = "i{MAG} : {MT}-postediting, translation quality evaluation and parallel corpus production (i{MAG} : post-{\'e}dition, {\'e}valuation de qualit{\'e} de {TA} et production d{'}un corpus parall{\`e}le) [in {F}rench]",
    author = "Wang, Lingxiao  and
      Zhang, Ying",
    editor = "Morin, Emmanuel  and
      Est{\`e}ve, Yannick",
    booktitle = "Proceedings of TALN 2013 (Volume 3: System Demonstrations)",
    month = jun,
    year = "2013",
    address = "Les Sables d{'}Olonne, France",
    publisher = "ATALA",
    url = "https://aclanthology.org/F13-3008",
    pages = "801--802",
}
@inproceedings{burchardt-2013-multidimensional,
    title = "Multidimensional quality metrics: a flexible system for assessing translation quality",
    author = "Burchardt, Aljoscha",
    booktitle = "Proceedings of Translating and the Computer 35",
    month = nov # " 28-29",
    year = "2013",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2013.tc-1.6",
}
@inproceedings{van-der-meer-2013-dqf,
    title = "The {DQF} - industry best-practices, metrics and benchmarks for translation quality estimation",
    author = "van der Meer, Jaap",
    booktitle = "Proceedings of Translating and the Computer 35",
    month = nov # " 28-29",
    year = "2013",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2013.tc-1.8",
}
@inproceedings{specia-2013-machine,
    title = "Machine translation quality estimation",
    author = "Specia, Lucia",
    booktitle = "Proceedings of Translating and the Computer 35",
    month = nov # " 28-29",
    year = "2013",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2013.tc-1.10",
}
@inproceedings{daems-etal-2013-quality,
    title = "Quality as the sum of its parts: a two-step approach for the identification of translation problems and translation quality assessment for {HT} and {MT}+{PE}",
    author = "Daems, Joke  and
      Macken, Lieve  and
      Vandepitte, Sonia",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Proceedings of the 2nd Workshop on Post-editing Technology and Practice",
    month = sep # " 2",
    year = "2013",
    address = "Nice, France",
    url = "https://aclanthology.org/2013.mtsummit-wptp.8",
}
@inproceedings{rubino-etal-2013-key,
    title = "Key Problems in Conversion from Simplified to Traditional {C}hinese Characters Topic Models for Translation Quality Estimation for Gisting Purposes",
    author = "Rubino, Raphael  and
      Camargo de Souza, Jose Guilherme  and
      Foster, Jennifer  and
      Specia, Lucia",
    editor = "Way, Andy  and
      Sima{'}an, Khalil  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of Machine Translation Summit XIV: Posters",
    month = sep # " 2-6",
    year = "2013",
    address = "Nice, France",
    url = "https://aclanthology.org/2013.mtsummit-posters.12",
}
@inproceedings{rubino-etal-2013-topic,
    title = "Topic Models for Translation Quality Estimation for Gisting Purposes",
    author = "Rubino, Raphael  and
      Camargo de Souza, Jose Guilherme  and
      Foster, Jennifer  and
      Specia, Lucia",
    editor = "Way, Andy  and
      Sima{'}an, Khalil  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of Machine Translation Summit XIV: Posters",
    month = sep # " 2-6",
    year = "2013",
    address = "Nice, France",
    url = "https://aclanthology.org/2013.mtsummit-posters.13",
}
@inproceedings{formiga-etal-2013-real,
    title = "Real-life Translation Quality Estimation for {MT} System Selection",
    author = "Formiga, Lluis  and
      Marquez, Lluis  and
      Pujantell, Jaume",
    editor = "Way, Andy  and
      Sima{'}an, Khalil  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of Machine Translation Summit XIV: Papers",
    month = sep # " 2-6",
    year = "2013",
    address = "Nice, France",
    url = "https://aclanthology.org/2013.mtsummit-papers.9",
}
@inproceedings{shah-etal-2013-investigation,
    title = "An Investigation on the Effectiveness of Features for Translation Quality Estimation",
    author = "Shah, Kashif  and
      Conn, Trevor  and
      Specia, Lucia",
    editor = "Way, Andy  and
      Sima{'}an, Khalil  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of Machine Translation Summit XIV: Papers",
    month = sep # " 2-6",
    year = "2013",
    address = "Nice, France",
    url = "https://aclanthology.org/2013.mtsummit-papers.21",
}
@inproceedings{gonzalez-rubio-etal-2013-emprical,
    title = "Emprical study of a two-step approach to estimate translation quality",
    author = "Gonz{\'a}lez-Rubio, Jes{\'u}s  and
      Navarro-Cerd{\'a}n, J. Ram{\'o}n  and
      Casacuberta, Francisco",
    editor = "Zhang, Joy Ying",
    booktitle = "Proceedings of the 10th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 5-6",
    year = "2013",
    address = "Heidelberg, Germany",
    url = "https://aclanthology.org/2013.iwslt-papers.5",
    abstract = "We present a method to estimate the quality of automatic translations when reference translations are not available. Quality estimation is addressed as a two-step regression problem where multiple features are combined to predict a quality score. Given a set of features, we aim at automatically extracting the variables that better explain translation quality, and use them to predict the quality score. The soundness of our approach is assessed by the encouraging results obtained in an exhaustive experimentation with several feature sets. Moreover, the studied approach is highly-scalable allowing us to employ hundreds of features to predict translation quality.",
}
@inproceedings{cho-etal-2013-crf,
    title = "{CRF}-based disfluency detection using semantic features for {G}erman to {E}nglish spoken language translation",
    author = "Cho, Eunah  and
      Ha, Than-Le  and
      Waibel, Alex",
    editor = "Zhang, Joy Ying",
    booktitle = "Proceedings of the 10th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 5-6",
    year = "2013",
    address = "Heidelberg, Germany",
    url = "https://aclanthology.org/2013.iwslt-papers.12",
    abstract = "Disfluencies in speech pose severe difficulties in machine translation of spontaneous speech. This paper presents our conditional random field (CRF)-based speech disfluency detection system developed on German to improve spoken language translation performance. In order to detect speech disfluencies considering syntactics and semantics of speech utterances, we carried out a CRF-based approach using information learned from the word representation and the phrase table used for machine translation. The word representation is gained using recurrent neural networks and projected words are clustered using the k-means algorithm. Using the output from the model trained with the word representations and phrase table information, we achieve an improvement of 1.96 BLEU points on the lecture test set. By keeping or removing humanannotated disfluencies, we show an upper bound and lower bound of translation quality. In an oracle experiment we gain 3.16 BLEU points of improvement on the lecture test set, compared to the same set with all disfluencies.",
}
@inproceedings{na-lee-2013-discriminative,
    title = "A discriminative reordering parser for {IWSLT} 2013",
    author = "Na, Hwidong  and
      Lee, Jong-Hyeok",
    editor = "Zhang, Joy Ying",
    booktitle = "Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 5-6",
    year = "2013",
    address = "Heidelberg, Germany",
    url = "https://aclanthology.org/2013.iwslt-evaluation.9",
    abstract = "We participated in the IWSLT 2013 Evaluation Campaign for the MT track for two official directions: German↔English. Our system consisted of a reordering module and a statistical machine translation (SMT) module under a pre-ordering SMT framework. We trained the reordering module using three scalable methods in order to utilize training instances as many as possible. The translation quality of our primary submissions were comparable to that of a hierarchical phrasebased SMT, which usually requires a longer time to decode.",
}
@inproceedings{freitag-etal-2013-eu,
    title = "{EU}-{BRIDGE} {MT}: text translation of talks in the {EU}-{BRIDGE} project",
    author = "Freitag, Markus  and
      Peitz, Stephan  and
      Wuebker, Joern  and
      Ney, Hermann  and
      Durrani, Nadir  and
      Huck, Matthias  and
      Koehn, Philipp  and
      Ha, Thanh-Le  and
      Niehues, Jan  and
      Mediani, Mohammed  and
      Herrmann, Teresa  and
      Waibel, Alex  and
      Bertoldi, Nicola  and
      Cettolo, Mauro  and
      Federico, Marcello",
    editor = "Zhang, Joy Ying",
    booktitle = "Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 5-6",
    year = "2013",
    address = "Heidelberg, Germany",
    url = "https://aclanthology.org/2013.iwslt-evaluation.16",
    abstract = "EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, English→French and German→English. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems.",
}
@inproceedings{hardmeier-etal-2012-tree,
    title = "Tree Kernels for Machine Translation Quality Estimation",
    author = {Hardmeier, Christian  and
      Nivre, Joakim  and
      Tiedemann, J{\"o}rg},
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Soricut, Radu  and
      Specia, Lucia",
    booktitle = "Proceedings of the Seventh Workshop on Statistical Machine Translation",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W12-3112",
    pages = "109--113",
}
@inproceedings{popovic-2012-morpheme,
    title = "Morpheme- and {POS}-based {IBM}1 and language model scores for translation quality estimation",
    author = "Popovi{\'c}, Maja",
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Soricut, Radu  and
      Specia, Lucia",
    booktitle = "Proceedings of the Seventh Workshop on Statistical Machine Translation",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W12-3116",
    pages = "133--137",
}
@inproceedings{pighin-etal-2012-analysis,
    title = "An Analysis (and an Annotated Corpus) of User Responses to Machine Translation Output",
    author = "Pighin, Daniele  and
      M{\`a}rquez, Llu{\'\i}s  and
      May, Jonathan",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/337_Paper.pdf",
    pages = "1131--1136",
    abstract = "We present an annotated resource consisting of open-domain translation requests, automatic translations and user-provided corrections collected from casual users of the translation portal \url{http://reverso.net}. The layers of annotation provide: 1) quality assessments for 830 correction suggestions for translations into English, at the segment level, and 2) 814 usefulness assessments for English-Spanish and English-French translation suggestions, a suggestion being useful if it contains at least local clues that can be used to improve translation quality. We also discuss the results of our preliminary experiments concerning 1) the development of an automatic filter to separate useful from non-useful feedback, and 2) the incorporation in the machine translation pipeline of bilingual phrases extracted from the suggestions. The annotated data, available for download from \url{ftp://mi.eng.cam.ac.uk/data/faust/LW-UPC-Oct11-FAUST-feedback-annotation.tgz}, is released under a Creative Commons license. To our best knowledge, this is the first resource of this kind that has ever been made publicly available.",
}
@inproceedings{mansour-ney-2012-arabic,
    title = "{A}rabic-Segmentation Combination Strategies for Statistical Machine Translation",
    author = "Mansour, Saab  and
      Ney, Hermann",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/509_Paper.pdf",
    pages = "3915--3920",
    abstract = "Arabic segmentation was already applied successfully for the task of statistical machine translation (SMT). Yet, there is no consistent comparison of the effect of different techniques and methods over the final translation quality. In this work, we use existing tools and further re-implement and develop new methods for segmentation. We compare the resulting SMT systems based on the different segmentation methods over the small IWSLT 2010 BTEC and the large NIST 2009 Arabic-to-English translation tasks. Our results show that for both small and large training data, segmentation yields strong improvements, but, the differences between the top ranked segmenters are statistically insignificant. Due to the different methodologies that we apply for segmentation, we expect a complimentary variation in the results achieved by each method. As done in previous work, we combine several segmentation schemes of the same model but achieve modest improvements. Next, we try a different strategy, where we combine the different segmentation methods rather than the different segmentation schemes. In this case, we achieve stronger improvements over the best single system. Finally, combining schemes and methods has another slight gain over the best combination strategy.",
}
@inproceedings{shi-etal-2012-two,
    title = "Two Phase Evaluation for Selecting Machine Translation Services",
    author = "Shi, Chunqi  and
      Lin, Donghui  and
      Shimada, Masahiko  and
      Ishida, Toru",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/802_Paper.pdf",
    pages = "1771--1778",
    abstract = "An increased number of machine translation services are now available. Unfortunately, none of them can provide adequate translation quality for all input sources. This forces the user to select from among the services according to his needs. However, it is tedious and time consuming to perform this manual selection. Our solution, proposed here, is an automatic mechanism that can select the most appropriate machine translation service. Although evaluation methods are available, such as BLEU, NIST, WER, etc., their evaluation results are not unanimous regardless of the translation sources. We proposed a two-phase architecture for selecting translation services. The first phase uses a data-driven classification to allow the most appropriate evaluation method to be selected according to each translation source. The second phase selects the most appropriate machine translation result by the selected evaluation method. We describe the architecture, detail the algorithm, and construct a prototype. Tests show that the proposal yields better translation quality than employing just one machine translation service.",
}
@inproceedings{shi-etal-2012-service,
    title = "Service Composition Scenarios for Task-Oriented Translation",
    author = "Shi, Chunqi  and
      Lin, Donghui  and
      Ishida, Toru",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/808_Paper.pdf",
    pages = "2951--2958",
    abstract = "Due to instant availability and low cost, machine translation is becoming popular. Machine translation mediated communication plays a more and more important role in international collaboration. However, machine translators cannot guarantee high quality translation. In a multilingual communication task, many in-domain resources, for example domain dictionaries, are needed to promote translation quality. This raises the problem of how to help communication task designers provide higher quality translation systems, systems that can take advantage of various in-domain resources. The Language Grid, a service-oriented collective intelligent platform, allows in-domain resources to be wrapped into language services. For task-oriented translation, we propose service composition scenarios for the composition of different language services, where various in-domain resources are utilized effectively. We design the architecture, provide a script language as the interface for the task designer, which is easy for describing the composition scenario, and make a case study of a Japanese-English campus orientation task. Based on the case study, we analyze the increase in translation quality possible and the usage of in-domain resources. The results demonstrate a clear improvement in translation accuracy when the in-domain resources are used.",
}
@inproceedings{bouamor-etal-2012-identifying,
    title = "Identifying bilingual Multi-Word Expressions for Statistical Machine Translation",
    author = "Bouamor, Dhouha  and
      Semmar, Nasredine  and
      Zweigenbaum, Pierre",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/886_Paper.pdf",
    pages = "674--679",
    abstract = "MultiWord Expressions (MWEs) repesent a key issue for numerous applications in Natural Language Processing (NLP) especially for Machine Translation (MT). In this paper, we describe a strategy for detecting translation pairs of MWEs in a French-English parallel corpus. In addition we introduce three methods aiming to integrate extracted bilingual MWE S in M OSES, a phrase based Statistical Machine Translation (SMT) system. We experimentally show that these textual units can improve translation quality.",
}
@inproceedings{holmqvist-etal-2012-alignment,
    title = "Alignment-based reordering for {SMT}",
    author = "Holmqvist, Maria  and
      Stymne, Sara  and
      Ahrenberg, Lars  and
      Merkel, Magnus",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/1000_Paper.pdf",
    pages = "3436--3440",
    abstract = "We present a method for improving word alignment quality for phrase-based statistical machine translation by reordering the source text according to the target word order suggested by an initial word alignment. The reordered text is used to create a second word alignment which can be an improvement of the first alignment, since the word order is more similar. The method requires no other pre-processing such as part-of-speech tagging or parsing. We report improved Bleu scores for English-to-German and English-to-Swedish translation. We also examined the effect on word alignment quality and found that the reordering method increased recall while lowering precision, which partly can explain the improved Bleu scores. A manual evaluation of the translation output was also performed to understand what effect our reordering method has on the translation system. We found that where the system employing reordering differed from the baseline in terms of having more words, or a different word order, this generally led to an improvement in translation quality.",
}
@inproceedings{gavrila-etal-2012-domain,
    title = "Same domain different discourse style - A case study on Language Resources for data-driven Machine Translation",
    author = "Gavrila, Monica  and
      v. Hahn, Walther  and
      Vertan, Cristina",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/1003_Paper.pdf",
    pages = "3441--3446",
    abstract = "Data-driven machine translation (MT) approaches became very popular during last years, especially for language pairs for which it is difficult to find specialists to develop transfer rules. Statistical (SMT) or example-based (EBMT) systems can provide reasonable translation quality for assimilation purposes, as long as a large amount of training data is available. Especially SMT systems rely on parallel aligned corpora which have to be statistical relevant for the given language pair. The construction of large domain specific parallel corpora is time- and cost-consuming; the current practice relies on one or two big such corpora per language pair. Recent developed strategies ensure certain portability to other domains through specialized lexicons or small domain specific corpora. In this paper we discuss the influence of different discourse styles on statistical machine translation systems. We investigate how a pure SMT performs when training and test data belong to same domain but the discourse style varies.",
}
@inproceedings{wang-etal-2012-demo,
    title = "Demo of i{MAG} Possibilities: {MT}-postediting, Translation Quality Evaluation, Parallel Corpus Production",
    author = "Wang, Ling Xiao  and
      Zhang, Ying  and
      Boitet, Christian  and
      Bellynck, Valerie",
    editor = "Kay, Martin  and
      Boitet, Christian",
    booktitle = "Proceedings of {COLING} 2012: Demonstration Papers",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://aclanthology.org/C12-3060",
    pages = "475--482",
}
@inproceedings{banerjee-etal-2012-translation,
    title = "Translation Quality-Based Supplementary Data Selection by Incremental Update of Translation Models",
    author = "Banerjee, Pratyush  and
      Naskar, Sudip Kumar  and
      Roturier, Johann  and
      Way, Andy  and
      van Genabith, Josef",
    editor = "Kay, Martin  and
      Boitet, Christian",
    booktitle = "Proceedings of {COLING} 2012",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://aclanthology.org/C12-1010",
    pages = "149--166",
}
@inproceedings{cho-etal-2012-segmentation,
    title = "Segmentation and punctuation prediction in speech language translation using a monolingual translation system",
    author = "Cho, Eunah  and
      Niehues, Jan  and
      Waibel, Alex",
    booktitle = "Proceedings of the 9th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 6-7",
    year = "2012",
    address = "Hong Kong, Table of contents",
    url = "https://aclanthology.org/2012.iwslt-papers.15",
    pages = "252--259",
    abstract = "In spoken language translation (SLT), finding proper segmentation and reconstructing punctuation marks are not only significant but also challenging tasks. In this paper we present our recent work on speech translation quality analysis for German-English by improving sentence segmentation and punctuation. From oracle experiments, we show an upper bound of translation quality if we had human-generated segmentation and punctuation on the output stream of speech recognition systems. In our oracle experiments we gain 1.78 BLEU points of improvements on the lecture test set. We build a monolingual translation system from German to German implementing segmentation and punctuation prediction as a machine translation task. Using the monolingual translation system we get an improvement of 1.53 BLEU points on the lecture test set, which is a comparable performance against the upper bound drawn by the oracle experiments.",
}
@inproceedings{mediani-etal-2012-kit,
    title = "The {KIT} translation systems for {IWSLT} 2012",
    author = {Mediani, Mohammed  and
      Zhang, Yuqi  and
      Ha, Thanh-Le  and
      Niehues, Jan  and
      Cho, Eunach  and
      Herrmann, Teresa  and
      K{\"a}rgel, Rainer  and
      Waibel, Alexander},
    booktitle = "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 6-7",
    year = "2012",
    address = "Hong Kong, Table of contents",
    url = "https://aclanthology.org/2012.iwslt-evaluation.3",
    pages = "38--45",
    abstract = "In this paper, we present the KIT systems participating in the English-French TED Translation tasks in the framework of the IWSLT 2012 machine translation evaluation. We also present several additional experiments on the English-German, English-Chinese and English-Arabic translation pairs. Our system is a phrase-based statistical machine translation system, extended with many additional models which were proven to enhance the translation quality. For instance, it uses the part-of-speech (POS)-based reordering, translation and language model adaptation, bilingual language model, word-cluster language model, discriminative word lexica (DWL), and continuous space language model. In addition to this, the system incorporates special steps in the preprocessing and in the post-processing step. In the preprocessing the noisy corpora are filtered by removing the noisy sentence pairs, whereas in the postprocessing the agreement between a noun and its surrounding words in the French translation is corrected based on POS tags with morphological information. Our system deals with speech transcription input by removing case information and punctuation except periods from the text translation model.",
}
@inproceedings{chu-etal-2012-ebmt,
    title = "{EBMT} system of {K}yoto {U}niversity in {OLYMPICS} task at {IWSLT} 2012",
    author = "Chu, Chenhui  and
      Nakazawa, Toshiaki  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 6-7",
    year = "2012",
    address = "Hong Kong, Table of contents",
    url = "https://aclanthology.org/2012.iwslt-evaluation.12",
    pages = "96--101",
    abstract = "This paper describes the EBMT system of Kyoto University that participated in the OLYMPICS task at IWSLT 2012. When translating very different language pairs such as Chinese-English, it is very important to handle sentences in tree structures to overcome the difference. Many recent studies incorporate tree structures in some parts of translation process, but not all the way from model training (alignment) to decoding. Our system is a fully tree-based translation system where we use the Bayesian phrase alignment model on dependency trees and example-based translation. To improve the translation quality, we conduct some special processing for the IWSLT 2012 OLYMPICS task, including sub-sentence splitting, non-parallel sentence filtering, adoption of an optimized Chinese segmenter and rule-based decoding constraints.",
}
@inproceedings{na-lee-2012-forest,
    title = "Forest-to-string translation using binarized dependency forest for {IWSLT} 2012 {OLYMPICS} task",
    author = "Na, Hwidong  and
      Lee, Jong-Hyeok",
    booktitle = "Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 6-7",
    year = "2012",
    address = "Hong Kong, Table of contents",
    url = "https://aclanthology.org/2012.iwslt-evaluation.18",
    pages = "130--135",
    abstract = "We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses.",
}
@inproceedings{nn-2012-confident,
    title = "Confident {MT} - Estimating Translation Quality for Improved Statistical Machine Translation",
    editor = "Cettolo, Mauro  and
      Federico, Marcello  and
      Specia, Lucia  and
      Way, Andy",
    booktitle = "Proceedings of the 16th Annual Conference of the European Association for Machine Translation",
    month = may # " 28{--}30",
    year = "2012",
    address = "Trento, Italy",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2012.eamt-1.30",
    pages = "98",
}
@inproceedings{melby-etal-2012-reliably,
    title = "Reliably Assessing the Quality of Post-edited Translation Based on Formalized Structured Translation Specifications",
    author = "Melby, Alan K.  and
      Housley, Jason  and
      Fields, Paul J.  and
      Tuioti, Emily",
    editor = "O'Brien, Sharon  and
      Simard, Michel  and
      Specia, Lucia",
    booktitle = "Workshop on Post-Editing Technology and Practice",
    month = oct # " 28",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-wptp.4",
    abstract = "Post-editing of machine translation has become more common in recent years. This has created the need for a formal method of assessing the performance of post-editors in terms of whether they are able to produce post-edited target texts that follow project specifications. This paper proposes the use of formalized structured translation specifications (FSTS) as a basis for post-editor assessment. To determine if potential evaluators are able to reliably assess the quality of post-edited translations, an experiment used texts representing the work of five fictional post-editors. Two software applications were developed to facilitate the assessment: the Ruqual Specifications Writer, which aids in establishing post-editing project specifications; and Ruqual Rubric Viewer, which provides a graphical user interface for constructing a rubric in a machine-readable format. Seventeen non-experts rated the translation quality of each simulated post-edited text. Intraclass correlation analysis showed evidence that the evaluators were highly reliable in evaluating the performance of the post-editors. Thus, we assert that using FSTS specifications applied through the Ruqual software tools provides a useful basis for evaluating the quality of post-edited texts.",
}
@inproceedings{ahmed-etal-2012-hierarchical,
    title = "Hierarchical Phrase-Based {MT} for Phonetic Representation-Based Speech Translation",
    author = "Ahmed, Zeeshan  and
      Jiang, Jie  and
      Carson-Berndsen, Julie  and
      Cahill, Peter  and
      Way, Andy",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.1",
    abstract = "The paper presents a novel technique for speech translation using hierarchical phrased-based statistical machine translation (HPB-SMT). The system is based on translation of speech from phone sequences as opposed to conventional approach of speech translation from word sequences. The technique facilitates speech translation by allowing a machine translation (MT) system to access to phonetic information. This enables the MT system to act as both a word recognition and a translation component. This results in better performance than conventional speech translation approaches by recovering from recognition error with help of a source language model, translation model and target language model. For this purpose, the MT translation models are adopted to work on source language phones using a grapheme-to-phoneme component. The source-side phonetic confusions are handled using a confusion network. The result on IWLST'10 English- Chinese translation task shows a significant improvement in translation quality. In this paper, results for HPB-SMT are compared with previously published results of phrase-based statistical machine translation (PB-SMT) system (Baseline). The HPB-SMT system outperforms PB-SMT in this regard.",
}
@inproceedings{clark-etal-2012-one,
    title = "One System, Many Domains: Open-Domain Statistical Machine Translation via Feature Augmentation",
    author = "Clark, Jonathan  and
      Lavie, Alon  and
      Dyer, Chris",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.4",
    abstract = "In this paper, we introduce a simple technique for incorporating domain information into a statistical machine translation system that significantly improves translation quality when test data comes from multiple domains. Our approach augments (conjoins) standard translation model and language model features with domain indicator features and requires only minimal modifications to the optimization and decoding procedures. We evaluate our method on two language pairs with varying numbers of domains, and observe significant improvements of up to 1.0 BLEU.",
}
@inproceedings{denkowski-lavie-2012-challenges,
    title = "Challenges in Predicting Machine Translation Utility for Human Post-Editors",
    author = "Denkowski, Michael  and
      Lavie, Alon",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.6",
    abstract = "As machine translation quality continues to improve, the idea of using MT to assist human translators becomes increasingly attractive. In this work, we discuss and provide empirical evidence of the challenges faced when adapting traditional MT systems to provide automatic translations for human post-editors to correct. We discuss the differences between this task and traditional adequacy-based tasks and the challenges that arise when using automatic metrics to predict the amount of effort required to post-edit translations. A series of experiments simulating a real-world localization scenario shows that current metrics under-perform on this task, even when tuned to maximize correlation with expert translator judgments, illustrating the need to rethink traditional MT pipelines when addressing the challenges of this translation task.",
}
@inproceedings{koehn-haddow-2012-interpolated,
    title = "Interpolated Backoff for Factored Translation Models",
    author = "Koehn, Philipp  and
      Haddow, Barry",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.9",
    abstract = "We propose interpolated backoff methods to strike the balance between traditional surface form translation models and factored models that decompose translation into lemma and morphological feature mapping steps. We show that this approach improves translation quality by 0.5 BLEU (German{--}English) over phrase-based models, due to the better translation of rare nouns and adjectives.",
}
@inproceedings{pighin-etal-2012-graph,
    title = "A Graph-based Strategy to Streamline Translation Quality Assessments",
    author = "Pighin, Daniele  and
      Formiga, Llu{\'\i}s  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.13",
    abstract = "We present a detailed analysis of a graph-based annotation strategy that we employed to annotate a corpus of 11,292 real-world English to Spanish automatic translations with relative (ranking) and absolute (adequate/non-adequate) quality assessments. The proposed approach, inspired by previous work in Interactive Evolutionary Computation and Interactive Genetic Algorithms, results in a simpler and faster annotation process. We empirically compare the method against a traditional, explicit ranking approach, and show that the graph-based strategy: 1) is considerably faster, and 2) produces consistently more reliable annotations.",
}
@inproceedings{sankaran-etal-2012-compact,
    title = "Compact Rule Extraction for Hierarchical Phrase-based Translation",
    author = "Sankaran, Baskaran  and
      Haffari, Gholamreza  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.16",
    abstract = "This paper introduces two novel approaches for extracting compact grammars for hierarchical phrase-based translation. The first is a combinatorial optimization approach and the second is a Bayesian model over Hiero grammars using Variational Bayes for inference. In contrast to the conventional Hiero (Chiang, 2007) rule extraction algorithm , our methods extract compact models reducing model size by 17.8{\%} to 57.6{\%} without impacting translation quality across several language pairs. The Bayesian model is particularly effective for resource-poor languages with evidence from Korean-English translation. To our knowledge, this is the first alternative to Hiero-style rule extraction that finds a more compact synchronous grammar without hurting translation performance.",
}
@inproceedings{niehues-waibel-2012-detailed,
    title = "Detailed Analysis of Different Strategies for Phrase Table Adaptation in {SMT}",
    author = "Niehues, Jan  and
      Waibel, Alex",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.19",
    abstract = "This paper gives a detailed analysis of different approaches to adapt a statistical machine translation system towards a target domain using small amounts of parallel in-domain data. Therefore, we investigate the differences between the approaches addressing adaptation on the two main steps of building a translation model: The candidate selection and the phrase scoring. For the latter step we characterized the differences by four key aspects. We performed experiments on two different tasks of speech translation and analyzed the influence of the different aspects on the overall translation quality. On both tasks we could show significant improvements by using the presented adaptation techniques.",
}
@inproceedings{meyer-etal-2012-machine,
    title = "Machine Translation of Labeled Discourse Connectives",
    author = "Meyer, Thomas  and
      Popescu-Belis, Andrei  and
      Hajlaoui, Najeh  and
      Gesmundo, Andrea",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.20",
    abstract = "This paper shows how the disambiguation of discourse connectives can improve their automatic translation, while preserving the overall performance of statistical MT as measured by BLEU. State-of-the-art automatic classifiers for rhetorical relations are used prior to MT to label discourse connectives that signal those relations. These labels are used for MT in two ways: (1) by augmenting factored translation models; and (2) by using the probability distributions of labels in order to train and tune SMT. The improvement of translation quality is demonstrated using a new semi-automated metric for discourse connectives, on the English/French WMT10 data, while BLEU scores remain comparable to non-discourse-aware systems, due to the low frequency of discourse connectives.",
}
@inproceedings{federmann-2012-hybrid,
    title = "Hybrid Machine Translation Using Joint, Binarised Feature Vectors",
    author = "Federmann, Christian",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.23",
    abstract = "We present an approach for Hybrid Machine Translation, based on a Machine-Learning framework. Our method combines output from several source systems. We first define an extensible, total order on translations and use it to estimate a ranking on the sentence level for a given set of systems. We introduce and define the notion of joint, binarised feature vectors. We train an SVM-based classifier and show how its classification results can be used to create hybrid translations. We describe a series of oracle experiments on data sets from the WMT11 translation task in order to find an upper bound regarding the achievable level of translation quality. We also present results from first experiments with an implemented version of our system. Evaluation using NIST and BLEU metrics indicates that the proposed method can outperform its individual source systems. An interesting finding is that our approach allows to leverage good translations from otherwise bad systems as the translation quality estimation is based on sentence-level phenomena rather than corpus-level metrics. We conclude by summarising our findings and by giving an outlook to future work.",
}
@inproceedings{johnson-2012-conditional,
    title = "Conditional Significance Pruning: Discarding More of Huge Phrase Tables",
    author = "Johnson, Howard",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.28",
    abstract = "The technique of pruning phrase tables that are used for statistical machine translation (SMT) can achieve substantial reductions in bulk and improve translation quality, especially for very large corpora such at the Giga-FrEn. This can be further improved by conditioning each significance test on other phrase pair co-occurrence counts resulting in an additional reduction in size and increase in BLEU score. A series of experiments using Moses and the WMT11 corpora for French to English have been performed to quantify the improvement. By adhering strictly to the recommendations for the WMT11 baseline system, a strong reproducible research baseline was employed.",
}
@inproceedings{egan-2012-machine,
    title = "Machine Translation Revisited: An Operational Reality Check",
    author = "Egan, Kathleen",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Government MT User Program",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-government.5",
    abstract = "The government and the research community have strived for the past few decades to develop machine translation capabilities. Historically, DARPA took the lead in the grand challenge aiming at surpassing human translation quality. While we have made strides from rule based, to statistical and hybrid machine translation engines, we cannot rely solely on machine translation to overcome the language barrier and accomplish the mission. Machine Translation is often misunderstood or misplaced in the operational settings as expectations are unrealistic and optimization not achieved. With the increase in volume, variety and velocity of data, new paradigms are needed when choosing machine translation software and embedding it into a business process so as to achieve the operational goals. The talk will focus on the operational requirements and frame where, when and how to use machine translation. We will also outline some gaps and suggest new areas for research, development, and implementation.",
}
@inproceedings{gibbs-didamo-2012-lsp,
    title = "An {LSP} Perspective: Business {\&} Process Challenges Implementing {MT} Solutions: Is {MT} Delivering Expected Value?",
    author = "Gibbs, Rustin  and
      DiDamo, Joe",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Commercial MT User Program",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-commercial.7",
    abstract = "Machine translation resurfaced as a viable business solution about 5 years ago, with much hype. With the amount of content requiring translation, and a mellowing of user expectations about translation quality, it seemed there was real business value in developing machine translation solutions. Since then, however, the discounts offered to enterprise customers have remained stubbornly meager in the 10-20{\%} range, with high, up-front costs{---}far from the anticipated savings. This paper provides an overview of the challenges encountered in the value chain between customer and Language Service Provider (LSP) which keep translation costs high and limit machine translation adoption, discusses existing and potential solutions to these challenges, and offers suggestions on how to enlist the support of the LSP and freelance translator community to address these challenges.",
}
@inproceedings{matusov-2012-incremental,
    title = "Incremental Re-Training of a Hybrid {E}nglish-{F}rench {MT} System with Customer Translation Memory Data",
    author = "Matusov, Evgeny",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Commercial MT User Program",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-commercial.11",
    abstract = "In this paper, we present SAIC{'}s hybrid machine translation (MT) system and show how it was adapted to the needs of our customer {--} a major global fashion company. The adaptation was performed in two ways: off-line selection of domain-relevant parallel and monolingual data from a background database, as well as on-line incremental adaptation with customer parallel and translation memory data. The translation memory was integrated into the statistical search using two novel features. We show that these features can be used to produce nearly perfect translations of data that fully or to a large extent partially matches the TM entries, without sacrificing on the translation quality of the data without TM matches. We also describe how the human post-editing effort was reduced due to significantly better MT quality after adaptation, but also due to improved formatting and readability of the MT output.",
}
@inproceedings{jabbari-etal-2012-developing,
    title = "Developing an Open-domain {E}nglish-{F}arsi Translation System Using {AFEC}: Amirkabir Bilingual {F}arsi-{E}nglish Corpus",
    author = "Jabbari, Fattaneh  and
      Bakshaei, Somayeh  and
      Mohammadzadeh Ziabary, Seyyed Mohammad  and
      Khadivi, Shahram",
    editor = "Farghaly, Ali  and
      Oroumchian, Farhad",
    booktitle = "Fourth Workshop on Computational Approaches to Arabic-Script-based Languages",
    month = nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-caas14.3",
    pages = "17--23",
    abstract = "The translation quality of Statistical Machine Translation (SMT) depends on the amount of input data especially for morphologically rich languages. Farsi (Persian) language is such a language which has few NLP resources. It also suffers from the non-standard written characters which causes a large variety in the written form of each character. Moreover, the structural difference between Farsi and English results in long range reorderings which cannot be modeled by common SMT reordering models. Here, we try to improve the existing English-Farsi SMT system focusing on these challenges first by expanding our bilingual limited-domain corpus to an open-domain one. Then, to alleviate the character variations, a new text normalization algorithm is offered. Finally, some hand-crafted rules are applied to reduce the structural differences. Using the new corpus, the experimental results showed 8.82{\%} BLEU improvement by applying new normalization method and 9.1{\%} BLEU when rules are used.",
}
@inproceedings{saadane-etal-2012-using,
    title = "Using {A}rabic Transliteration to Improve Word Alignment from {F}rench- {A}rabic Parallel Corpora",
    author = "Saadane, Houda  and
      Benterki, Ouafa  and
      Semmar, Nasredine  and
      Fluhr, Christian",
    editor = "Farghaly, Ali  and
      Oroumchian, Farhad",
    booktitle = "Fourth Workshop on Computational Approaches to Arabic-Script-based Languages",
    month = nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-caas14.6",
    pages = "38--46",
    abstract = "In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistics-based word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entities, cognates and grammatical tags to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the Moses statistical machine translation system. The obtained results show that Arabic transliteration improves the quality of both alignment and translation.",
}
@inproceedings{banchs-li-2011-fm,
    title = "{AM}-{FM}: A Semantic Framework for Translation Quality Assessment",
    author = "Banchs, Rafael E.  and
      Li, Haizhou",
    editor = "Lin, Dekang  and
      Matsumoto, Yuji  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-2027",
    pages = "153--158",
}
@inproceedings{paul-sumita-2011-translation,
    title = "Translation Quality Indicators for Pivot-based Statistical {MT}",
    author = "Paul, Michael  and
      Sumita, Eiichiro",
    editor = "Wang, Haifeng  and
      Yarowsky, David",
    booktitle = "Proceedings of 5th International Joint Conference on Natural Language Processing",
    month = nov,
    year = "2011",
    address = "Chiang Mai, Thailand",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I11-1091",
    pages = "811--818",
}
@inproceedings{gangadharaiah-2011-reducing,
    title = "Reducing Asymmetry between language-pairs to Improve Alignment and Translation Quality",
    author = "Gangadharaiah, Rashmi",
    editor = "Wang, Haifeng  and
      Yarowsky, David",
    booktitle = "Proceedings of 5th International Joint Conference on Natural Language Processing",
    month = nov,
    year = "2011",
    address = "Chiang Mai, Thailand",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I11-1151",
    pages = "1346--1350",
}
@inproceedings{ma-etal-2011-confidence,
    title = "From the Confidence Estimation of Machine Translation to the Integration of {MT} and Translation Memory",
    author = "Ma, Yanjun  and
      He, Yifan  and
      van Genabith, Josef",
    booktitle = "Proceedings of Machine Translation Summit XIII: Tutorial Abstracts",
    month = sep # " 19",
    year = "2011",
    address = "Xiamen, China",
    url = "https://aclanthology.org/2011.mtsummit-tutorials.2",
    abstract = "In this tutorial, we cover techniques that facilitate the integration of Machine Translation (MT) and Translation Memory (TM), which can help the adoption of MT technology in localisation industry. The tutorial covers four parts: i) brief introduction of MT and TM systems, ii) MT confidence estimation measures tailored for the TM environment, iii) segment-level MT and MT integration, iv) sub-segment level MT and TM integration, and v) human evaluation of MT and TM integration. We will first briefly describe and compare how translations are generated in MT and TM systems, and suggest possible avenues to combines these two systems. We will also cover current quality / cost estimation measures applied in MT and TM systems, such as the fuzzy-match score in the TM, and the evaluation/confidence metrics used to judge MT outputs. We then move on to introduce the recent developments in the field of MT confidence estimation tailored towards predicting post-editing efforts. We will especially focus on the confidence metrics proposed by Specia et al., which is shown to have high correlation with human preference, as well as post-editing time. For segment-level MT and TM integration, we present translation recommendation and translation re-ranking models, where the integration happens at the 1-best or the N-best level, respectively. Given an input to be translated, MT-TM recommendation compares the output from the MT and the TM systems, and presents the better one to the post-editor. MT-TM re-ranking, on the other hand, combines k-best lists from both systems, and generates a new list according to estimated post-editing effort. We observe high precision of these models in automatic and human evaluations, indicating that they can be integrated into TM environments without the risk of deteriorating the quality of the post-editing candidate. For sub-segment level MT and TM integration, we try to reuse high quality TM chunks to improve the quality of MT systems. We can also predict whether phrase pairs derived from fuzzy matches should be used to constrain the translation of an input segment. Using a series of linguistically- motivated features, our constraints lead both to more consistent translation output, and to improved translation quality, as is measured by automatic evaluation scores. Finally, we present several methodologies that can be used to track post-editing effort, perform human evaluation of MT-TM integration, or help translators to access MT outputs in a TM environment.",
}
@inproceedings{lavie-2011-evaluating,
    title = "Evaluating the Output of Machine Translation Systems",
    author = "Lavie, Alon",
    booktitle = "Proceedings of Machine Translation Summit XIII: Tutorial Abstracts",
    month = sep # " 19",
    year = "2011",
    address = "Xiamen, China",
    url = "https://aclanthology.org/2011.mtsummit-tutorials.3",
    abstract = "This half-day tutorial provides a broad overview of how to evaluate translations that are produced by machine translation systems. The range of issues covered includes a broad survey of both human evaluation measures and commonly-used automated metrics, and a review of how these are used for various types of evaluation tasks, such as assessing the translation quality of MT-translated sentences, comparing the performance of alternative MT systems, or measuring the productivity gains of incorporating MT into translation workflows.",
}
@inproceedings{huet-etal-2011-utilisation,
    title = "Utilisation d{'}un score de qualit{\'e} de traduction pour le r{\'e}sum{\'e} multi-document cross-lingue (Using translation quality scores for cross-language multi-document summarization)",
    author = "Huet, St{\'e}phane  and
      Boudin, Florian  and
      Torres-Moreno, Juan-Manuel",
    editor = "Lafourcade, Mathieu  and
      Prince, Violaine",
    booktitle = "Actes de la 18e conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs",
    month = jun,
    year = "2011",
    address = "Montpellier, France",
    publisher = "ATALA",
    url = "https://aclanthology.org/2011.jeptalnrecital-long.5",
    pages = "49--58",
    abstract = "Le r{\'e}sum{\'e} automatique cross-lingue consiste {\`a} g{\'e}n{\'e}rer un r{\'e}sum{\'e} r{\'e}dig{\'e} dans une langue diff{\'e}rente de celle utilis{\'e}e dans les documents sources. Dans cet article, nous proposons une approche de r{\'e}sum{\'e} automatique multi-document, bas{\'e}e sur une repr{\'e}sentation par graphe, qui prend en compte des scores de qualit{\'e} de traduction lors du processus de s{\'e}lection des phrases. Nous {\'e}valuons notre m{\'e}thode sur un sous-ensemble manuellement traduit des donn{\'e}es utilis{\'e}es lors de la campagne d{'}{\'e}valuation internationale DUC 2004. Les r{\'e}sultats exp{\'e}rimentaux indiquent que notre approche permet d{'}am{\'e}liorer la lisibilit{\'e} des r{\'e}sum{\'e}s g{\'e}n{\'e}r{\'e}s, sans pour autant d{\'e}grader leur informativit{\'e}.",
    language = "French",
}
@inproceedings{mansour-etal-2011-combining,
    title = "Combining translation and language model scoring for domain-specific data filtering",
    author = "Mansour, Saab  and
      Wuebker, Joern  and
      Ney, Hermann",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-papers.5",
    pages = "222--229",
    abstract = "The increasing popularity of statistical machine translation (SMT) systems is introducing new domains of translation that need to be tackled. As many resources are already available, domain adaptation methods can be applied to utilize these recourses in the most beneficial way for the new domain. We explore adaptation via filtering, using the crossentropy scores to discard irrelevant sentences. We focus on filtering for two important components of an SMT system, namely the language model (LM) and the translation model (TM). Previous work has already applied LM cross-entropy based scoring for filtering. We argue that LM cross-entropy might be appropriate for LM filtering, but not as much for TM filtering. We develop a novel filtering approach based on a combined TM and LM cross-entropy scores. We experiment with two large-scale translation tasks, the Arabic-to-English and English-to-French IWSLT 2011 TED Talks MT tasks. For LM filtering, we achieve strong perplexity improvements which carry over to the translation quality with improvements up to +0.4{\%} BLEU. For TM filtering, the combined method achieves small but consistent improvements over the standalone methods. As a side effect of adaptation via filtering, the fully fledged SMT system vocabulary size and phrase table size are reduced by a factor of at least 2 while up to +0.6{\%} BLEU improvement is observed.",
}
@inproceedings{niehues-waibel-2011-using,
    title = "Using {W}ikipedia to translate domain-specific terms in {SMT}",
    author = "Niehues, Jan  and
      Waibel, Alex",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-papers.6",
    pages = "230--237",
    abstract = "When building a university lecture translation system, one important step is to adapt it to the target domain. One problem in this adaptation task is to acquire translations for domain specific terms. In this approach we tried to get these translations from Wikipedia, which provides articles on very specific topics in many different languages. To extract translations for the domain specific terms, we used the interlanguage links of Wikipedia . We analyzed different methods to integrate this corpus into our system and explored methods to disambiguate between different translations by using the text of the articles. In addition, we developed methods to handle different morphological forms of the specific terms in morphologically rich input languages like German. The results show that the number of out-of-vocabulary (OOV) words could be reduced by 50{\%} on computer science lectures and the translation quality could be improved by more than 1 BLEU point.",
}
@inproceedings{tomeh-etal-2011-good,
    title = "How good are your phrases? Assessing phrase quality with single class classification",
    author = "Tomeh, Nadi  and
      Turchi, Marco  and
      Wisinewski, Guillaume  and
      Allauzen, Alexandre  and
      Yvon, Fran{\c{c}}ois",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-papers.10",
    pages = "261--268",
    abstract = "We present a novel translation quality informed procedure for both extraction and scoring of phrase pairs in PBSMT systems. We reformulate the extraction problem in the supervised learning framework. Our goal is twofold. First, We attempt to take the translation quality into account; and second we incorporating arbitrary features in order to circumvent alignment errors. One-Class SVMs and the Mapping Convergence algorithm permit training a single-class classifier to discriminate between useful and useless phrase pairs. Such classifier can be learned from a training corpus that comprises only useful instances. The confidence score, produced by the classifier for each phrase pairs, is employed as a selection criteria. The smoothness of these scores allow a fine control over the size of the resulting translation model. Finally, confidence scores provide a new accuracy-based feature to score phrase pairs. Experimental evaluation of the method shows accurate assessments of phrase pairs quality even for regions in the space of possible phrase pairs that are ignored by other approaches. This enhanced evaluation of phrase pairs leads to improvements in the translation performance as measured by BLEU.",
}
@inproceedings{finch-etal-2011-nict,
    title = "The {NICT} translation system for {IWSLT} 2011",
    author = "Finch, Andrew  and
      Goh, Chooi-Ling  and
      Neubig, Graham  and
      Sumita, Eiichiro",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-evaluation.5",
    pages = "49--56",
    abstract = "This paper describes NICT{'}s participation in the IWSLT 2011 evaluation campaign for the TED speech translation ChineseEnglish shared-task. Our approach was based on a phrasebased statistical machine translation system that was augmented in two ways. Firstly we introduced rule-based re-ordering constraints on the decoding. This consisted of a set of rules that were used to segment the input utterances into segments that could be decoded almost independently. This idea here being that constraining the decoding process in this manner would greatly reduce the search space of the decoder, and cut out many possibilities for error while at the same time allowing for a correct output to be generated. The rules we used exploit punctuation and spacing in the input utterances, and we use these positions to delimit our segments. Not all punctuation/spacing positions were used as segment boundaries, and the set of used positions were determined by a set of linguistically-based heuristics. Secondly we used two heterogeneous methods to build the translation model, and lexical reordering model for our systems. The first method employed the popular method of using GIZA++ for alignment in combination with phraseextraction heuristics. The second method used a recentlydeveloped Bayesian alignment technique that is able to perform both phrase-to-phrase alignment and phrase pair extraction within a single unsupervised process. The models produced by this type of alignment technique are typically very compact whilst at the same time maintaining a high level of translation quality. We evaluated both of these methods of translation model construction in isolation, and our results show their performance is comparable. We also integrated both models by linear interpolation to obtain a model that outperforms either component. Finally, we added an indicator feature into the log-linear model to indicate those phrases that were in the intersection of the two translation models. The addition of this feature was also able to provide a small improvement in performance.",
}
@inproceedings{he-etal-2011-msr,
    title = "The {MSR} system for {IWSLT} 2011 evaluation",
    author = "He, Xiaodong  and
      Axelrod, Amittai  and
      Deng, Li  and
      Acero, Alex  and
      Hwang, Mei-Yuh  and
      Nguyen, Alisa  and
      Wang, Andrew  and
      Huang, Xiahui",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-evaluation.6",
    pages = "57--61",
    abstract = "This paper describes the Microsoft Research (MSR) system for the evaluation campaign of the 2011 international workshop on spoken language translation. The evaluation task is to translate TED talks (www.ted.com). This task presents two unique challenges: First, the underlying topic switches sharply from talk to talk. Therefore, the translation system needs to adapt to the current topic quickly and dynamically. Second, only a very small amount of relevant parallel data (transcripts of TED talks) is available. Therefore, it is necessary to perform accurate translation model estimation with limited data. In the preparation for the evaluation, we developed two new methods to attack these problems. Specifically, we developed an unsupervised topic modeling based adaption method for machine translation models. We also developed a discriminative training method to estimate parameters in the generative components of the translation models with limited data. Experimental results show that both methods improve the translation quality. Among all the submissions, ours achieves the best BLEU score in the machine translation Chinese-to-English track (MT{\_}CE) of the IWSLT 2011 evaluation that we participated.",
}
@inproceedings{lecouteux-etal-2011-lig,
    title = "{LIG} {E}nglish-{F}rench spoken language translation system for {IWSLT} 2011",
    author = "Lecouteux, Benjamin  and
      Besacier, Laurent  and
      Blanchon, Herv{\'e}",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-evaluation.8",
    pages = "68--72",
    abstract = "This paper describes the system developed by the LIG laboratory for the 2011 IWSLT evaluation. We participated to the English-French MT and SLT tasks. The development of a reference translation system (MT task), as well as an ASR output translation system (SLT task) are presented. We focus this year on the SLT task and on the use of multiple 1-best ASR outputs to improve overall translation quality. The main experiment presented here compares the performance of a SLT system where multiple ASR 1-best are combined before translation (source combination), with a SLT system where multiple ASR 1-best are translated, the system combination being conducted afterwards on the target side (target combination). The experimental results show that the second approach (target combination) overpasses the first one, when the performance is measured with BLEU.",
}
@inproceedings{boudahmane-etal-2011-advances,
    title = "Advances on spoken language translation in the Quaero program",
    author = "Boudahmane, Karim  and
      Buschbeck, Bianka  and
      Cho, Eunah  and
      Crego, Josep Maria  and
      Freitag, Markus  and
      Lavergne, Thomas  and
      Ney, Hermann  and
      Niehues, Jan  and
      Peitz, Stephan  and
      Senellart, Jean  and
      Sokolov, Artem  and
      Waibel, Alex  and
      Wandmacher, Tonio  and
      Wuebker, Joern  and
      Yvon, Fran{\c{c}}ois",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-evaluation.15",
    pages = "114--120",
    abstract = "The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality.",
}
@inproceedings{gupta-etal-2011-extending,
    title = "Extending a probabilistic phrase alignment approach for {SMT}",
    author = "Gupta, Mridul  and
      Hewavitharana, Sanjika  and
      Vogel, Stephan",
    editor = {Federico, Marcello  and
      Hwang, Mei-Yuh  and
      R{\"o}dder, Margit  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 8-9",
    year = "2011",
    address = "San Francisco, California",
    url = "https://aclanthology.org/2011.iwslt-evaluation.23",
    pages = "175--182",
    abstract = "Phrase alignment is a crucial step in phrase-based statistical machine translation. We explore a way of improving phrase alignment by adding syntactic information in the form of chunks as soft constraints guided by an in-depth and detailed analysis on a hand-aligned data set. We extend a probabilistic phrase alignment model that extracts phrase pairs by optimizing phrase pair boundaries over the sentence pair [1]. The boundaries of the target phrase are chosen such that the overall sentence alignment probability is optimal. Viterbi alignment information is also added in the extended model with a view of improving phrase alignment. We extract phrase pairs using a relatively larger number of features which are discriminatively trained using a large-margin online learning algorithm, i.e., Margin Infused Relaxed Algorithm (MIRA) and integrate it in our approach. Initial experiments show improvements in both phrase alignment and translation quality for Arabic-English on a moderate-size translation task.",
}
@inproceedings{hardmeier-2011-improving,
    title = "Improving Machine Translation Quality Prediction with Syntactic Tree Kernels",
    author = "Hardmeier, Christian",
    editor = "Forcada, Mikel L.  and
      Depraetere, Heidi  and
      Vandeghinste, Vincent",
    booktitle = "Proceedings of the 15th Annual Conference of the European Association for Machine Translation",
    month = may # " 30{--}31",
    year = "2011",
    address = "Leuven, Belgium",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2011.eamt-1.32",
}
@inproceedings{wan-etal-2010-cross,
    title = "Cross-Language Document Summarization Based on Machine Translation Quality Prediction",
    author = "Wan, Xiaojun  and
      Li, Huiying  and
      Xiao, Jianguo",
    editor = "Haji{\v{c}}, Jan  and
      Carberry, Sandra  and
      Clark, Stephen  and
      Nivre, Joakim",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1094",
    pages = "917--926",
}
@inproceedings{costa-jussa-etal-2010-automatic,
    title = "Automatic and Human Evaluation Study of a Rule-based and a Statistical {C}atalan-{S}panish Machine Translation Systems",
    author = "Costa-juss{\`a}, Marta R.  and
      Farr{\'u}s, Mireia  and
      Mari{\~n}o, Jos{\'e} B.  and
      Fonollosa, Jos{\'e} A. R.",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/47_Paper.pdf",
    abstract = "Machine translation systems can be classified into rule-based and corpus-based approaches, in terms of their core technology. Since both paradigms have largely been used during the last years, one of the aims in the research community is to know how these systems differ in terms of translation quality. To this end, this paper reports a study and comparison of a rule-based and a corpus-based (particularly, statistical) Catalan-Spanish machine translation systems, both of them freely available in the web. The translation quality analysis is performed under two different domains: journalistic and medical. The systems are evaluated by using standard automatic measures, as well as by native human evaluators. Automatic results show that the statistical system performs better than the rule-based system. Human judgements show that in the Spanish-to-Catalan direction the statistical system also performs better than the rule-based system, while in the Catalan-to-Spanish direction is the other way round. Although the statistical system obtains the best automatic scores, its errors tend to be more penalized by human judgements than the errors of the rule-based system. This can be explained because statistical errors are usually unexpected and they do not follow any pattern.",
}
@inproceedings{khalilov-etal-2010-towards,
    title = "Towards Improving {E}nglish-{L}atvian Translation: A System Comparison and a New Rescoring Feature",
    author = "Khalilov, Maxim  and
      Fonollosa, Jos{\'e} A. R.  and
      Skadin̨a, Inguna  and
      Br{\=a}l{\=\i}tis, Edgars  and
      Pretkalnin̨a, Lauma",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/228_Paper.pdf",
    abstract = "Translation into the languages with relatively free word order has received a lot less attention than translation into fixed word order languages (English), or into analytical languages (Chinese). At the same time this translation task is found among the most difficult challenges for machine translation (MT), and intuitively it seems that there is some space in improvement intending to reflect the free word order structure of the target language. This paper presents a comparative study of two alternative approaches to statistical machine translation (SMT) and their application to a task of English-to-Latvian translation. Furthermore, a novel feature intending to reflect the relatively free word order scheme of the Latvian language is proposed and successfully applied on the n-best list rescoring step. Moving beyond classical automatic scores of translation quality that are classically presented in MT research papers, we contribute presenting a manual error analysis of MT systems output that helps to shed light on advantages and disadvantages of the SMT systems under consideration.",
}
@inproceedings{ambati-etal-2010-active,
    title = "Active Learning and Crowd-Sourcing for Machine Translation",
    author = "Ambati, Vamshi  and
      Vogel, Stephan  and
      Carbonell, Jaime",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/244_Paper.pdf",
    abstract = "Large scale parallel data generation for new language pairs requires intensive human effort and availability of experts. It becomes immensely difficult and costly to provide Statistical Machine Translation (SMT) systems for most languages due to the paucity of expert translators to provide parallel data. Even if experts are present, it appears infeasible due to the impending costs. In this paper we propose Active Crowd Translation (ACT), a new paradigm where active learning and crowd-sourcing come together to enable automatic translation for low-resource language pairs. Active learning aims at reducing cost of label acquisition by prioritizing the most informative data for annotation, while crowd-sourcing reduces cost by using the power of the crowds to make do for the lack of expensive language experts. We experiment and compare our active learning strategies with strong baselines and see significant improvements in translation quality. Similarly, our experiments with crowd-sourcing on Mechanical Turk have shown that it is possible to create parallel corpora using non-experts and with sufficient quality assurance, a translation system that is trained using this corpus approaches expert quality.",
}
@inproceedings{lin-etal-2010-composing,
    title = "Composing Human and Machine Translation Services: Language Grid for Improving Localization Processes",
    author = "Lin, Donghui  and
      Murakami, Yoshiaki  and
      Ishida, Toru  and
      Murakami, Yohei  and
      Tanaka, Masahiro",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/317_Paper.pdf",
    abstract = "With the development of the Internet environments, more and more language services become accessible for common people. However, the gap between human translators and machine translators remains huge especially for the domain of localization processes that requires high translation quality. Although efforts of combining human and machine translators for supporting multilingual communication have been reported in previous research, how to apply such approaches for improving localization processes are rarely discussed. In this paper, we aim at improving localization processes by composing human and machine translation services based on the Language Grid, which is a language service platform that we have developed. Further, we conduct experiments to compare the translation quality and translation cost using several translation processes, including absolute machine translation processes, absolute human translation processes and translation processes by human and machine translation services. The experiment results show that composing monolingual roles and dictionary services improves the translation quality of machine translators, and that collaboration of human and machine translators is possible to reduce the cost comparing with the absolute bilingual human translation. We also discuss the generality of the experimental results and further challenging issues of the proposed localization processes.",
}
@inproceedings{baker-etal-2010-modality,
    title = "A Modality Lexicon and its use in Automatic Tagging",
    author = "Baker, Kathryn  and
      Bloodgood, Michael  and
      Dorr, Bonnie  and
      Filardo, Nathaniel W.  and
      Levin, Lori  and
      Piatko, Christine",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/446_Paper.pdf",
    abstract = "This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one tagger―a structure-based tagger―results in precision around 86{\%} (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data.",
}
@inproceedings{fishel-kirik-2010-linguistically,
    title = "Linguistically Motivated Unsupervised Segmentation for Machine Translation",
    author = "Fishel, Mark  and
      Kirik, Harri",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/604_Paper.pdf",
    abstract = "In this paper we use statistical machine translation and morphology information from two different morphological analyzers to try to improve translation quality by linguistically motivated segmentation. The morphological analyzers we use are the unsupervised Morfessor morpheme segmentation and analyzer toolkit and the rule-based morphological analyzer T3. Our translations are done using the Moses statistical machine translation toolkit with training on the JRC-Acquis corpora and translating on Estonian to English and English to Estonian language directions. In our work we model such linguistic phenomena as word lemmas and endings and splitting compound words into simpler parts. Also lemma information was used to introduce new factors to the corpora and to use this information for better word alignment or for alternative path back-off translation. From the results we find that even though these methods have shown previously and keep showing promise of improved translation, their success still largely depends on the corpora and language pairs used.",
}
@inproceedings{li-etal-2010-enriching,
    title = "Enriching Word Alignment with Linguistic Tags",
    author = "Li, Xuansong  and
      Ge, Niyu  and
      Grimes, Stephen  and
      Strassel, Stephanie M.  and
      Maeda, Kazuaki",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/670_Paper.pdf",
    abstract = "Incorporating linguistic knowledge into word alignment is becoming increasingly important for current approaches in statistical machine translation research. To improve automatic word alignment and ultimately machine translation quality, an annotation framework is jointly proposed by LDC (Linguistic Data Consortium) and IBM. The framework enriches word alignment corpora to capture contextual, syntactic and language-specific features by introducing linguistic tags to the alignment annotation. Two annotation schemes constitute the framework: alignment and tagging. The alignment scheme aims to identify minimum translation units and translation relations by using minimum-match and attachment annotation approaches. A set of word tags and alignment link tags are designed in the tagging scheme to describe these translation units and relations. The framework produces a solid ground-level alignment base upon which larger translation unit alignment can be automatically induced. To test the soundness of this work, evaluation is performed on a pilot annotation, resulting in inter- and intra- annotator agreement of above 90{\%}. To date LDC has produced manual word alignment and tagging on 32,823 Chinese-English sentences following this framework.",
}
@inproceedings{bojar-etal-2010-data,
    title = "Data Issues in {E}nglish-to-{H}indi Machine Translation",
    author = "Bojar, Ond{\v{r}}ej  and
      Stra{\v{n}}{\'a}k, Pavel  and
      Zeman, Daniel",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/756_Paper.pdf",
    abstract = "Statistical machine translation to morphologically richer languages is a challenging task and more so if the source and target languages differ in word order. Current state-of-the-art MT systems thus deliver mediocre results. Adding more parallel data often helps improve the results; if it doesn't, it may be caused by various problems such as different domains, bad alignment or noise in the new data. In this paper we evaluate the English-to-Hindi MT task from this data perspective. We discuss several available parallel data sources and provide cross-evaluation results on their combinations using two freely available statistical MT systems. We demonstrate various problems encountered in the data and describe automatic methods of data cleaning and normalization. We also show that the contents of two independently distributed data sets can unexpectedly overlap, which negatively affects translation quality. Together with the error analysis, we also present a new tool for viewing aligned corpora, which makes it easier to detect difficult parts in the data even for a developer not speaking the target language.",
}
@inproceedings{wong-2010-semantic,
    title = "Semantic Evaluation of Machine Translation",
    author = "Wong, Billy Tak-Ming",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Rosner, Mike  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10)",
    month = may,
    year = "2010",
    address = "Valletta, Malta",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2010/pdf/837_Paper.pdf",
    abstract = "It is recognized that many evaluation metrics of machine translation in use that focus on surface word level suffer from their lack of tolerance of linguistic variance, and the incorporation of linguistic features can improve their performance. To this end, WordNet is therefore widely utilized by recent evaluation metrics as a thesaurus for identifying synonym pairs. On this basis, word pairs in similar meaning, however, are still neglected. We investigate the significance of this particular word group to the performance of evaluation metrics. In our experiments we integrate eight different measures of lexical semantic similarity into an evaluation metric based on standard measures of unigram precision, recall and F-measure. It is found that a knowledge-based measure proposed by Wu and Palmer and a corpus-based measure, namely Latent Semantic Analysis, lead to an observable gain in correlation with human judgments of translation quality, in an extent to which better than the use of WordNet for synonyms.",
}
@inproceedings{isozaki-etal-2010-automatic,
    title = "Automatic Evaluation of Translation Quality for Distant Language Pairs",
    author = "Isozaki, Hideki  and
      Hirao, Tsutomu  and
      Duh, Kevin  and
      Sudoh, Katsuhito  and
      Tsukada, Hajime",
    editor = "Li, Hang  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D10-1092",
    pages = "944--952",
}
@inproceedings{carpuat-etal-2010-reordering,
    title = "Reordering Matrix Post-verbal Subjects for {A}rabic-to-{E}nglish {SMT}",
    author = "Carpuat, Marine  and
      Marton, Yuval  and
      Habash, Nizar",
    editor = "Langlais, Philippe  and
      Gagnon, Michel",
    booktitle = "Actes de la 17e conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs",
    month = jul,
    year = "2010",
    address = "Montr{\'e}al, Canada",
    publisher = "ATALA",
    url = "https://aclanthology.org/2010.jeptalnrecital-long.30",
    pages = "292--301",
    abstract = "We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44{\%} of the gain in BLEU and 51{\%} of the gain in TER obtained with a word alignment training bitext that is 5 times larger.",
}
@inproceedings{heger-etal-2010-combination,
    title = "A combination of hierarchical systems with forced alignments from phrase-based systems",
    author = "Heger, Carmen  and
      Wuebker, Joern  and
      Vilar, David  and
      Ney, Hermann",
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-papers.11",
    pages = "291--297",
    abstract = "Currently most state-of-the-art statistical machine translation systems present a mismatch between training and generation conditions. Word alignments are computed using the well known IBM models for single-word based translation. Afterwards phrases are extracted using extraction heuristics, unrelated to the stochastic models applied for finding the word alignment. In the last years, several research groups have tried to overcome this mismatch, but only with limited success. Recently, the technique of forced alignments has shown to improve translation quality for a phrase-based system, applying a more statistically sound approach to phrase extraction. In this work we investigate the first steps to combine forced alignment with a hierarchical model. Experimental results on IWSLT and WMT data show improvements in translation quality of up to 0.7{\%} BLEU and 1.0{\%} TER.",
}
@inproceedings{leusch-etal-2010-multi,
    title = "Multi-pivot translation by system combination",
    author = "Leusch, Gregor  and
      Max, Aur{\'e}lien  and
      Crego, Josep Maria  and
      Ney, Hermann",
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Papers",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-papers.12",
    pages = "299--306",
    abstract = "This paper describes a technique to exploit multiple pivot languages when using machine translation (MT) on language pairs with scarce bilingual resources, or where no translation system for a language pair is available. The principal idea is to generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using MT system combination techniques. Our technique can also be applied when a translation system for a language pair is available, but is limited in its translation accuracy because of scarce resources. Using statistical MT systems for the 11 different languages of Europarl, we show experimentally that a direct translation system can be replaced by this pivot approach without a loss in translation quality if about six pivot languages are available. Furthermore, we can already improve an existing MT system by adding two pivot systems to it. The maximum improvement was found to be 1.4{\%} abs. in BLEU in our experiments for 8 or more pivot languages.",
}
@inproceedings{matusov-kopru-2010-appteks,
    title = "{A}pp{T}ek{'}s {APT} machine translation system for {IWSLT} 2010",
    author = {Matusov, Evgeny  and
      K{\"o}pr{\"u}, Sel{\c{c}}uk},
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-evaluation.2",
    pages = "29--36",
    abstract = "In this paper, we describe AppTek{'}s new APT machine translation system that we employed in the IWSLT 2010 evaluation campaign. This year, we participated in the Arabic-to-English and Turkish-to-English BTEC tasks. We discuss the architecture of the system, the preprocessing steps and the experiments carried out during the campaign. We show that competitive translation quality can be obtained with a system that can be turned into a real-life product without much effort.",
}
@inproceedings{ling-etal-2010-inesc,
    title = "The {INESC}-{ID} machine translation system for the {IWSLT} 2010",
    author = "Ling, Wang  and
      Lu{\'\i}s, Tiago  and
      Gra{\c{c}}a, Jo{\~a}o  and
      Coheur, Lu{\'\i}sa  and
      Trancoso, Isabel",
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-evaluation.9",
    pages = "81--84",
    abstract = "In this paper we describe the Instituto de Engenharia de Sistemas e Computadores Investigac ̧a ̃o e Desenvolvimento (INESC-ID) system that participated in the IWSLT 2010 evaluation campaign. Our main goal for this evaluation was to employ several state-of-the-art methods applied to phrase-based machine translation in order to improve the translation quality. Aside from the IBM M4 alignment model, two constrained alignment models were tested, which produced better overall results. These results were further improved by using weighted alignment matrixes during phrase extraction, rather than the single best alignment. Finally, we tested several filters that ruled out phrase pairs based on puntuation. Our system was evaluated on the BTEC and DIALOG tasks, having achieved a better overall ranking in the DIALOG task.",
}
@inproceedings{henriquez-etal-2010-upc,
    title = "{UPC}-{BMIC}-{VDU} system description for the {IWSLT} 2010: testing several collocation segmentations in a phrase-based {SMT} system",
    author = "Henr{\'\i}quez, Carlos  and
      Costa-juss{\`a}, Marta R.  and
      Daudaravicius, Vidas  and
      Banchs, Rafael E.  and
      Mari{\~n}o, Jos{\'e} B.",
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-evaluation.26",
    pages = "189--195",
    abstract = "This paper describes the UPC-BMIC-VMU participation in the IWSLT 2010 evaluation campaign. The SMT system is a standard phrase-based enriched with novel segmentations. These novel segmentations are computed using statistical measures such as Log-likelihood, T-score, Chi-squared, Dice, Mutual Information or Gravity-Counts. The analysis of translation results allows to divide measures into three groups. First, Log-likelihood, Chi-squared and T-score tend to combine high frequency words and collocation segments are very short. They improve the SMT system by adding new translation units. Second, Mutual Information and Dice tend to combine low frequency words and collocation segments are short. They improve the SMT system by smoothing the translation units. And third, GravityCounts tends to combine high and low frequency words and collocation segments are long. However, in this case, the SMT system is not improved. Thus, the road-map for translation system improvement is to introduce new phrases with either low frequency or high frequency words. It is hard to introduce new phrases with low and high frequency words in order to improve translation quality. Experimental results are reported in the French-to-English IWSLT 2010 evaluation where our system was ranked 3rd out of nine systems.",
}
@inproceedings{kronrod-etal-2010-position,
    title = "Position Paper: Improving Translation via Targeted Paraphrasing",
    author = "Kronrod, Yakov  and
      Resnik, Philip  and
      Buzek, Olivia  and
      Hu, Chang  and
      Quinn, Alex  and
      Bederson, Ben",
    booktitle = "Proceedings of the Workshop on Collaborative Translation: technology, crowdsourcing, and the translator perspective",
    month = oct # " 31",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-workshop.3",
    abstract = "Targeted paraphrasing is a new approach to the problem of obtaining cost-effective, reasonable quality translation that makes use of simple and inexpensive human computations by monolingual speakers in combination with machine translation. The key insight behind the process is that it is possible to spot likely translation errors with only monolingual knowledge of the target language, and it is possible to generate alternative ways to say the same thing (i.e. paraphrases) with only monolingual knowledge of the source language. Evaluations demonstrate that this approach can yield substantial improvements in translation quality.",
}
@inproceedings{singh-bandyopadhyay-2010-statistical,
    title = "Statistical Machine Translation of {E}nglish-{M}anipuri using Morpho-syntactic and Semantic Information",
    author = "Singh, Thoudam Doren  and
      Bandyopadhyay, Savaji",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Student Research Workshop",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-srw.1",
    abstract = "English-Manipuri language pair is one of the rarely investigated with restricted bilingual resources. The development of a factored Statistical Machine Translation (SMT) system between English as source and Manipuri, a morphologically rich language as target is reported. The role of the suffixes and dependency relations on the source side and case markers on the target side are identified as important translation factors. The morphology and dependency relations play important roles to improve the translation quality. A parallel corpus of 10350 sentences from news domain is used for training and the system is tested with 500 sentences. Using the proposed translation factors, the output of the translation quality is improved as indicated by the BLEU score and subjective evaluation.",
}
@inproceedings{baker-etal-2010-semantically,
    title = "Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach",
    author = "Baker, Kathryn  and
      Bloodgood, Michael  and
      Callison-Burch, Chris  and
      Dorr, Bonnie  and
      Filardo, Nathaniel  and
      Levin, Lori  and
      Miller, Scott  and
      Piatko, Christine",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.7",
    abstract = "We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality{---}and further demonstrates that large gains can be achieved for low-resource languages with different word order than English.",
}
@inproceedings{huang-etal-2010-using,
    title = "Using Sublexical Translations to Handle the {OOV} Problem in {MT}",
    author = "Huang, Chung-chi  and
      Yen, Ho-ching  and
      Huang, Shih-ting  and
      Chang, Jason",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.13",
    abstract = "We introduce a method for learning to translate out-of-vocabulary (OOV) words. The method focuses on combining sublexical/constituent translations of an OOV to generate its translation candidates. In our approach, wild-card searches are formulated based on our OOV analysis, aimed at maximizing the probability of retrieving OOVs{'} sublexical translations from existing resource of machine translation (MT) systems. At run-time, translation candidates of the unknown words are generated from their suitable sublexical translations and ranked based on monolingual and bilingual information. We have incorporated the OOV model into a state-of-the-art MT system and experimental results show that our model indeed helps to ease the negative impact of OOVs on translation quality, especially for sentences containing more OOVs (significant improvement).",
}
@inproceedings{banerjee-etal-2010-combining,
    title = "Combining Multi-Domain Statistical Machine Translation Models using Automatic Classifiers",
    author = "Banerjee, Pratyush  and
      Du, Jinhua  and
      Li, Baoli  and
      Naskar, Sudip  and
      Way, Andy  and
      van Genabith, Josef",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.16",
    abstract = "This paper presents a set of experiments on Domain Adaptation of Statistical Machine Translation systems. The experiments focus on Chinese-English and two domain-specific corpora. The paper presents a novel approach for combining multiple domain-trained translation models to achieve improved translation quality for both domain-specific as well as combined sets of sentences. We train a statistical classifier to classify sentences according to the appropriate domain and utilize the corresponding domain-specific MT models to translate them. Experimental results show that the method achieves a statistically significant absolute improvement of 1.58 BLEU (2.86{\%} relative improvement) score over a translation model trained on combined data, and considerable improvements over a model using multiple decoding paths of the Moses decoder, for the combined domain test set. Furthermore, even for domain-specific test sets, our approach works almost as well as dedicated domain-specific models and perfect classification.",
}
@inproceedings{mohit-etal-2010-using,
    title = "Using Variable Decoding Weight for Language Model in Statistical Machine Translation",
    author = "Mohit, Behrang  and
      Hwa, Rebecca  and
      Lavie, Alon",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.17",
    abstract = "This paper investigates varying the decoder weight of the language model (LM) when translating different parts of a sentence. We determine the condition under which the LM weight should be adapted. We find that a better translation can be achieved by varying the LM weight when decoding the most problematic spot in a sentence, which we refer to as a difficult segment. Two adaptation strategies are proposed and compared through experiments. We find that adapting a different LM weight for every difficult segment resulted in the largest improvement in translation quality.",
}
@inproceedings{hardt-elming-2010-incremental,
    title = "Incremental Re-training for Post-editing {SMT}",
    author = "Hardt, Daniel  and
      Elming, Jakob",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.21",
    abstract = "A method is presented for incremental re-training of an SMT system, in which a local phrase table is created and incrementally updated as a file is translated and post-edited. It is shown that translation data from within the same file has higher value than other domain-specific data. In two technical domains, within-file data increases BLEU score by several full points. Furthermore, a strong recency effect is documented; nearby data within the file has greater value than more distant data. It is also shown that the value of translation data is strongly correlated with a metric defined over new occurrences of n-grams. Finally, it is argued that the incremental re-training prototype could serve as the basis for a practical system which could be interactively updated in real time in a post-editing setting. Based on the results here, such an interactive system has the potential to dramatically improve translation quality.",
}
@inproceedings{feng-etal-2010-source,
    title = "A Source-side Decoding Sequence Model for Statistical Machine Translation",
    author = "Feng, Minwei  and
      Mauser, Arne  and
      Ney, Hermann",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.22",
    abstract = "We propose a source-side decoding sequence language model for phrase-based statistical machine translation. This model is a reordering model in the sense that it helps the decoder find the correct decoding sequence. The model uses word-aligned bilingual training data. We show improved translation quality of up to 1.34{\%} BLEU and 0.54{\%} TER using this model compared to three other widely used reordering models.",
}
@inproceedings{haque-etal-2010-supertags,
    title = "Supertags as Source Language Context in Hierarchical Phrase-Based {SMT}",
    author = "Haque, Rejwanul  and
      Naskar, Sudip  and
      van den Bosch, Antal  and
      Way, Andy",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.23",
    abstract = "Statistical machine translation (SMT) models have recently begun to include source context modeling, under the assumption that the proper lexical choice of the translation for an ambiguous word can be determined from the context in which it appears. Various types of lexical and syntactic features have been explored as effective source context to improve phrase selection in SMT. In the present work, we introduce lexico-syntactic descriptions in the form of supertags as source-side context features in the state-of-the-art hierarchical phrase-based SMT (HPB) model. These features enable us to exploit source similarity in addition to target similarity, as modelled by the language model. In our experiments two kinds of supertags are employed: those from lexicalized tree-adjoining grammar (LTAG) and combinatory categorial grammar (CCG). We use a memory-based classification framework that enables the efficient estimation of these features. Despite the differences between the two supertagging approaches, they give similar improvements. We evaluate the performance of our approach on an English-to-Dutch translation task, and report statistically significant improvements of 4.48{\%} and 6.3{\%} BLEU scores in translation quality when adding CCG and LTAG supertags, respectively, as context-informed features.",
}
@inproceedings{foster-etal-2010-translating,
    title = "Translating Structured Documents",
    author = "Foster, George  and
      Isabelle, Pierre  and
      Kuhn, Roland",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.24",
    abstract = "Machine Translation traditionally treats documents as sets of independent sentences. In many genres, however, documents are highly structured, and their structure contains information that can be used to improve translation quality. We present a preliminary approach to document translation that uses structural features to modify the behaviour of a language model, at sentence-level granularity. To our knowledge, this is the first attempt to incorporate structural information into statistical MT. In experiments on structured English/French documents from the Hansard corpus, we demonstrate small but statistically significant improvements.",
}
@inproceedings{penkale-etal-2010-accuracy,
    title = "Accuracy-Based Scoring for Phrase-Based Statistical Machine Translation",
    author = "Penkale, Sergio  and
      May, Yanjun  and
      Galron, Daniel  and
      Way, Andy",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.28",
    abstract = "Although the scoring features of state-of-the-art Phrase-Based Statistical Machine Translation (PB-SMT) models are weighted so as to optimise an objective function measuring translation quality, the estimation of the features themselves does not have any relation to such quality metrics. In this paper, we introduce a translation quality-based feature to PB-SMT in a bid to improve the translation quality of the system. Our feature is estimated by averaging the edit-distance between phrase pairs involved in the translation of oracle sentences, chosen by automatic evaluation metrics from the N-best outputs of a baseline system, and phrase pairs occurring in the N-best list. Using our method, we report a statistically significant 2.11{\%} relative improvement in BLEU score for the WMT 2009 Spanish-to-English translation task. We also report that using our method we can achieve statistically significant improvements over the baseline using many other MT evaluation metrics, and a substantial increase in speed and reduction in memory use (due to a reduction in phrase-table size of 87{\%}) while maintaining significant gains in translation quality.",
}
@inproceedings{wendt-2010-better,
    title = "Better translations with user collaboration {--} Integrated {MT} at {M}icrosoft",
    author = "Wendt, Chris",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Commercial MT User Program",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-commercial.10",
    abstract = "This paper outlines the methodologies Microsoft has deployed for seamless integration of human translation into the translation workflow, and describes a variety of methods to gather and collect human translation data. Increased amounts of parallel training data help to enhance the translation quality of the statistical MT system in use at Microsoft. The presentation covers the theory, the technical methodology as well as the experiences Microsoft has with the implementation, and practical use of such a system. Included is a discussion of the factors influencing the translation quality of a statistical MT system, a short description of the feedback collection mechanism in use at Microsoft, and the metrics it observed on its MT deployments.",
}
@inproceedings{kim-2009-syntactic,
    title = "Syntactic Category Prediction for Improving Translation Quality in {E}nglish-{K}orean Machine Translation",
    author = "Kim, Sung-Dong",
    editor = "Kwong, Olivia",
    booktitle = "Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",
    month = dec,
    year = "2009",
    address = "Hong Kong",
    publisher = "City University of Hong Kong",
    url = "https://aclanthology.org/Y09-2032",
    pages = "710--717",
}
@inproceedings{callison-burch-2009-fast,
    title = "Fast, Cheap, and Creative: Evaluating Translation Quality Using {A}mazon{'}s {M}echanical {T}urk",
    author = "Callison-Burch, Chris",
    editor = "Koehn, Philipp  and
      Mihalcea, Rada",
    booktitle = "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing",
    month = aug,
    year = "2009",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D09-1030",
    pages = "286--295",
}
@inproceedings{specia-etal-2009-improving,
    title = "Improving the Confidence of Machine Translation Quality Estimates",
    author = "Specia, Lucia  and
      Turqui, Marco  and
      Wang, Zhuoran  and
      Shawe-Taylor, John  and
      Saunders, Craig",
    booktitle = "Proceedings of Machine Translation Summit XII: Papers",
    month = aug # " 26-30",
    year = "2009",
    address = "Ottawa, Canada",
    url = "https://aclanthology.org/2009.mtsummit-papers.16",
}
@inproceedings{ma-etal-2009-low,
    title = "Low-resource machine translation using {M}a{T}r{E}x",
    author = {Ma, Yanjun  and
      Okita, Tsuyoshi  and
      {\c{C}}etino{\u{g}}lu, {\"O}zlem  and
      Du, Jinhua  and
      Way, Andy},
    booktitle = "Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 1-2",
    year = "2009",
    address = "Tokyo, Japan",
    url = "https://aclanthology.org/2009.iwslt-evaluation.4",
    pages = "29--36",
    abstract = "In this paper, we give a description of the Machine Translation (MT) system developed at DCU that was used for our fourth participation in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT 2009). Two techniques are deployed in our system in order to improve the translation quality in a low-resource scenario. The first technique is to use multiple segmentations in MT training and to utilise word lattices in decoding stage. The second technique is used to select the optimal training data that can be used to build MT systems. In this year{'}s participation, we use three different prototype SMT systems, and the output from each system are combined using standard system combination method. Our system is the top system for Chinese{--}English CHALLENGE task in terms of BLEU score.",
}
@inproceedings{bertoldi-etal-2009-fbk,
    title = "{FBK} at {IWSLT} 2009",
    author = "Bertoldi, Nicola  and
      Bisazza, Arianna  and
      Cettolo, Mauro  and
      Sanchis-Trilles, Germ{\'a}n  and
      Federico, Marcello",
    booktitle = "Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 1-2",
    year = "2009",
    address = "Tokyo, Japan",
    url = "https://aclanthology.org/2009.iwslt-evaluation.5",
    pages = "37--44",
    abstract = "This paper reports on the participation of FBK at the IWSLT 2009 Evaluation. This year we worked on the Arabic-English and Turkish-English BTEC tasks with a special effort on linguistic preprocessing techniques involving morphological segmentation. In addition, we investigated the adaptation problem in the development of systems for the Chinese-English and English-Chinese challenge tasks; in particular, we explored different ways for clustering training data into topic or dialog-specific subsets: by producing (and combining) smaller but more focused models, we intended to make better use of the available training data, with the ultimate purpose of improving translation quality.",
}
@inproceedings{mi-etal-2009-ict,
    title = "The {ICT} statistical machine translation system for the {IWSLT} 2009",
    author = "Mi, Haitao  and
      Li, Yang  and
      Xia, Tian  and
      Xiao, Xinyan  and
      Feng, Yang  and
      Xie, Jun  and
      Xiong, Hao  and
      Tu, Zhaopeng  and
      Zheng, Daqi  and
      Lu, Yanjuan  and
      Liu, Qun",
    booktitle = "Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 1-2",
    year = "2009",
    address = "Tokyo, Japan",
    url = "https://aclanthology.org/2009.iwslt-evaluation.8",
    pages = "55--59",
    abstract = "This paper describes the ICT Statistical Machine Translation systems that used in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2009. For this year{'}s evaluation, we participated in the Challenge Task (Chinese-English and English-Chinese) and BTEC Task (Chinese-English). And we mainly focus on one new method to improve single system{'}s translation quality. Specifically, we developed a sentence-similarity based development set selection technique. For each task, we finally submitted the single system who got the maximum BLEU scores on the selected development set. The four single translation systems are based on different techniques: a linguistically syntax-based system, two formally syntax-based systems and a phrase-based system. Typically, we didn{'}t use any rescoring or system combination techniques in this year{'}s evaluation.",
}
@inproceedings{nakov-etal-2009-nus,
    title = "The {NUS} statistical machine translation system for {IWSLT} 2009",
    author = "Nakov, Preslav  and
      Liu, Chang  and
      Lu, Wei  and
      Ng, Hwee Tou",
    booktitle = "Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 1-2",
    year = "2009",
    address = "Tokyo, Japan",
    url = "https://aclanthology.org/2009.iwslt-evaluation.14",
    pages = "91--98",
    abstract = "We describe the system developed by the team of the National University of Singapore for the Chinese-English BTEC task of the IWSLT 2009 evaluation campaign. We adopted a state-of-the-art phrase-based statistical machine translation approach and focused on experiments with different Chinese word segmentation standards. In our official submission, we trained a separate system for each segmenter and we combined the outputs in a subsequent re-ranking step. Given the small size of the training data, we further re-trained the system on the development data after tuning. The evaluation results show that both strategies yield sizeable and consistent improvements in translation quality.",
}
@inproceedings{tyers-nordfalk-2009-shallow,
    title = "Shallow-transfer rule-based machine translation for {S}wedish to {D}anish",
    author = "Tyers, Francis M.  and
      Nordfalk, Jacob",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Martinez, Felipe  and
      Tyers, Francis M.",
    booktitle = "Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation",
    month = nov # " 2-3",
    year = "2009",
    address = "Alacant, Spain",
    url = "https://aclanthology.org/2009.freeopmt-1.6",
    pages = "27--34",
    abstract = "This article describes the development of a shallow-transfer machine translation system from Swedish to Danish in the Apertium platform. It gives details of the resources used, the methods for constructing the system and an evaluation of the translation quality. The quality is found to be comparable with that of current commercial systems, despite the particularly low coverage of the lexicons.",
}
@inproceedings{unhammer-trosterud-2009-reuse,
    title = "Reuse of free resources in machine translation between {N}ynorsk and {B}okm{\aa}l",
    author = "Unhammer, Kevin  and
      Trosterud, Trond",
    editor = "P{\'e}rez-Ortiz, Juan Antonio  and
      S{\'a}nchez-Martinez, Felipe  and
      Tyers, Francis M.",
    booktitle = "Proceedings of the First International Workshop on Free/Open-Source Rule-Based Machine Translation",
    month = nov # " 2-3",
    year = "2009",
    address = "Alacant, Spain",
    url = "https://aclanthology.org/2009.freeopmt-1.7",
    pages = "35--42",
    abstract = "We describe the development of a two-way shallow-transfer machine translation system between Norwegian Nynorsk and Norwegian Bokma ̊l built on the Apertium platform, using the Free and Open Source resources Norsk Ordbank and the Oslo{--}Bergen Constraint Grammar tagger. We detail the integration of these and other resources in the system along with the construction of the lexical and structural transfer, and evaluate the translation quality in comparison with another system. Finally, some future work is suggested.",
}
@inproceedings{koehn-etal-2008-towards,
    title = "Towards better Machine Translation Quality for the {G}erman-{E}nglish Language Pairs",
    author = "Koehn, Philipp  and
      Arun, Abhishek  and
      Hoang, Hieu",
    editor = "Callison-Burch, Chris  and
      Koehn, Philipp  and
      Monz, Christof  and
      Schroeder, Josh  and
      Fordyce, Cameron Shaw",
    booktitle = "Proceedings of the Third Workshop on Statistical Machine Translation",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W08-0318",
    pages = "139--142",
}
@inproceedings{chen-etal-2008-improving,
    title = "Improving Statistical Machine Translation Efficiency by Triangulation",
    author = "Chen, Yu  and
      Eisele, Andreas  and
      Kay, Martin",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/733_paper.pdf",
    abstract = "In current phrase-based Statistical Machine Translation systems, more training data is generally better than less. However, a larger data set eventually introduces a larger model that enlarges the search space for the decoder, and consequently requires more time and more resources to translate. This paper describes an attempt to reduce the model size by filtering out the less probable entries based on testing correlation using additional training data in an intermediate third language. The central idea behind the approach is triangulation, the process of incorporating multilingual knowledge in a single system, which eventually utilizes parallel corpora available in more than two languages. We conducted experiments using Europarl corpus to evaluate our approach. The reduction of the model size can be up to 70{\%} while the translation quality is being preserved.",
}
@inproceedings{lavecchia-etal-2008-phrase,
    title = "Phrase-Based Machine Translation based on Simulated Annealing",
    author = {Lavecchia, Caroline  and
      Langlois, David  and
      Sma{\"\i}li, Kamel},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/791_paper.pdf",
    abstract = "In this paper, we propose a new phrase-based translation model based on inter-lingual triggers. The originality of our method is double. First we identify common source phrases. Then we use inter-lingual triggers in order to retrieve their translations. Furthermore, we consider the way of extracting phrase translations as an optimization issue. For that we use simulated annealing algorithm to find out the best phrase translations among all those determined by inter-lingual triggers. The best phrases are those which improve the translation quality in terms of Bleu score. Tests are achieved on movie subtitle corpora. They show that our phrase-based machine translation (PBMT) system outperforms a state-of-the-art PBMT system by almost 7 points.",
}
@inproceedings{carpuat-wu-2008-evaluation,
    title = "Evaluation of Context-Dependent Phrasal Translation Lexicons for Statistical Machine Translation",
    author = "Carpuat, Marine  and
      Wu, Dekai",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/896_paper.pdf",
    abstract = "We present new direct data analysis showing that dynamically-built context-dependent phrasal translation lexicons are more useful resources for phrase-based statistical machine translation (SMT) than conventional static phrasal translation lexicons, which ignore all contextual information. After several years of surprising negative results, recent work suggests that context-dependent phrasal translation lexicons are an appropriate framework to successfully incorporate Word Sense Disambiguation (WSD) modeling into SMT. However, this approach has so far only been evaluated using automatic translation quality metrics, which are important, but aggregate many different factors. A direct analysis is still needed to understand how context-dependent phrasal translation lexicons impact translation quality, and whether the additional complexity they introduce is really necessary. In this paper, we focus on the impact of context-dependent translation lexicons on lexical choice in phrase-based SMT and show that context-dependent lexicons are more useful to a phrase-based SMT system than a conventional lexicon. A typical phrase-based SMT system makes use of more and longer phrases with context modeling, including phrases that were not seen very frequently in training. Even when the segmentation is identical, the context-dependent lexicons yield translations that match references more often than conventional lexicons.",
}
@inproceedings{hasan-ney-2008-multi,
    title = "A Multi-Genre {SMT} System for {A}rabic to {F}rench",
    author = "Hasan, Sa{\v{s}}a  and
      Ney, Hermann",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/549_paper.pdf",
    abstract = "This work presents improvements of a large-scale Arabic to French statistical machine translation system over a period of three years. The development includes better preprocessing, more training data, additional genre-specific tuning for different domains, namely newswire text and broadcast news transcripts, and improved domain-dependent language models. Starting with an early prototype in 2005 that participated in the second CESTA evaluation, the system was further upgraded to achieve favorable BLEU scores of 44.8{\%} for the text and 41.1{\%} for the audio setting. These results are compared to a system based on the freely available Moses toolkit. We show significant gains both in terms of translation quality (up to +1.2{\%} BLEU absolute) and translation speed (up to 16 times faster) for comparable configuration settings.",
}
@inproceedings{gimenez-marquez-2008-towards,
    title = "Towards Heterogeneous Automatic {MT} Error Analysis",
    author = "Gim{\'e}nez, Jes{\'u}s  and
      M{\`a}rquez, Llu{\'\i}s",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/483_paper.pdf",
    abstract = "This work studies the viability of performing heterogeneous automatic MT error analyses. Error analysis is, undoubtly, one of the most crucial stages in the development cycle of an MT system. However, often not enough attention is paid to this process. The reason is that performing an accurate error analysis requires intensive human labor. In order to speed up the error analysis process, we suggest partially automatizing it by having automatic evaluation metrics play a more active role. For that purpose, we have compiled a large and heterogeneous set of features at different linguistic levels and at different levels of granularity. Through a practical case study, we show how these features provide an effective means of ellaborating interpretable and detailed automatic reports of translation quality.",
}
@inproceedings{przybocki-etal-2008-translation,
    title = "Translation Adequacy and Preference Evaluation Tool ({TAP}-{ET})",
    author = "Przybocki, Mark  and
      Peterson, Kay  and
      Bronsart, S{\'e}bastien",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/299_paper.pdf",
    abstract = "Evaluation of Machine Translation (MT) technology is often tied to the requirement for tedious manual judgments of translation quality. While automated MT metrology continues to be an active area of research, a well known and often accepted standard metric is the manual human assessment of adequacy and fluency. There are several software packages that have been used to facilitate these judgments, but for the 2008 NIST Open MT Evaluation, NISTs Speech Group created an online software tool to accommodate the requirement for centralized data and distributed judges. This paper introduces the NIST TAP-ET application and reviews the reasoning underlying its design. Where available, analysis of data sets judged for Adequacy and Preference using the TAP-ET application will be presented. TAP-ET is freely available and ready to download, and contains a variety of customizable features.",
}
@inproceedings{eck-etal-2008-communicating,
    title = "Communicating Unknown Words in Machine Translation",
    author = "Eck, Matthias  and
      Vogel, Stephan  and
      Waibel, Alex",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/392_paper.pdf",
    abstract = "A new approach to handle unknown words in machine translation is presented. The basic idea is to find definitions for the unknown words on the source language side and translate those definitions instead. Only monolingual resources are required, which generally offer a broader coverage than bilingual resources and are available for a large number of languages. In order to use this in a machine translation system definitions are extracted automatically from online dictionaries and encyclopedias. The translated definition is then inserted and clearly marked in the original hypothesis. This is shown to lead to significant improvements in (subjective) translation quality.",
}
@inproceedings{cartoni-2008-lexical,
    title = "Lexical Resources for Automatic Translation of Constructed Neologisms: the Case Study of Relational Adjectives",
    author = "Cartoni, Bruno",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Piperidis, Stelios  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08)",
    month = may,
    year = "2008",
    address = "Marrakech, Morocco",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2008/pdf/247_paper.pdf",
    abstract = "This paper deals with the treatment of constructed neologisms in a machine translation system. It focuses on a particular issue in Romance languages: relational adjectives and the role they play in prefixation. Relational adjectives are formally adjectives but are semantically linked to their base-noun. In prefixation processes, the prefix is formally attached to the adjective, but its semantic value(s) is applied to the semantic features of the base-noun. This phenomenon has to be taken into account by any morphological analyser or generator. Moreover, in a contrastive perspective, the possibilities of creating adjectives out of nouns are not the same in every language. We present the special mechanism we put in place to deal with this type of prefixation, and the automatic method we used to extend lexicons, so that they can retrieve the base-nouns of prefixed relational adjectives, and improve the translation quality.",
}
@inproceedings{hearne-etal-2008-comparing,
    title = "Comparing Constituency and Dependency Representations for {SMT} Phrase-Extraction",
    author = "Hearne, Mary  and
      Ozdowska, Sylwia  and
      Tinsley, John",
    editor = "B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Bonastre, Jean-Francois",
    booktitle = "Actes de la 15{\`e}me conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts",
    month = jun,
    year = "2008",
    address = "Avignon, France",
    publisher = "ATALA",
    url = "https://aclanthology.org/2008.jeptalnrecital-court.14",
    pages = "131--140",
    abstract = "We consider the value of replacing and/or combining string-basedmethods with syntax-based methods for phrase-based statistical machine translation (PBSMT), and we also consider the relative merits of using constituency-annotated vs. dependency-annotated training data. We automatically derive two subtree-aligned treebanks, dependency-based and constituency-based, from a parallel English{--}French corpus and extract syntactically motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results show that combining string-based and syntax-based word- and phrase-pairs can improve translation quality irrespective of the type of syntactic annotation. Furthermore, using dependency annotation yields greater translation quality than constituency annotation for PB-SMT.",
}
@inproceedings{zens-ney-2008-improvements,
    title = "Improvements in dynamic programming beam search for phrase-based statistical machine translation.",
    author = "Zens, Richard  and
      Ney, Hermann",
    booktitle = "Proceedings of the 5th International Workshop on Spoken Language Translation: Papers",
    month = oct # " 20-21",
    year = "2008",
    address = "Waikiki, Hawaii",
    url = "https://aclanthology.org/2008.iwslt-papers.8",
    pages = "195--205",
    abstract = "Search is a central component of any statistical machine translation system. We describe the search for phrase-based SMT in detail and show its importance for achieving good translation quality. We introduce an explicit distinction between reordering and lexical hypotheses and organize the pruning accordingly. We show that for the large Chinese-English NIST task already a small number of lexical alternatives is sufficient, whereas a large number of reordering hypotheses is required to achieve good translation quality. The resulting system compares favorably with the current stateof-the-art, in particular we perform a comparison with cube pruning as well as with Moses.",
}
@inproceedings{zollmann-etal-2008-cmu,
    title = "The {CMU} syntax-augmented machine translation system: {SAMT} on Hadoop with n-best alignments.",
    author = "Zollmann, Andreas  and
      Venugopal, Ashish  and
      Vogel, Stephan",
    booktitle = "Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = oct # " 20-21",
    year = "2008",
    address = "Waikiki, Hawaii",
    url = "https://aclanthology.org/2008.iwslt-evaluation.2",
    pages = "18--25",
    abstract = "We present the CMU Syntax Augmented Machine Translation System that was used in the IWSLT-08 evaluation campaign. We participated in the Full-BTEC data track for Chinese-English translation, focusing on transcript translation. For this year{'}s evaluation, we ported the Syntax Augmented MT toolkit [1] to the Hadoop MapReduce [2] parallel processing architecture, allowing us to efficiently run experiments evaluating a novel {``}wider pipelines{''} approach to integrate evidence from N -best alignments into our translation models. We describe each step of the MapReduce pipeline as it is implemented in the open-source SAMT toolkit, and show improvements in translation quality by using N-best alignments in both hierarchical and syntax augmented translation systems.",
}
@inproceedings{ma-etal-2008-exploiting,
    title = "Exploiting alignment techniques in {MATREX}: the {DCU} machine translation system for {IWSLT} 2008.",
    author = "Ma, Yanjun  and
      Tinsley, John  and
      Hassan, Hany  and
      Du, Jinhua  and
      Way, Andy",
    booktitle = "Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = oct # " 20-21",
    year = "2008",
    address = "Waikiki, Hawaii",
    url = "https://aclanthology.org/2008.iwslt-evaluation.3",
    pages = "26--33",
    abstract = "In this paper, we give a description of the machine translation (MT) system developed at DCU that was used for our third participation in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT 2008). In this participation, we focus on various techniques for word and phrase alignment to improve system quality. Specifically, we try out our word packing and syntax-enhanced word alignment techniques for the Chinese{--}English task and for the English{--}Chinese task for the first time. For all translation tasks except Arabic{--}English, we exploit linguistically motivated bilingual phrase pairs extracted from parallel treebanks. We smooth our translation tables with out-of-domain word translations for the Arabic{--}English and Chinese{--}English tasks in order to solve the problem of the high number of out of vocabulary items. We also carried out experiments combining both in-domain and out-of-domain data to improve system performance and, finally, we deploy a majority voting procedure combining a language model-based method and a translation-based method for case and punctuation restoration. We participated in all the translation tasks and translated both the single-best ASR hypotheses and the correct recognition results. The translation results confirm that our new word and phrase alignment techniques are often helpful in improving translation quality, and the data combination method we proposed can significantly improve system performance.",
}
@inproceedings{lee-lee-2008-postech,
    title = "{POSTECH} machine translation system for {IWSLT} 2008 evaluation campaign.",
    author = "Lee, Jonghoon  and
      Lee, Gary Geunbae",
    booktitle = "Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = oct # " 20-21",
    year = "2008",
    address = "Waikiki, Hawaii",
    url = "https://aclanthology.org/2008.iwslt-evaluation.14",
    pages = "98--103",
    abstract = "In this paper, we describe POSTECH system for IWSLT 2008 evaluation campaign. The system is based on phrase based statistical machine translation. We set up a baseline system using well known freely available software. A preprocessing method and a language modeling method have been applied to the baseline system in order to improve machine translation quality. The preprocessing method is to identify and remove useless tokens in source texts. And the language modeling method models phrase level n-gram. We have participated in the BTEC tasks to see the effects of our methods.",
}
@inproceedings{khalilov-etal-2008-talp-i2r,
    title = "The {TALP}{\&}{I}2{R} {SMT} systems for {IWSLT} 2008.",
    author = "Khalilov, Maxim  and
      Costa-juss{\`a}, Maria R.  and
      Q., Carlos A. Henr{\'\i}quez  and
      Fonollosa, Jos{\'e} A. R.  and
      H., Adolfo Hern{\'a}ndez  and
      Mari{\~n}o, Jos{\'e} B.  and
      Banchs, Rafael E.  and
      Boxing, Chen  and
      Zhang, Min  and
      Aw, Aiti  and
      Li, Haizhou",
    booktitle = "Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = oct # " 20-21",
    year = "2008",
    address = "Waikiki, Hawaii",
    url = "https://aclanthology.org/2008.iwslt-evaluation.17",
    pages = "116--123",
    abstract = "This paper gives a description of the statistical machine translation (SMT) systems developed at the TALP Research Center of the UPC (Universitat Polite`cnica de Catalunya) for our participation in the IWSLT{'}08 evaluation campaign. We present Ngram-based (TALPtuples) and phrase-based (TALPphrases) SMT systems. The paper explains the 2008 systems{'} architecture and outlines translation schemes we have used, mainly focusing on the new techniques that are challenged to improve speech-to-speech translation quality. The novelties we have introduced are: improved reordering method, linear combination of translation and reordering models and new technique dealing with punctuation marks insertion for a phrase-based SMT system. This year we focus on the Arabic-English, Chinese-Spanish and pivot Chinese-(English)-Spanish translation tasks.",
}
@inproceedings{hildebrand-vogel-2008-combination,
    title = "Combination of Machine Translation Systems via Hypothesis Selection from Combined N-Best Lists",
    author = "Hildebrand, Almut Silja  and
      Vogel, Stephan",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Student Research Workshop",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-srw.3",
    pages = "254--261",
    abstract = "Different approaches in machine translation achieve similar translation quality with a variety of translations in the output. Recently it has been shown, that it is possible to leverage the individual strengths of various systems and improve the overall translation quality by combining translation outputs. In this paper we present a method of hypothesis selection which is relatively simple compared to system combination methods which construct a synthesis of the input hypotheses. Our method uses information from n-best lists from several MT systems and features on the sentence level which are independent from the MT systems involved to improve the translation quality.",
}
@inproceedings{schwartz-2008-multi,
    title = "Multi-Source Translation Methods",
    author = "Schwartz, Lane",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Student Research Workshop",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-srw.6",
    pages = "279--288",
    abstract = "Multi-parallel corpora provide a potentially rich resource for machine translation. This paper surveys existing methods for utilizing such resources, including hypothesis ranking and system combination techniques. We find that despite significant research into system combination, relatively little is know about how best to translate when multiple parallel source languages are available. We provide results to show that the MAX multilingual multi-source hypothesis ranking method presented by Och and Ney (2001) does not reliably improve translation quality when a broad range of language pairs are considered. We also show that the PROD multilingual multi-source hypothesis ranking method of Och and Ney (2001) cannot be used with standard phrase-based translation engines, due to a high number of unreachable hypotheses. Finally, we present an oracle experiment which shows that current hypothesis ranking methods fall far short of the best results reachable via sentence-level ranking.",
}
@inproceedings{madnani-etal-2008-multiple,
    title = "Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization",
    author = "Madnani, Nitin  and
      Resnik, Philip  and
      Dorr, Bonnie J.  and
      Schwartz, Richard",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-papers.13",
    pages = "143--152",
    abstract = "Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases.",
}
@inproceedings{venugopal-etal-2008-wider,
    title = "Wider Pipelines: N-Best Alignments and Parses in {MT} Training",
    author = "Venugopal, Ashish  and
      Zollmann, Andreas  and
      Smith, Noah A.  and
      Vogel, Stephan",
    booktitle = "Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 21-25",
    year = "2008",
    address = "Waikiki, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2008.amta-papers.18",
    pages = "192--201",
    abstract = "State-of-the-art statistical machine translation systems use hypotheses from several maximum a posteriori inference steps, including word alignments and parse trees, to identify translational structure and estimate the parameters of translation models. While this approach leads to a modular pipeline of independently developed components, errors made in these {``}single-best{''} hypotheses can propagate to downstream estimation steps that treat these inputs as clean, trustworthy training data. In this work we integrate N-best alignments and parses by using a probability distribution over these alternatives to generate posterior fractional counts for use in downstream estimation. Using these fractional counts in a DOP-inspired syntax-based translation system, we show significant improvements in translation quality over a single-best trained baseline.",
}
@inproceedings{johnson-etal-2007-improving,
    title = "Improving Translation Quality by Discarding Most of the Phrasetable",
    author = "Johnson, Howard  and
      Martin, Joel  and
      Foster, George  and
      Kuhn, Roland",
    editor = "Eisner, Jason",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1103",
    pages = "967--975",
}
@inproceedings{paul-etal-2007-reducing,
    title = "Reducing human assessment of machine translation quality to binary classifiers",
    author = "Paul, Michael  and
      Finch, Andrew  and
      Sumita, Eiichiro",
    editor = "Way, Andy  and
      Gawronska, Barbara",
    booktitle = "Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers",
    month = sep # " 7-9",
    year = "2007",
    address = {Sk{\"o}vde, Sweden},
    url = "https://aclanthology.org/2007.tmi-papers.19",
}
@inproceedings{makoushina-2007-translation,
    title = "Translation Quality Assurance Tools: current state and future approaches",
    author = "Makoushina, Julia",
    booktitle = "Proceedings of Translating and the Computer 29",
    month = nov # " 29-30",
    year = "2007",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/2007.tc-1.8",
}
@inproceedings{aikawa-etal-2007-impact,
    title = "Impact of controlled language on translation quality and post-editing in a statistical machine translation environment",
    author = "Aikawa, Takako  and
      Schwartz, Lee  and
      King, Ronit  and
      Corston-Oliver, Mo  and
      Lozano, Carmen",
    editor = "Maegaard, Bente",
    booktitle = "Proceedings of Machine Translation Summit XI: Papers",
    month = sep # " 10-14",
    year = "2007",
    address = "Copenhagen, Denmark",
    url = "https://aclanthology.org/2007.mtsummit-papers.1",
}
@inproceedings{stadler-peter-sporndli-2007-quest,
    title = "The quest for machine translation quality at {CLS} Communication",
    author = {Stadler, Hans-Udo  and
      Peter-Sp{\"o}rndli, Ursula},
    editor = "Maegaard, Bente",
    booktitle = "Proceedings of Machine Translation Summit XI: Papers",
    month = sep # " 10-14",
    year = "2007",
    address = "Copenhagen, Denmark",
    url = "https://aclanthology.org/2007.mtsummit-papers.58",
}
@inproceedings{zhu-wang-2006-effect,
    title = "The Effect of Translation Quality in {MT}-Based Cross-Language Information Retrieval",
    author = "Zhu, Jiang  and
      Wang, Haifeng",
    editor = "Calzolari, Nicoletta  and
      Cardie, Claire  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-1075",
    doi = "10.3115/1220175.1220250",
    pages = "593--600",
}
@inproceedings{huang-knight-2006-relabeling,
    title = "Relabeling Syntax Trees to Improve Syntax-Based Machine Translation Quality",
    author = "Huang, Bryant  and
      Knight, Kevin",
    editor = "Moore, Robert C.  and
      Bilmes, Jeff  and
      Chu-Carroll, Jennifer  and
      Sanderson, Mark",
    booktitle = "Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",
    month = jun,
    year = "2006",
    address = "New York City, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N06-1031",
    pages = "240--247",
}
@inproceedings{rapp-vide-2006-example,
    title = "Example-Based Machine Translation Using a Dictionary of Word Pairs",
    author = "Rapp, Reinhard  and
      Vide, Carlos Martin",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)",
    month = may,
    year = "2006",
    address = "Genoa, Italy",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2006/pdf/74_pdf.pdf",
    abstract = "Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words.",
}
@inproceedings{ma-cieri-2006-corpus,
    title = "Corpus Support for Machine Translation at {LDC}",
    author = "Ma, Xiaoyi  and
      Cieri, Christopher",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06)",
    month = may,
    year = "2006",
    address = "Genoa, Italy",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2006/pdf/754_pdf.pdf",
    abstract = "This paper describes LDC's efforts in collecting, creating and processing different types of linguistic data, including lexicons, parallel text, multiple translation corpora, and human assessment of translation quality, to support the research and development in Machine Translation. Through a combination of different procedures and core technologies, the LDC was able to create very large, high quality, and cost-efficient corpora, which have contributed significantly to recent advances in Machine Translation. Multiple translation corpora and human assessment together facilitate, validate and improve automatic evaluation metrics, which are vital to the development of MT systems. The Bilingual Internet Text Search (BITS) and Champollion sentence aligner enable the finding and processing of large quantities of parallel text. All specifications and tools used by LDC and described in the paper are or will be available to the general public.",
}
@inproceedings{crego-marino-2006-integration,
    title = "Integration of {POS}tag-based Source Reordering into {SMT} Decoding by an Extended Search Graph",
    author = "Crego, Josep M.  and
      Mari{\~n}o, Jos{\'e} B.",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.4",
    pages = "29--36",
    abstract = "This paper presents a reordering framework for statistical machine translation (SMT) where source-side reorderings are integrated into SMT decoding, allowing for a highly constrained reordered search graph. The monotone search is extended by means of a set of reordering patterns (linguistically motivated rewrite patterns). Patterns are automatically learnt in training from word-to-word alignments and source-side Part-Of-Speech (POS) tags. Traversing the extended search graph, the decoder evaluates every hypothesis making use of a group of widely used SMT models and helped by an additional Ngram language model of source-side POS tags. Experiments are reported on the Euparl task (Spanish-to-English and English-to- Spanish). Results are presented regarding translation accuracy (using human and automatic evaluations) and computational efficiency, showing significant improvements in translation quality for both translation directions at a very low computational cost.",
}
@inproceedings{huang-etal-2006-statistical,
    title = "Statistical Syntax-Directed Translation with Extended Domain of Locality",
    author = "Huang, Liang  and
      Knight, Kevin  and
      Joshi, Aravind",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.8",
    pages = "66--73",
    abstract = "In syntax-directed translation, the source-language input is first parsed into a parse-tree, which is then recursively converted into a string in the target-language. We model this conversion by an extended tree-to-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear frame-work in order to incorporate other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-the- art phrase-based system.",
}
@inproceedings{lopez-resnik-2006-word,
    title = "Word-Based Alignment, Phrase-Based Translation: What{'}s the Link?",
    author = "Lopez, Adam  and
      Resnik, Philip",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.11",
    pages = "90--99",
    abstract = "State-of-the-art statistical machine translation is based on alignments between phrases {--} sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results suggest that the relationship between alignment quality and translation quality is weaker than previously thought. We investigate this question directly, comparing the impact of high-quality alignments with a carefully constructed set of degraded alignments. In order to tease apart various interactions, we report experiments investigating the impact of alignments on different aspects of the system. Our results confirm a weak correlation, but they also illustrate that more data and better feature engineering may be more beneficial than better alignment.",
}
@inproceedings{nesson-etal-2006-induction,
    title = "Induction of Probabilistic Synchronous Tree-Insertion Grammars for Machine Translation",
    author = "Nesson, Rebecca  and
      Shieber, Stuart  and
      Rush, Alexander",
    booktitle = "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = aug # " 8-12",
    year = "2006",
    address = "Cambridge, Massachusetts, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2006.amta-papers.15",
    pages = "128--137",
    abstract = "The more expressive and flexible a base formalism for machine translation is, the less efficient parsing of it will be. However, even among formalisms with the same parse complexity, some formalisms better realize the desired characteristics for machine translation formalisms than others. We introduce a particular formalism, probabilistic synchronous tree-insertion grammar (PSTIG) that we argue satisfies the desiderata optimally within the class of formalisms that can be parsed no less efficiently than context-free grammars and demonstrate that it outperforms state-of-the-art word-based and phrase-based finite-state translation models on training and test data taken from the EuroParl corpus (Koehn, 2005). We then argue that a higher level of translation quality can be achieved by hybridizing our in- duced model with elementary structures produced using supervised techniques such as those of Groves et al. (2004).",
}
@inproceedings{wu-etal-2005-improving,
    title = "Improving Translation Memory with Word Alignment Information",
    author = "Wu, Hua  and
      Wang, Haifeng  and
      Liu, Zhanyi  and
      Tang, Kai",
    booktitle = "Proceedings of Machine Translation Summit X: Posters",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-posters.7",
    pages = "364--371",
    abstract = "This paper describes a generalized translation memory system, which takes advantage of sentence level matching, sub-sentential matching, and pattern-based machine translation technologies. All of the three techniques generate translation suggestions with the assistance of word alignment information. For the sentence level matching, the system generates the translation suggestion by modifying the translations of the most similar example with word alignment information. For sub-sentential matching, the system locates the translation fragments in several examples with word alignment information, and then generates the translation suggestion by combining these translation fragments. For pattern-based machine translation, the system first extracts translation patterns from examples using word alignment information and then generates translation suggestions with pattern matching. This system is compared with a traditional translation memory system without word alignment information in terms of translation efficiency and quality. Evaluation results indicate that our system improves the translation quality and saves about 20{\%} translation time.",
}
@inproceedings{phaholphinyo-etal-2005-practical,
    title = "A Practical of Memory-based Approach for Improving Accuracy of {MT}",
    author = "Phaholphinyo, Sitthaa  and
      Modhiran, Teerapong  and
      Kritsuthikul, Nattapol  and
      Supnithi, Thepchai",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.6",
    pages = "41--46",
    abstract = "Rule-Based Machine Translation (RBMT) [1] approach is a major approach in MT research. It needs linguistic knowledge to create appropriate rules of translation. However, we cannot completely add all linguistic rules to the system because adding new rules may cause a conflict with the old ones. So, we propose a memory based approach to improve the translation quality without modifying the existing linguistic rules. This paper analyses the translation problems and shows how this approach works.",
}
@inproceedings{surcin-etal-2005-evaluation,
    title = "Evaluation of Machine Translation with Predictive Metrics beyond {BLEU}/{NIST}: {CESTA} Evaluation Campaign {\#} 1",
    author = {Surcin, Sylvain  and
      Hamon, Olivier  and
      Hartley, Antony  and
      Rajman, Martin  and
      Popescu-Belis, Andrei  and
      Hadi, Widad Mustafa El  and
      Timimi, Isma{\"\i}l  and
      Dabbadie, Marianne  and
      Choukri, Khalid},
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.16",
    pages = "117--124",
    abstract = "In this paper, we report on the results of a full-size evaluation campaign of various MT systems. This campaign is novel compared to the classical DARPA/NIST MT evaluation campaigns in the sense that French is the target language, and that it includes an experiment of meta-evaluation of various metrics claiming to better predict different attributes of translation quality. We first describe the campaign, its context, its protocol and the data we used. Then we summarise the results obtained by the participating systems and discuss the meta-evaluation of the metrics used.",
}
@inproceedings{aramaki-etal-2005-probabilistic,
    title = "Probabilistic Model for Example-based Machine Translation",
    author = "Aramaki, Eiji  and
      Kurohashi, Sadao  and
      Kashioka, Hideki  and
      Kato, Naoto",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.29",
    pages = "219--226",
    abstract = "Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in retrieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the proposed model, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems.",
}
@inproceedings{crego-etal-2005-reordered,
    title = "Reordered Search, and Tuple Unfolding for Ngram-based {SMT}",
    author = "Crego, Josep M.  and
      Mari{\~n}o, Jos{\'e} B.  and
      de Gispert, Adri{\`a}",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.37",
    pages = "283--289",
    abstract = "In Statistical Machine Translation, the use of reordering for certain language pairs can produce a significant improvement on translation accuracy. However, the search problem is shown to be NP-hard when arbitrary reorderings are allowed. This paper addresses the question of reordering for an Ngram-based SMT approach following two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we allow for an Ngram-based decoder (MARIE) to perform a reordered search over the source sentence, while combining a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its reordered search produces an improved translation. On the other hand, we allow for a modification of the translation units that unfolds the tuples, so that shorter units are learnt from a new parallel corpus, where the source sentences are reordered according to the target language. This tuple unfolding technique reduces data sparseness and, when combined with the reordered search, further boosts translation performance. Translation accuracy and efficency results are reported for the IWSLT 2004 Chinese to English task.",
}
@inproceedings{jiang-haifeng-2005-effect,
    title = "The Effect of Adding Rules into the Rule-based {MT} System",
    author = "Jiang, Zhu  and
      Haifeng, Wang",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.39",
    pages = "298--304",
    abstract = "This paper investigates the relationship between the amount of the rules and the performance of the rule-based machine translation system. We keep adding more rules into the system and observe successive changes of the translation quality. Evaluations on translation quality reveal that the more the rules, the better the translation quality. A linear regression analysis shows that a positive linear relationship exists between the translation quality and the amount of the rules. We use this linear model to make prediction and test the prediction with newly developed rules. Experimental results indicate that the linear model effectively predicts the possible performance that the rule-based machine translation system may achieve with more rules added.",
}
@inproceedings{thurmair-2005-improving,
    title = "Improving Machine Translation Quality",
    author = "Thurmair, Gregor",
    booktitle = "Proceedings of Machine Translation Summit X: Invited papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-invited.9",
    abstract = "This paper reports on measures to improve the quality of MT systems, by using a hybrid system architecture which adds corpus-based and statistical components to an existing rule-based system backbone. The focus is on improving the accuracy of the dictionary resources.",
}
@inproceedings{morrissey-way-2005-example,
    title = "An Example-Based Approach to Translating Sign Language",
    author = "Morrissey, Sara  and
      Way, Andy",
    booktitle = "Workshop on example-based machine translation",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-ebmt.14",
    pages = "109--116",
    abstract = "Users of sign languages are often forced to use a language in which they have reduced competence simply because documentation in their preferred format is not available. While some research exists on translating between natural and sign languages, we present here what we believe to be the first attempt to tackle this problem using an example-based (EBMT) approach. Having obtained a set of English{--}Dutch Sign Language examples, we employ an approach to EBMT using the {`}Marker Hypothesis{'} (Green, 1979), analogous to the successful system of (Way {\&} Gough, 2003), (Gough {\&} Way, 2004a) and (Gough {\&} Way, 2004b). In a set of experiments, we show that encouragingly good translation quality may be obtained using such an approach.",
}
@inproceedings{paul-etal-2005-machine,
    title = "A Machine Learning Approach to Hypotheses Selection of Greedy Decoding for {SMT}",
    author = "Paul, Michael  and
      Sumita, Eiichiro  and
      Yamamoto, Seiichi",
    booktitle = "Workshop on example-based machine translation",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-ebmt.15",
    pages = "117--124",
    abstract = "This paper proposes a method for integrating example-based and rule-based machine translation systems with statistical methods. It extends a greedy decoder for statistical machine translation (SMT), which searches for an optimal translation by using SMT models starting from a decoder seed, i.e., the source language input paired with an initial translation hypothesis. In order to reduce local optima problems inherent in the search, the outputs generated by multiple translation engines, such as rule-based (RBMT) and example-based (EBMT) systems, are utilized as the initial translation hypotheses. This method outperforms conventional greedy decoding approaches using initial translation hypotheses based on translation examples retrieved from a parallel text corpus. However, the decoding of multiple initial translation hypotheses is computationally expensive. This paper proposes a method to select a single initial translation hypothesis before decoding based on a machine learning approach that judges the appropriateness of multiple initial translation hypotheses and selects the most confident one for decoding. Our approach is evaluated for the translation of dialogues in the travel domain, and the results show that it drastically reduces computational costs without a loss in translation quality.",
}
@inproceedings{lin-och-2004-automatic,
    title = "Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics",
    author = "Lin, Chin-Yew  and
      Och, Franz Josef",
    booktitle = "Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04)",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    url = "https://aclanthology.org/P04-1077",
    doi = "10.3115/1218955.1219032",
    pages = "605--612",
}
@inproceedings{cardey-etal-2004-designing,
    title = "Designing a controlled language for the machine translation of medical protocols: the case of {E}nglish to {C}hinese",
    author = "Cardey, Sylviane  and
      Greenfield, Peter  and
      Wu, Xiahong",
    editor = "Frederking, Robert E.  and
      Taylor, Kathryn B.",
    booktitle = "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = sep # " 28 - " # oct # " 2",
    year = "2004",
    address = "Washington, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/978-3-540-30194-3_5",
    pages = "37--47",
    abstract = "Because of its clarity and its simplified way of writing, controlled language (CL) is being paid increasing attention by NLP (natural language processing) researchers, such as in machine translation. The users of controlled languages are of two types, firstly the authors of documents written in the controlled language and secondly the end-user readers of the documents. As a subset of natural language, controlled language restricts vocabulary, grammar, and style for the purpose of reducing or eliminating both ambiguity and complexity. The use of controlled language can help decrease the complexity of natural language to a certain degree and thus improve the translation quality, especially for the partial or total automatic translation of non-general purpose texts, such as technical documents, manuals, instructions and medical reports. Our focus is on the machine translation of medical protocols applied in the field of zoonosis. In this article we will briefly introduce why controlled language is preferred in our research work, what kind of benefits it will bring to our work and how we could make use of this existing technique to facilitate our translation tool.",
}
@inproceedings{lavie-etal-2004-significance,
    title = "The significance of recall in automatic metrics for {MT} evaluation",
    author = "Lavie, Alon  and
      Sagae, Kenji  and
      Jayaraman, Shyamsundar",
    editor = "Frederking, Robert E.  and
      Taylor, Kathryn B.",
    booktitle = "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = sep # " 28 - " # oct # " 2",
    year = "2004",
    address = "Washington, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/978-3-540-30194-3_16",
    pages = "134--143",
    abstract = "Recent research has shown that a balanced harmonic mean (F1 measure) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality. We show that significantly better correlations can be achieved by placing more weight on recall than on precision. While this may seem unexpected, since BLEU and NIST focus on n-gram precision and disregard recall, our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall. We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST.",
}
@inproceedings{babych-hartley-2003-improving,
    title = "Improving Machine Translation Quality with Automatic Named Entity Recognition",
    author = "Babych, Bogdan  and
      Hartley, Anthony",
    booktitle = "Proceedings of the 7th International {EAMT} workshop on {MT} and other language technology tools, Improving {MT} through other language technology tools, Resource and tools for building {MT} at {EACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-2201",
}
@inproceedings{coughlin-2003-correlating,
    title = "Correlating automated and human assessments of machine translation quality",
    author = "Coughlin, Deborah",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.9",
    abstract = "We describe a large-scale investigation of the correlation between human judgments of machine translation quality and the automated metrics that are increasingly used to drive progress in the field. We compare the results of 124 human evaluations of machine translated sentences to the scores generated by two automatic evaluation metrics (BLEU and NIST). When datasets are held constant or file size is sufficiently large, BLEU and NIST scores closely parallel human judgments. Surprisingly, this was true even though these scores were calculated using just one human reference. We suggest that when human evaluators are forced to make decisions without sufficient context or domain expertise, they fall back on strategies that are not unlike determining n-gram precision.",
}
@inproceedings{culy-riehemann-2003-limits,
    title = "The limits of n-gram translation evaluation metrics",
    author = "Culy, Christopher  and
      Riehemann, Susanne Z.",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.10",
    abstract = "N-gram measures of translation quality, such as BLEU and the related NIST metric, are becoming increasingly important in machine translation, yet their behaviors are not fully understood. In this paper we examine the performance of these metrics on professional human translations into German of two literary genres, the Bible and Tom Sawyer. The most surprising result is that some machine translations outscore some professional human translations. In addition, it can be difficult to distinguish some other human translations from machine translations with only two reference translations; with four reference translations it is much easier. Our results lead us to conclude that much care must be taken in using n-gram measures in formal evaluations of machine translation quality, though they are still valuable as part of the iterative development cycle.",
}
@inproceedings{hajic-etal-2003-simple,
    title = "A simple multilingual machine translation system",
    author = "Haji{\v{c}}, Jan  and
      Homola, Petr  and
      Kubo{\v{n}}, Vladislav",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.21",
    abstract = "The multilingual machine translation system described in the first part of this paper demonstrates that the translation memory (TM) can be used in a creative way for making the translation process more automatic (in a way which in fact does not depend on the languages used). The MT system is based upon exploitation of syntactic similarities between more or less related natural languages. It currently covers the translation from Czech to Slovak, Polish and Lithuanian. The second part of the paper also shows that one of the most popular TM based commercial systems, TRADOS, can be used not only for the translation itself, but also for a relatively fast and natural method of evaluation of the translation quality of MT systems.",
}
@inproceedings{senellart-etal-2003-systran-intuitive,
    title = "{SYSTRAN} intuitive coding technology",
    author = "Senellart, Jean  and
      Yang, Jin  and
      Rebollo, Anabel",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.46",
    abstract = "Customizing a general-purpose MT system is an effective way to improve machine translation quality for specific usages. Building a user-specific dictionary is the first and most important step in the customization process. An intuitive dictionary-coding tool was developed and is now utilized to allow the user to build user dictionaries easily and intelligently. SYSTRAN{'}s innovative and proprietary IntuitiveCoding® technology is the engine powering this tool. It is comprised of various components: massive linguistic resources, a morphological analyzer, a statistical guesser, finite-state automaton, and a context-free grammar. Methodologically, IntuitiveCoding® is also a cross-application approach for high quality dictionary building in terminology import and exchange. This paper describes the various components and the issues involved in its implementation. An evaluation frame and utilization of the technology are also presented.",
}
@inproceedings{somers-sugita-2003-evaluating,
    title = "Evaluating commercial spoken language translation software",
    author = "Somers, Harold  and
      Sugita, Yuri",
    booktitle = "Proceedings of Machine Translation Summit IX: Papers",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-papers.49",
    abstract = "While spoken language translation remains a research goal, a crude form of it is widely available commercially for Japanese{--}English as a pipeline concatenation of speech-to-text recognition (SR), text-to-text translation (MT) and text-to-speech synthesis (SS). This paper proposes and illustrates an evaluation methodology for this noisy channel which tries to quantify the relative amount of degradation in translation quality due to each of the contributing modules. A small pilot experiment involving word-accuracy rate for the SR, and a fidelity evaluation for the MT and SS modules is proposed in which subjects are asked to paraphrase translated and/or synthesised sentences from a tourist{'}s phrasebook. Results show (as expected) that MT is the {``}noisiest{''} channel, with SS contributing least noise. The concatenation of the three channels is worse than could be predicted from the performance of each as individual tasks.",
}
@inproceedings{forsbom-2003-training,
    title = "Training a super model look-alike",
    author = "Forsbom, Eva",
    booktitle = "Workshop on Systemizing MT Evaluation",
    month = sep # " 23-27",
    year = "2003",
    address = "New Orleans, USA",
    url = "https://aclanthology.org/2003.mtsummit-eval.4",
    abstract = "Two string comparison measures, edit distance and n-gram co-occurrence, are tested for automatic evaluation of translation quality, where the quality is compared to one or several reference translations. The measures are tested in combination for diagnostic evaluation on segments. Both measures have been used for evaluation of translation quality before, but for another evaluation purpose (performance) and with another granularity (system). Preliminary experiments showed that the measures are not portable without redefinitions, so two new measures are defined, WAFT and NEVA. The new measures could be applied for both purposes and granularities.",
}
@inproceedings{charoenpornsawat-etal-2002-improving,
    title = "Improving Translation Quality of Rule-based Machine Translation",
    author = "Charoenpornsawat, Paisarn  and
      Sornlertlamvanich, Virach  and
      Charoenporn, Thatsanee",
    booktitle = "{COLING}-02: Machine Translation in Asia",
    year = "2002",
    url = "https://aclanthology.org/W02-1605",
}
@inproceedings{godden-2002-towards,
    title = "Towards a Speech-to-Speech Machine Translation Quality Metric",
    author = "Godden, Kurt",
    booktitle = "Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems",
    month = jul,
    year = "2002",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-0716",
    doi = "10.3115/1118656.1118672",
    pages = "117--120",
}
@inproceedings{flournoy-callison-burch-2001-secondary,
    title = "Secondary benefits of feedback and user interaction in machine translation tools",
    author = "Flournoy, Raymond S.  and
      Callison-Burch, Chris",
    editor = "Krauwer, Steven",
    booktitle = "Workshop on MT2010: Towards a Road Map for MT",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-road.3",
    abstract = "User feedback has often been proposed as a method for improving the accuracy of machine translation systems, but useful feedback can also serve a number of secondary benefits, including increasing user confidence in the MT technology and expanding the potential audience of users. Amikai, Inc. has produced a number of communication tools which embed translation technology and which attempt to improve the user experience by maximizing useful user interaction and feedback. As MT continues to develop, further attention needs to be paid to developing the overall user experience, which can improve the utility of translation tools even when translation quality itself plateaus.",
}
@inproceedings{miller-vanni-2001-scaling,
    title = "Scaling the {ISLE} taxonomy: development of metrics for the multi-dimensional characterization of machine translation quality",
    author = "Miller, Keith J.  and
      Vanni, Michelle",
    editor = "Maegaard, Bente",
    booktitle = "Proceedings of Machine Translation Summit VIII",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-papers.42",
    abstract = "The DARPA MT evaluations of the early 1990s, along with subsequent work on the MT Scale, and the International Standards for Language Engineering (ISLE) MT Evaluation framework represent two of the principal efforts in Machine Translation Evaluation (MTE) over the past decade. We describe a research program that builds on both of these efforts. This paper focuses on the selection of MT output features suggested in the ISLE framework, as well as the development of metrics for the features to be used in the study. We define each metric and describe the rationale for its development. We also discuss several of the finer points of the evaluation measures that arose as a result of verification of the measures against sample output texts from three machine translation systems.",
}
@inproceedings{och-ney-2001-statistical,
    title = "Statistical multi-source translation",
    author = "Och, Franz Josef  and
      Ney, Hermann",
    editor = "Maegaard, Bente",
    booktitle = "Proceedings of Machine Translation Summit VIII",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-papers.46",
    abstract = "We describe methods for translating a text given in multiple source languages into a single target language. The goal is to improve translation quality in applications where the ultimate goal is to translate the same document into many languages. We describe a statistical approach and two specific statistical models to deal with this problem. Our method is generally applicable as it is independent of specific models, languages or application domains. We evaluate the approach on a multilingual corpus covering all eleven official European Union languages that was collected automatically from the Internet. In various tests we show that these methods can significantly improve translation quality. As a side effect, we also compare the quality of statistical machine translation systems for many European languages in the same domain.",
}
@inproceedings{yasuda-etal-2001-automatic,
    title = "An automatic evaluation method of translation quality using translation answer candidates queried from a parallel corpus",
    author = "Yasuda, Keiji  and
      Sugaya, Fumiaki  and
      Takezawa, Toshiyuki  and
      Yamamoto, Seiichi  and
      Yanagida, Masuzo",
    editor = "Maegaard, Bente",
    booktitle = "Proceedings of Machine Translation Summit VIII",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-papers.67",
    abstract = "An automatic translation quality evaluation method is proposed. In the proposed method, a parallel corpus is used to query translation answer candidates. The translation output is evaluated by measuring the similarity between the translation output and translation answer candidates with DP matching. This method evaluates a language translation subsystem of the Japanese-to-English ATR-MATRIX speech translation system developed at ATR Interpreting Telecommunications Research Laboratories. Discriminant analysis is then carried out to examine the evaluation performance of the proposed method. Experimental results show the effectiveness of the proposed method. The discriminant ratio is 83.5{\%} for 2-class discrimination between absolutely correct and less appropriate translations classified subjectively. Also discussed are issues of the proposed method when it is applied to speech translation systems which inevitably make recognition errors.",
}
@inproceedings{marrafa-ribeiro-2001-quantitative,
    title = "Quantitative evaluation of machine translation systems: sentence level",
    author = "Marrafa, Palmira  and
      Ribeiro, Ant{\'o}nio",
    editor = "Hovy, Eduard  and
      King, Margaret  and
      Manzi, Sandra  and
      Reeder, Florence",
    booktitle = "Workshop on MT Evaluation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-eval.2",
    abstract = "This paper reports the first results of an on-going research on evaluation of Machine Translation quality. The starting point for this work was the framework of ISLE (the International Standards for Language Engineering), which provides a classification for evaluation of Machine Translation. In order to make a quantitative evaluation of translation quality, we pursue a more consistent, fine-grained and comprehensive classification of possible translation errors and we propose metrics for sentence level errors, specifically lexical and syntactic errors.",
}
@inproceedings{mustafa-el-hadi-etal-2001-setting,
    title = "Setting a methodology for machine translation evaluation",
    author = "Mustafa El Hadi, Widad  and
      Timimi, Ismail  and
      Dabbadie, Marianne",
    editor = "Hovy, Eduard  and
      King, Margaret  and
      Manzi, Sandra  and
      Reeder, Florence",
    booktitle = "Workshop on MT Evaluation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-eval.4",
    abstract = "In this paper some of the problems encountered in designing an evaluation for an MT system will be examined. The source text, in French, provided by INRA (Institut National pour la Recherche Agronomique i.e. National Institute for Agronomic Research) deals with biotechnology and animal reproduction. It has been translated into English. The output of the system (i.e. the result of the assembling of several components), as opposed to its individual modules or specific components (i.e. analysis, generation, grammar, lexicon, core, etc.), will be evaluated. Moreover, the evaluation will concentrate on translation quality and its fidelity to the source text. The evaluation is not comparative, which means that we tested a specific MT system, not necessarily representative of other MT systems that can be found on the market.",
}
@inproceedings{vanni-miller-2001-scaling,
    title = "Scaling the {ISLE} framework: validating tests of machine translation quality for multi-dimensional measurement",
    author = "Vanni, Michelle  and
      Miller, Keith J.",
    editor = "Hovy, Eduard  and
      King, Margaret  and
      Manzi, Sandra  and
      Reeder, Florence",
    booktitle = "Workshop on MT Evaluation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-eval.9",
    abstract = "Work on comparing a set of linguistic test scores for MT output to a set of the same tests{'} scores for naturally-occurring target language text (Jones and Rusk 2000) broke new ground in automating MT Evaluation. However, the tests used were selected on an ad hoc basis. In this paper, we report on work to extend our understanding, through refinement and validation, of suitable linguistic tests in the context of our novel approach to MTE. This approach was introduced in Miller and Vanni (2001a) and employs standard, rather than randomly-chosen, tests of MT output quality selected from the ISLE framework as well as a scoring system for predicting the type of information processing task performable with the output. Since the intent is to automate the scoring system, this work can also be viewed as the preliminary steps of algorithm design.",
}
@inproceedings{schaler-2001-beyond,
    title = "Beyond translation memories",
    author = {Sch{\"a}ler, Reinhard},
    editor = "Carl, Michael  and
      Way, Andy",
    booktitle = "Workshop on Example-Based machine Translation",
    month = sep # " 18-22",
    year = "2001",
    address = "Santiago de Compostela, Spain",
    url = "https://aclanthology.org/2001.mtsummit-ebmt.5",
    abstract = "One key to the success of EBMT is the removal of the boundaries limiting the potential of translation memories. To bring EBMT to fruition, researchers and developers have to go beyond the self-imposed limitations of what is now traditional, in computing terms almost old fashioned, TM technology. Experiments have shown that the probability of finding exact matches at phrase level is higher than the probability of finding exact matches at the current TM segment level. We outline our implementation of a linguistically enhanced translation memory system (or Phrasal Lexicon) implementing phrasal matching. This system takes advantage of the huge and underused resources available in existing translation memories and develops a traditional TM into a sophisticated example-based machine translation engine which when integrated into a hybrid MT solution can yield significant improvements in translation quality.",
}
@inproceedings{bohan-etal-2000-evaluating,
    title = "Evaluating Translation Quality as Input to Product Development",
    author = "Bohan, Niamh  and
      Breidt, Elisabeth  and
      Volk, Martin",
    editor = "Gavrilidou, M.  and
      Carayannis, G.  and
      Markantonatou, S.  and
      Piperidis, S.  and
      Stainhauer, G.",
    booktitle = "Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00)",
    month = may,
    year = "2000",
    address = "Athens, Greece",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2000/pdf/136.pdf",
}
@inproceedings{holland-etal-2000-evaluating,
    title = "Evaluating embedded machine translation in military field exercises",
    author = "Holland, M.  and
      Schlesinger, C.  and
      Tate, C.",
    editor = "White, John S.",
    booktitle = "Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: User Studies",
    month = oct # " 10-14",
    year = "2000",
    address = "Cuernavaca, Mexico",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-39965-8_27",
    pages = "239--247",
    abstract = "{``}Embedded{''} machine translation (MT) refers to an end-to-end computational process of which MT is one of the components. Integrating these components and evaluating the whole has proved to be problematic. As an example of embedded MT, we describe a prototype system called Falcon, which permits paper documents to be scanned and translated into English. MT is thus embedded in the preprocessing of hardcopy pages and subject to its noise. Because Falcon is intended for use by people in the military who are trying to screen foreign documents, and not to understand them in detail, its application makes low demands on translation quality. We report on a series of user trials that speak to the utility of embedded MT in army tasks.",
}
@inproceedings{schutz-1999-deploying-sae,
    title = "Deploying the {SAE} J2450 Translation Quality Metric in Language Technology Evaluation Projects",
    author = {Sch{\"u}tz, J{\"o}rg},
    booktitle = "Proceedings of Translating and the Computer 21",
    month = nov # " 10-11",
    year = "1999",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/1999.tc-1.14",
}
@inproceedings{sakamoto-moriguchi-1999-report,
    title = "Report on machine translation market in {J}apan",
    author = "Sakamoto, Yoshiyuki  and
      Moriguchi, Minoru",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.14",
    pages = "92--99",
    abstract = "This paper reports the current situation of the machine translation (MT) market in Japan, based on a survey conducted through questionnaires and interviews. The research targets three groups: MT manufacturers (including sales agents), professional translators and translation agencies, and general users. We completed the questionnaire on the first group and are now querying the second group through interviews and questionnaires. According to the survey of manufacturers and vendors, shipments and sales of MT systems plunged during 1996 to 1998. but respondents are expecting a slight recovery in 1999 and 2000. The primary requirement to raise shipments and sales is improvement of translation quality, most respondents believe. The survey of translation professionals started with the first interview on June 25. We plan to interview at least 20 people in the translation industry in four meetings. The results will be orally reported at the conference site. We are also designing the questionnaire for general users, which we plan to finish by the end of this year.",
}
@inproceedings{king-1999-transrouter,
    title = "{T}rans{R}outer : a decision support tool for translation managers",
    author = "King, Margaret",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.40",
    pages = "272--277",
    abstract = "Translation managers often have to decide on the most appropriate way to deal with a translation project. Possible options may include human translation, translation using a specific terminology resource, translation in interaction with a translation memory system, and machine translation. The decision making involved is complex, and it is not always easy to decide by inspection whether a specific text lends itself to certain kinds of treatment. TransRouter supports the decision making by offering a suite of computer based tools which can be used to analyse the text to be translated. Some tools, such as the word counter, the repetition detector, the sentence length estimator and the sentence simplicity checker look at characteristics of the text itself. A version comparison tool compares the new text to previously translated texts. Other tools, such as the unknown terms detector and the translation memory coverage estimator, estimate overlap between the text and a set of known resources. The information gained, combined with further information provided by the user, is input to a decision kernel which calculates possible routes towards achieving the translation together with their cost and consequences on translation quality. The user may influence the kernel by, for example, specifying particular resources or refining routes under investigation. The final decision on how to treat the project rests with the translation manager.",
}
@inproceedings{schutz-1999-deploying,
    title = "Deploying the {SAE} J2450 translation quality metric in {MT} projects",
    author = {Sch{\"u}tz, J{\"o}rg},
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.41",
    pages = "278--284",
    abstract = "This paper provides a nutshell description of how the recently published proposal of a translation quality metric for automotive service information is applicable in an evaluation scenario that deploys multilingual human language technology (mHLT). This proposal is the result of the J2450 task force group of the Society of Automotive Engineers (SAE). The main focus of the developed metric is on the syntactic level of a translation product. Since it is our belief that any evaluation of a translation (human and machine) should also take into account the semantic level of a human language product, we have slightly reshaped the SAE J2450 metric. In addition, we have embedded the whole evaluation process into an object-oriented quality model approach to account for the established business processes in the acquisition, production, translation and dissemination of automotive service information in SGML/XML environments. This scenario then provides the solid grounding for the setup of a quality assurance process for all dimensions related to the processing (human and machine) of automotive service information. The work reported here is one part of the ongoing European Multidoc project that has brought together several European automotive companies to taming the complexity of service information products in an integrated way. Within Multidoc integration means first and foremost the coupling of advanced information technology and mHLT. These aspects will be further motivated and detailed in the context of the specification of an evaluation scenario.",
}
@inproceedings{takezawa-etal-1999-new,
    title = "A new evaluation method for speech translation systems and a case study on {ATR}-{MATRIX} from {J}apanese to {E}nglish",
    author = "Takezawa, Toshiyuki  and
      Sugaya, Fumiaki  and
      Yokoo, Akio  and
      Yamamoto, Seiichi",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.44",
    pages = "299--307",
    abstract = "ATR-MATRIX is a multi-lingual speech-to-speech translation system designed to facilitate communications between two parties of different languages engaged in a spontaneous conversation in a travel arrangement domain. In this paper, we propose a new evaluation method for speech translation systems. Our current focus is on measuring the robustness of a language translation sub-system, with quick calculation and low cost. Therefore, we calculate the difference between the translation output from transcription texts and the translation output from input speech by a dynamic programming method. We present the first trial experiment of this method applied to our Japanese-to-English speech translation system. We also provide related discussions on such points as error analysis and the relationship between the proposed method and translation quality evaluation manually done by humans.",
}
@inproceedings{shimohata-etal-1999-machine,
    title = "Machine translation system {PENS{\'E}E}: system design and implementation",
    author = "Shimohata, Sayori  and
      Murata, Toshiki  and
      Ikeno, Atsushi  and
      Fukui, Tsuyoshi  and
      Yamamoto, Hideki",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.55",
    pages = "380--384",
    abstract = "This paper describes a new version of our machine translation system PENSÉE. In the light of its past systems, new PENSÉE is designed to improve portability from developers' point of view and translation quality from users' point of view. The features of new PENSÉE are: 1) Java implementation and 2) pattern-based transfer approach. In addition, new PENSÉE places a great importance on user interface especially in building user dictionaries. We will discuss why and how we resolve the existing MT problems and present dictionary building tools to support user customization.",
}
@inproceedings{choi-etal-1999-english,
    title = "{E}nglish-to-{K}orean Web translator : {``}{F}rom{T}o/Web-{EK}{''}",
    author = "Choi, Sung-Kwon  and
      Kim, Taewan  and
      Yuh, Sanghwa  and
      Jung, Han-Min  and
      Sim, Chul-Min  and
      Park, Sang-Kyu",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.64",
    pages = "432--437",
    abstract = "The previous English-Korean MT system that have been developed in Korea have dealt with only written text as translation object. Most of them enumerated a following list of the problems that had not seemed to be easy to solve in the near future : 1) processing of non-continuous idiomatic expressions 2) reduction of too many POS or structural ambiguities 3) robust processing for long sentence and parsing failure 4) selecting correct word correspondence between several alternatives. The problems can be considered as important factors that have influence on the translation quality of machine translation system. This paper describes not only the solutions of problems of the previous English-to-Korean machine translation systems but also the HTML tags management between two structurally different languages, English and Korean. Through the solutions we translate successfully English web documents into Korean one in the English-to-Korean web translator ``FromTo/Web-EK'' which has been developed from 1997.",
}
@inproceedings{yoshimi-sata-1999-improvement,
    title = "Improvement of translation quality of {E}nglish newspaper headlines by automatic preediting",
    author = "Yoshimi, Takehiko  and
      Sata, Ichiko",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.73",
    pages = "496--500",
    abstract = "Since the headlines of English news articles have a characteristic style, different from the styles which prevail in ordinary sentences, it is difficult for MT systems to generate high quality translations for headlines. We try to solve this problem by adding to an existing system a preediting module which rewrites the headlines to ordinary expressions. Rewriting of headlines makes it possible to generate better translations which would not otherwise be generated, with little or no changes to the existing parts of the system. Focusing on the absence of a form of the verb of 'be', we have described rewriting rules for putting properly the verb 'be' into the headlines.",
}
@inproceedings{nakayama-kumano-1999-collection,
    title = "Collection of dictionary data through {I}nternet translation service",
    author = "Nakayama, Keisuke  and
      Kumano, Akira",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.87",
    pages = "586--592",
    abstract = "We have developed an Internet translation service, which we began to provide in 1997 for English to Japanese translation and in 1998 for Japanese to English. In this service, users send a translation request from a web page and receive by e-mail the result of the translation outputted by Toshiba{'}s machine translation system. As in other similar services, users can specify English-Japanese word pairs(dictionary data) when making a translation request. What distinguishes our service from others is that our service system constructs users{'} own dictionaries on the server and helps them with this work by extracting words which the system expects to improve the system's translation quality if included in the dictionaries. With this function, users can efficiently add new word pairs so as to upgrade their own dictionaries when requesting re-translation. The dictionary data thus obtained from users can be utilized to improve the system dictionary on the server also.",
}
@inproceedings{lange-yang-1999-automatic,
    title = "Automatic domain recognition for machine translation",
    author = "Lange, Elke D.  and
      Yang, Jin",
    booktitle = "Proceedings of Machine Translation Summit VII",
    month = sep # " 13-17",
    year = "1999",
    address = "Singapore, Singapore",
    url = "https://aclanthology.org/1999.mtsummit-1.95",
    pages = "641--645",
    abstract = "This paper describes an ongoing project which has the goal of improving machine translation quality by increasing knowledge about the text to be translated. A basic piece of such knowledge is the domain or subject field of the text. When this is known, it is possible to improve meaning selection appropriate to that domain. Our current effort consists in automating both recognition of the text{'}s domain and the assignment of domain-specific translations. Results of our implementation show that the approach of using terminology categorization already existing in the machine translation system is very promising.",
}
@inproceedings{choi-etal-1998-hybrid-approaches,
    title = "Hybrid Approaches to Improvement of Translation Quality in Web-based {E}nglish-{K}orean Machine Translation",
    author = "Choi, Sung-Kwon  and
      Jung, Han-Min  and
      Sim, Chul-Min  and
      Kim, Taewan  and
      Park, Dong-In  and
      Park, Jun-Sik  and
      Choi, Key-Sun",
    booktitle = "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",
    month = aug,
    year = "1998",
    address = "Montreal, Quebec, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P98-1039",
    doi = "10.3115/980845.980886",
    pages = "251--255",
}
@inproceedings{choi-etal-1998-hybrid,
    title = "Hybrid Approaches to Improvement of Translation Quality in Web-based {E}nglish-{K}orean Machine Translation",
    author = "Choi, Sung-Kwon  and
      Jung, Han-Min  and
      Park, Jun-Sik  and
      Choi, Key-Sun",
    booktitle = "{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics",
    year = "1998",
    url = "https://aclanthology.org/C98-1039",
}
@inproceedings{gerber-hovy-1998-improving,
    title = "Improving translation quality by manipulating sentence length",
    author = "Gerber, Laurie  and
      Hovy, Eduard",
    editor = "Farwell, David  and
      Gerber, Laurie  and
      Hovy, Eduard",
    booktitle = "Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = oct # " 28-31",
    year = "1998",
    address = "Langhorne, PA, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/3-540-49478-2_40",
    pages = "448--460",
    abstract = "Translation systems tend to have more trouble with long sentences than with short ones for a variety of reasons. When the source and target languages differ rather markedly, as do Japanese and English, this problem is reflected in lower quality output. To improve readability, we experimented with automatically splitting long sentences into shorter ones. This paper outlines the problem, describes the sentence splitting procedure and rules, and provides an evaluation of the results.",
}
@inproceedings{nubel-1997-end,
    title = "End-to-End Evaluation in {VERBMOBIL} {I}",
    author = {N{\"u}bel, Rita},
    editor = "Teller, Virginia  and
      Sundheim, Beth",
    booktitle = "Proceedings of Machine Translation Summit VI: Papers",
    month = oct # " 29 {--} " # nov # " 1",
    year = "1997",
    address = "San Diego, California",
    url = "https://aclanthology.org/1997.mtsummit-papers.22",
    pages = "232--239",
    abstract = "VERBMOBIL is a speech-to-speech translation system for spoken dialogues between two speakers. The application scenario is appointment scheduling for business meetings, with spoken dialogues between two speakers. Both dialogue participants have at least a passive knowledge of English which serves as intermediate language1. The transfer directions are German to English and Japanese to English. A special feature of VERBMOBIL is that translations are produced on demand when the dialogue participants are unable to express themselves in English and therefore prefer to use their mother tongue. In this paper2 we present the criteria and the evaluation procedure for evaluating the translation quality of the VERBMOBIL prototype. The evaluated data have been produced by three concurrent processing methods that are integrated in the VERBMOBIL prototype. These processing methods differ with respect to processing depth, processing speed and translation quality ([2], p. 2). The paper is structured as follows: we start by giving a short description of the VERBMOBIL architecture focusing on the concurrent linguistic analyses and transfer processes which lead to three alternative translation outputs for each turn3. In section two we outline the evaluation procedure and criteria. The third section discusses the evaluation results, and the conclusion of the paper gives an outlook to future applications of automated evaluation procedures for machine translation (MT) based on an MT architecture where several concurrent translation approaches are integrated.",
}
@inproceedings{okumura-etal-1991-multi,
    title = "Multi-lingual Sentence Generation from the {PIVOT} interlingua",
    author = "Okumura, Akitoshi  and
      Muraki, Kazunori  and
      Akamine, Susumu",
    booktitle = "Proceedings of Machine Translation Summit III: Papers",
    month = jul # " 1-4",
    year = "1991",
    address = "Washington DC, USA",
    url = "https://aclanthology.org/1991.mtsummit-papers.11",
    pages = "63--65",
    abstract = "We wrote this report in Japanese and translated it by NEC's machine translation system PIVOT/JE.) IBS (International Business Service) is the company which does the documentation service which contains translation business. We introduced a machine translation system into translation business in earnest last year. The introduction of a machine translation system changed the form of our translation work. The translation work was divided into some steps and the person who isn't experienced became able to take it of the work of each of translation steps. As a result, a total translation cost reduced. In this paper, first, we report on the usage of our machine translation system. Next, we report on translation quality and the translation cost with a machine translation system. Lastly, we report on the merit which was gotten by introducing machine translation.",
}
@inproceedings{jin-1991-translation,
    title = "Translation Accuracy and Translation Efficiency",
    author = "Jin, Wanying",
    booktitle = "Proceedings of Machine Translation Summit III: Papers",
    month = jul # " 1-4",
    year = "1991",
    address = "Washington DC, USA",
    url = "https://aclanthology.org/1991.mtsummit-papers.14",
    pages = "85--92",
    abstract = "ULTRA (Universal Language Translator) is a multi-lingua] bidirectional translation system between English, Spanish, German, Japanese and Chinese. It employs an interlingua] structure to translate among these five languages. An interlingual representation is used as a deep structure through which any pair of these languages can be translated in either direction. This paper describes some techniques used in the Chinese system to solve problems in word ordering, language equivalency, Chinese verb constituent and prepositional phrase attachment. By means of these techniques translation quality has been significantly improved. Heuristic search, which results in translation efficiency, is also discussed.",
}
@inproceedings{kahl-1990-translation,
    title = "Translation quality - how can we tell it{'}s good enough?",
    author = "Kahl, Peter",
    editor = "Picken, Catriona",
    booktitle = "Proceedings of Translating and the Computer 12: Applying technology to the translation process",
    month = nov # " 8-9",
    year = "1990",
    address = "London, UK",
    publisher = "Aslib",
    url = "https://aclanthology.org/1990.tc-1.17",
}
@inproceedings{knowles-1978-error,
    title = "Error analysis of Systran output - a suggested criterion for the {`}internal{'} evaluation of translation quality and a possible corrective for system design",
    author = "Knowles, F.",
    editor = "Snell, Barbara M.",
    booktitle = "Translating and the Computer",
    month = nov # " 14",
    year = "1978",
    address = "London, UK",
    publisher = "Aslib Proceedings",
    url = "https://aclanthology.org/1978.tc-1.6",
}